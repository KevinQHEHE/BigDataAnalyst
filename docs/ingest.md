# ğŸ“š Ingest Pipeline â€” TÃ i liá»‡u HoÃ n chá»‰nh

HÆ°á»›ng dáº«n toÃ n bá»™ quy trÃ¬nh ingestion tá»« Bronze â†’ Silver â†’ Gold cho AQI Lakehouse pipeline.

---

## ğŸ“‹ Tá»•ng quan

Pipeline AQI sá»­ dá»¥ng **Medallion Architecture** - 3 layers xá»­ lÃ½ dá»¯ liá»‡u:

```
Open-Meteo API
     â†“ (fetch hourly)
Bronze Layer (raw data)
     â†“ (clean + add keys)
Silver Layer (data warehouse)
     â†“ (aggregate + enrich)
Gold Layer (analytics-ready)
```

| Layer | ThÃ nh pháº§n | Input | Output | Má»¥c Ä‘Ã­ch |
|-------|-----------|-------|--------|---------|
| **Bronze** | `run_bronze_pipeline.py` | Open-Meteo API | `bronze.open_meteo_hourly` | Raw hourly data |
| **Silver** | `run_silver_pipeline.py` | Bronze | `silver.air_quality_hourly_clean` | Cleaned + keyed |
| **Gold** | `load_dim_*.py` + `transform_fact_*.py` | Silver | `gold.dim_*` + `gold.fact_*` | Star schema |

---

## ğŸš€ Quick Start

### âš¡ One-time Setup (Láº§n Ä‘áº§u)

```bash
# 1ï¸âƒ£ Backfill Bronze vá»›i 1 nÄƒm dá»¯ liá»‡u lá»‹ch sá»­
bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- \
  --mode backfill \
  --start-date 2024-01-01 \
  --end-date 2024-12-31 \
  --chunk-days 90

# 2ï¸âƒ£ Transform Bronze â†’ Silver (full)
bash scripts/spark_submit.sh jobs/silver/run_silver_pipeline.py -- --mode full

# 3ï¸âƒ£ Load Gold Dimensions
bash scripts/spark_submit.sh jobs/gold/load_dim_date.py -- --mode full
bash scripts/spark_submit.sh jobs/gold/load_dim_time.py
bash scripts/spark_submit.sh jobs/gold/load_dim_location.py
bash scripts/spark_submit.sh jobs/gold/load_dim_pollutant.py

# 4ï¸âƒ£ Transform Gold Facts
bash scripts/spark_submit.sh jobs/gold/transform_fact_hourly.py -- --mode full
bash scripts/spark_submit.sh jobs/gold/transform_fact_daily.py -- --mode full
bash scripts/spark_submit.sh jobs/gold/detect_episodes.py -- --mode full
```

### ğŸ”„ Daily (HÃ ng ngÃ y)

```bash
# 1ï¸âƒ£ Bronze: Upsert new data from API
bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- --mode upsert

# 2ï¸âƒ£ Silver: Transform incremental
bash scripts/spark_submit.sh jobs/silver/run_silver_pipeline.py -- --mode incremental

# 3ï¸âƒ£ Gold: Refresh facts
bash scripts/spark_submit.sh jobs/gold/transform_fact_hourly.py -- --mode incremental
bash scripts/spark_submit.sh jobs/gold/transform_fact_daily.py -- --mode incremental
bash scripts/spark_submit.sh jobs/gold/detect_episodes.py -- --mode incremental
```

---

## ğŸ—ï¸ Bronze Layer â€” API Data Ingestion

### ğŸ“Œ Má»¥c Ä‘Ã­ch

**Láº¥y dá»¯ liá»‡u thÃ´ tá»« Open-Meteo Air Quality API** vÃ  lÆ°u vÃ o Iceberg table.

**Äáº·c Ä‘iá»ƒm**:
- Hourly measurements tá»« 50+ locations worldwide
- ~47K+ records (~720 records/location/year)
- Raw data gáº§n nhÆ° API response (minimal transformation)
- Partition theo `date_utc` â†’ efficient time-series queries

### ğŸ“ File

- **Script**: `jobs/bronze/run_bronze_pipeline.py` (403 lines)
- **Input**: `data/locations.jsonl` (location metadata)
- **Output**: `hadoop_catalog.lh.bronze.open_meteo_hourly` (Iceberg table)
- **External**: Open-Meteo API (free, no key required)

### ğŸ”§ Mode: Backfill

Náº¡p dá»¯ liá»‡u lá»‹ch sá»­. DÃ¹ng láº§n Ä‘áº§u setup hoáº·c reprocess data cÅ©.

```bash
# Local Spark
python3 jobs/bronze/run_bronze_pipeline.py \
  --mode backfill \
  --start-date 2024-01-01 \
  --end-date 2024-12-31 \
  --chunk-days 90

# YARN Cluster (recommended for production)
bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- \
  --mode backfill \
  --start-date 2024-01-01 \
  --end-date 2024-12-31 \
  --chunk-days 30 \
  --override
```

**Parameters**:
- `--start-date`, `--end-date`: Date range (YYYY-MM-DD format)
- `--chunk-days`: Split date range thÃ nh chunks (default 90)
  - Nhá» hÆ¡n = safer, nhÆ°ng cháº­m hÆ¡n
  - Lá»›n hÆ¡n = nhanh hÆ¡n, nhÆ°ng risk memory/timeout
- `--override`: Ghi Ä‘Ã¨ data cÅ© (optional, default False)
  - False = skip dates already exist
  - True = reprocess all dates
- `--locations`: Path tá»›i locations file (default HDFS)
- `--table`: Target Iceberg table (default: `hadoop_catalog.lh.bronze.open_meteo_hourly`)

**Logic**:
1. Load danh sÃ¡ch locations tá»« JSONL (CSV-like format, line-by-line JSON)
2. TÃ¡ch date range thÃ nh chunks nhá» (vÃ­ dá»¥ 90 ngÃ y chunks trÃ¡nh API timeout)
3. Vá»›i má»—i (location, chunk):
   - Check xem data Ä‘Ã£ tá»“n táº¡i khÃ´ng (trá»« khi `--override`)
   - Gá»i Open-Meteo API fetch hourly data cho khoáº£ng thá»i gian
   - Chuáº©n hÃ³a: NaN â†’ NULL, cast sang type Ä‘Ãºng (AQI â†’ Int, PM â†’ Double)
   - Deduplicate: `drop_duplicates(['location_key', 'ts_utc'])`
   - Append vÃ o Iceberg table (khÃ´ng overwrite)
4. Sleep 1s giá»¯a API calls (trÃ¡nh rate limit)

**VÃ­ dá»¥**:
```bash
# Backfill Q1 2024 (3 months)
bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- \
  --mode backfill \
  --start-date 2024-01-01 \
  --end-date 2024-03-31 \
  --chunk-days 30

# Reprocess January (override old data)
bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- \
  --mode backfill \
  --start-date 2024-01-01 \
  --end-date 2024-01-31 \
  --override
```

### ğŸ”§ Mode: Upsert (Daily Update)

Cáº­p nháº­t data má»›i hÃ ng ngÃ y. DÃ¹ng sau backfill Ä‘á»ƒ maintain fresh data.

```bash
# Local Spark
python3 jobs/bronze/run_bronze_pipeline.py --mode upsert

# YARN Cluster
bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- --mode upsert
```

**Logic**:
1. Load danh sÃ¡ch locations
2. Vá»›i má»—i location:
   - TÃ¬m timestamp má»›i nháº¥t (MAX(ts_utc)) trong bronze table
   - **Náº¿u khÃ´ng cÃ³ data**: Skip location (cáº§n backfill trÆ°á»›c)
   - **Náº¿u cÃ³ data**: TÃ­nh range tá»« (latest_ts + 1 day) Ä‘áº¿n hÃ´m nay
   - Gá»i API fetch data cho khoáº£ng má»›i nÃ y
   - Append data má»›i (deduplicated)
3. Print summary: locations processed, total records

**Key behavior**:
- âœ… **Safe**: Chá»‰ append, khÃ´ng ghi Ä‘Ã¨ data cÅ©
- âœ… **Idempotent**: Cháº¡y láº¡i cÃ¹ng ngÃ y khÃ´ng duplicate (dedupe á»Ÿ pandas)
- âŒ **Requires backfill first**: Locations chÆ°a cÃ³ initial data sáº½ bá»‹ skip
- â±ï¸ **Daily schedule**: DÃ¹ng cho hÃ ng ngÃ y updates

**Cron setup** (cháº¡y lÃºc 02:00 AM má»—i ngÃ y):
```bash
0 2 * * * cd /home/dlhnhom2/dlh-aqi && \
  bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- --mode upsert >> logs/bronze.log 2>&1
```

### ğŸ“Š Bronze Schema (47 columns)

```python
# Coordinates & Location (5 cols)
location_key (STRING)        # e.g., "hanoi"
ts_utc (TIMESTAMP)           # Hourly timestamp UTC (partition)
date_utc (DATE)              # Partition column for efficiency
latitude (DOUBLE)
longitude (DOUBLE)

# AQI Indices per US EPA (7 cols)
aqi (INT)                    # Overall AQI 0-500 range
aqi_pm25, aqi_pm10, aqi_no2, aqi_o3, aqi_so2, aqi_co (INT)

# Pollutant Concentrations in Î¼g/mÂ³ (6 cols)
pm25, pm10, o3, no2, so2, co (DOUBLE)

# Additional Environmental Parameters (4 cols)
aod (DOUBLE)                 # Aerosol Optical Depth
dust (DOUBLE)
uv_index (DOUBLE)
co2 (DOUBLE)

# Metadata (3 cols)
model_domain (STRING)
request_timezone (STRING)
_ingested_at (TIMESTAMP)     # Ingestion timestamp for audit
```

### ğŸ“ˆ Performance Expectations

| Operation | Duration | Notes |
|-----------|----------|-------|
| Backfill 1 location, 1 month | ~30s | 720 hourly records, 1 API call |
| Backfill 1 location, 1 year (90-day chunks) | ~5 min | 4 API calls (with 1s delays) |
| Backfill 50 locations, 1 year (90-day chunks) | ~4-5h | ~200 API calls total (1s delay) |
| Upsert 50 locations, 1 day | ~2-3 min | Incremental only new data |
| Write 100K records to Iceberg | ~10-15s | Optimized with coalesce |

---

## ğŸ—ï¸ Silver Layer â€” Data Cleaning & Enrichment

### ğŸ“Œ Má»¥c Ä‘Ã­ch

**LÃ m sáº¡ch & chuáº©n hÃ³a** dá»¯ liá»‡u tá»« Bronze:
- ThÃªm dimensional keys (`date_key`, `time_key`)
- Loáº¡i bá» duplicates
- MERGE INTO cho idempotent upsert

**Káº¿t quáº£**: Báº£ng `hadoop_catalog.lh.silver.air_quality_hourly_clean` - Analytics-ready.

### ğŸ“ File

- **Script**: `jobs/silver/run_silver_pipeline.py` (313 lines)
- **Input**: `hadoop_catalog.lh.bronze.open_meteo_hourly`
- **Output**: `hadoop_catalog.lh.silver.air_quality_hourly_clean`

### ğŸ”§ Mode: Full

Rebuild toÃ n bá»™ Silver table tá»« Bronze (first-time setup).

```bash
# Local Spark
python3 jobs/silver/run_silver_pipeline.py --mode full

# YARN Cluster
bash scripts/spark_submit.sh jobs/silver/run_silver_pipeline.py -- --mode full
```

**Logic**:
1. Read **táº¥t cáº£** dá»¯ liá»‡u tá»« Bronze table
2. ThÃªm dimensional keys:
   - `date_key`: Extract tá»« date_utc, format YYYYMMDD (e.g., 20240115)
   - `time_key`: Extract hour tá»« ts_utc, format HHMM (e.g., 1400 for 14:00)
3. Drop duplicates theo (location_key, ts_utc) â€” keep first/last?
4. **Overwrite** Silver table (replace all)

**Timing**: ~45-60 giÃ¢y cho 47K records

### ğŸ”§ Mode: Incremental

Cáº­p nháº­t chá»‰ dá»¯ liá»‡u má»›i (sau Bronze upsert).

```bash
# Auto-detect new data in Bronze
python3 jobs/silver/run_silver_pipeline.py --mode incremental

# Explicit date range
bash scripts/spark_submit.sh jobs/silver/run_silver_pipeline.py -- \
  --mode incremental \
  --start-date 2024-01-01 \
  --end-date 2024-12-31
```

**Logic (auto-detect)**:
1. Find MAX(ts_utc) trong Silver table
2. Read Bronze WHERE ts_utc > max_ts
3. Transform (add keys) + Merge vÃ o Silver (upsert mode)

**Logic (explicit range)**:
1. Filter Bronze WHERE date BETWEEN start_date AND end_date
2. Transform + Merge

**Key behavior**:
- âœ… **Idempotent**: MERGE vÃ o, cháº¡y láº¡i cÃ¹ng range khÃ´ng duplicate
- âœ… **Auto-detect**: Tá»± tÃ¬m data má»›i náº¿u khÃ´ng specify date
- â±ï¸ **Daily schedule**: Cháº¡y sau Bronze upsert

**Cron setup** (cháº¡y 1h sau Bronze, lÃºc 03:00 AM):
```bash
0 3 * * * cd /home/dlhnhom2/dlh-aqi && \
  bash scripts/spark_submit.sh jobs/silver/run_silver_pipeline.py -- --mode incremental >> logs/silver.log 2>&1
```

### ğŸ“Š Silver Schema

**ThÃªm 2 cá»™t vÃ o Bronze**:
- `date_key`: INT (YYYYMMDD format)
- `time_key`: INT (HHMM format, e.g., 1400)

**Preserve**: Táº¥t cáº£ 47 cá»™t tá»« Bronze + 2 new = 49 cols total

**Deduplication key**: (location_key, ts_utc)

### ğŸ“ˆ Performance

| Operation | Duration | Records |
|-----------|----------|---------|
| Full load from Bronze | ~45-60s | 47K |
| Incremental (1 day) | ~10-15s | ~2K |
| Incremental (1 month) | ~30s | ~60K |

---

## ğŸ—ï¸ Gold Layer â€” Star Schema

### ğŸ“Œ Má»¥c Ä‘iff

**Táº¡o star schema** cho analytics: 4 dimension tables + 3 fact tables.

### ğŸ“ Files

```
jobs/gold/
â”œâ”€â”€ load_dim_date.py                    # Date dimension (365+ days)
â”œâ”€â”€ load_dim_time.py                    # Time dimension (24 hours, static)
â”œâ”€â”€ load_dim_location.py                # Location metadata
â”œâ”€â”€ load_dim_pollutant.py               # Pollutant definitions
â”œâ”€â”€ transform_fact_hourly.py            # Hourly air quality facts
â”œâ”€â”€ transform_fact_daily.py             # Daily aggregates
â””â”€â”€ detect_episodes.py                  # Episode detection algorithm
```

### ğŸ”· Dimensions (4 tables)

#### 1. dim_date (365+ records)

Date dimension vá»›i calendar attributes.

```bash
# Full load (rebuild from Silver unique dates)
bash scripts/spark_submit.sh jobs/gold/load_dim_date.py -- --mode full

# Incremental (add only new dates)
bash scripts/spark_submit.sh jobs/gold/load_dim_date.py -- --mode incremental
```

**Columns**:
- `date_key` (INT): YYYYMMDD format
- `date_value` (DATE): Actual date
- `day_of_month` (INT): 1-31
- `day_of_week` (INT): 1=Monday, 7=Sunday
- `week_of_year` (INT): 1-53
- `month` (INT): 1-12
- `month_name` (STRING): "January", "February", ...
- `quarter` (INT): 1-4
- `year` (INT): YYYY
- `is_weekend` (BOOLEAN): Saturday/Sunday = TRUE

#### 2. dim_time (24 records, static)

Má»—i giá» trong ngÃ y.

```bash
bash scripts/spark_submit.sh jobs/gold/load_dim_time.py
```

**Columns**:
- `time_key` (INT): 0, 100, 200, ..., 2300
- `time_value` (STRING): "00:00", "01:00", ..., "23:00"
- `hour` (INT): 0-23
- `work_shift` (STRING): "night" | "morning" | "afternoon" | "evening"

**Shift definitions**:
- night: 00:00-05:59 (hours 0-5)
- morning: 06:00-11:59 (hours 6-11)
- afternoon: 12:00-17:59 (hours 12-17)
- evening: 18:00-23:59 (hours 18-23)

#### 3. dim_location (N records)

Location metadata.

```bash
bash scripts/spark_submit.sh jobs/gold/load_dim_location.py -- \
  --locations hdfs://khoa-master:9000/user/dlhnhom2/data/locations.jsonl
```

**Columns**:
- `location_key` (STRING)
- `location_name` (STRING)
- `latitude` (DOUBLE)
- `longitude` (DOUBLE)
- `timezone` (STRING): e.g., "Asia/Ho_Chi_Minh"

**Input**: `data/locations.jsonl` (JSONL format, one JSON per line)

#### 4. dim_pollutant (10 records)

Pollutant definitions.

```bash
bash scripts/spark_submit.sh jobs/gold/load_dim_pollutant.py -- \
  --pollutants hdfs://khoa-master:9000/user/dlhnhom2/data/dim_pollutant.jsonl
```

**Columns**:
- `pollutant_code` (STRING): "pm25", "pm10", "o3", etc.
- `display_name` (STRING): "Fine Particulate Matter", etc.
- `unit_default` (STRING): "Î¼g/mÂ³", etc.
- `aqi_timespan` (STRING, nullable): Averaging period for AQI

**Input**: `data/dim_pollutant.jsonl` (JSONL format)

### ğŸ”¶ Facts (3 tables)

#### 1. fact_air_quality_hourly (47K+ records)

Hourly measurements with enrichments.

```bash
# Full load
bash scripts/spark_submit.sh jobs/gold/transform_fact_hourly.py -- --mode full

# Incremental (auto-detect new Silver data)
bash scripts/spark_submit.sh jobs/gold/transform_fact_hourly.py -- --mode incremental

# Explicit date range
bash scripts/spark_submit.sh jobs/gold/transform_fact_hourly.py -- \
  --mode incremental \
  --start-date 2024-01-01 \
  --end-date 2024-12-31
```

**Enrichments** (added from Silver):
- `dominant_pollutant`: argmax(aqi_pm25, aqi_pm10, aqi_o3, aqi_no2, aqi_so2, aqi_co)
  - Which pollutant drives overall AQI?
- `data_completeness`: (non-null pollutant columns / 10) * 100%
  - How much data we have for this hour?
- `record_id`: UUID for audit trail

**Schema**: Silver columns + enrichments (49 + 3 = 52 cols)

**Size**: ~47K records (all hours Ã— all locations)

#### 2. fact_city_daily (1.9K+ records)

Daily aggregates: max AQI, hour counts by category.

```bash
bash scripts/spark_submit.sh jobs/gold/transform_fact_daily.py -- \
  --mode [full|incremental] \
  --start-date 2024-01-01 \
  --end-date 2024-12-31
```

**Key Columns**:
- `location_key`, `date_utc`, `date_key`
- `aqi_daily_max`: MAX(aqi) per (location, date)
- `dominant_pollutant_daily`: Pollutant with highest AQI
- `hours_in_cat_*`: Hour counts by AQI category:
  - `hours_in_cat_good`: AQI 0-50 (how many hours)
  - `hours_in_cat_moderate`: AQI 51-100
  - `hours_in_cat_usg`: AQI 101-150 (Unhealthy for Sensitive Groups)
  - `hours_in_cat_unhealthy`: AQI 151-200
  - `hours_in_cat_very_unhealthy`: AQI 201-300
  - `hours_in_cat_hazardous`: AQI 301+
- `hours_measured`: Count of non-null AQI hours
- `data_completeness`: (hours_measured / 24) * 100%

**Size**: ~1.9K records (1 per location per date)

#### 3. fact_episode (396 episodes)

High AQI episodes: sustained periods â‰¥151 AQI for â‰¥4h.

```bash
# Default: AQI >= 151, duration >= 4 hours
bash scripts/spark_submit.sh jobs/gold/detect_episodes.py -- --mode full

# Custom thresholds
bash scripts/spark_submit.sh jobs/gold/detect_episodes.py -- \
  --mode full \
  --aqi-threshold 200 \
  --min-hours 6

# Incremental detection
bash scripts/spark_submit.sh jobs/gold/detect_episodes.py -- --mode incremental
```

**Key Columns**:
- `episode_id` (UUID): Unique identifier
- `location_key`, `start_ts_utc`, `end_ts_utc`
- `duration_hours`: Inclusive duration (end - start + 1)
- `peak_aqi`: MAX(aqi) during episode
- `hours_flagged`: Count hours in episode
- `dominant_pollutant`: Pollutant with highest AQI
- `rule_code`: e.g., "AQI>=151_4h"

**Algorithm**:
1. Flag hours where AQI â‰¥ threshold
2. Identify runs of consecutive flagged hours
3. Filter runs â‰¥ min_hours duration
4. Aggregate each run with metrics

**Size**: ~396 episodes (year data)

---

## âš™ï¸ Configuration

### Environment Variables (`.env`)

```bash
# Iceberg Warehouse
WAREHOUSE_URI=hdfs://khoa-master:9000/warehouse/iceberg

# Spark Master (leave empty for local Spark)
SPARK_MASTER=yarn

# Data Input Files
LOCATIONS_FILE=hdfs://khoa-master:9000/user/dlhnhom2/data/locations.jsonl
POLLUTANTS_FILE=hdfs://khoa-master:9000/user/dlhnhom2/data/dim_pollutant.jsonl
```

### Locations File Format

`data/locations.jsonl` â€” JSONL format (one JSON per line):

```jsonl
{"location_key":"hanoi","location_name":"HÃ  Ná»™i","latitude":21.0278,"longitude":105.8342,"timezone":"Asia/Ho_Chi_Minh"}
{"location_key":"hcm","location_name":"TP. Há»“ ChÃ­ Minh","latitude":10.7769,"longitude":106.7009,"timezone":"Asia/Ho_Chi_Minh"}
{"location_key":"danang","location_name":"ÄÃ  Náºµng","latitude":16.0544,"longitude":108.2022,"timezone":"Asia/Ho_Chi_Minh"}
```

**Required fields**: location_key, latitude, longitude
**Optional fields**: location_name, timezone

---

## ğŸ“ˆ Performance & Monitoring

### Expected Timing

| Operation | Duration | Notes |
|-----------|----------|-------|
| Bronze backfill 1 year (50 loc) | 4-5h | ~200 API calls with 1s delay |
| Silver full load | 45-60s | 47K records transform + dedupe |
| Silver incremental (1 day) | 10-15s | ~2K records |
| Gold dimensions load | ~30s | One-time only |
| Gold fact_hourly | ~1 min | Transform + UUID generation |
| Gold fact_daily aggregation | ~30s | GroupBy + aggregate |
| Gold episode detection | ~20s | Window function + flagging |
| **Total daily pipeline** | **~15 min** | All 3 layers incremental |

### Monitoring Queries

```bash
# Bronze: Row count by date
spark-sql -e "
  SELECT date_utc, COUNT(*) as count 
  FROM hadoop_catalog.lh.bronze.open_meteo_hourly 
  GROUP BY date_utc ORDER BY date_utc DESC LIMIT 10
"

# Silver: Deduplication check
spark-sql -e "
  SELECT 
    location_key,
    COUNT(*) as silver_records,
    COUNT(DISTINCT ts_utc) as unique_ts
  FROM hadoop_catalog.lh.silver.air_quality_hourly_clean
  GROUP BY location_key LIMIT 5
"

# Gold: Fact record counts
spark-sql -e "
  SELECT 
    'hourly' as fact_table,
    COUNT(*) as record_count
  FROM hadoop_catalog.lh.gold.fact_air_quality_hourly
  UNION ALL
  SELECT 'daily', COUNT(*) FROM hadoop_catalog.lh.gold.fact_city_daily
  UNION ALL
  SELECT 'episodes', COUNT(*) FROM hadoop_catalog.lh.gold.fact_episode
"

# Data quality: AQI distribution
spark-sql -e "
  SELECT 
    ROUND(aqi/50)*50 as aqi_bucket, 
    COUNT(*) as count
  FROM hadoop_catalog.lh.gold.fact_air_quality_hourly
  GROUP BY ROUND(aqi/50)*50
  ORDER BY aqi_bucket DESC
"
```

### Logs

```bash
# Real-time logs
tail -f logs/bronze.log logs/silver.log logs/gold_*.log

# Spark cluster status
yarn application -list
yarn application -status <app_id>

# HDFS usage
hdfs dfs -du -sh /warehouse/iceberg/hadoop_catalog/lh/
```

---

## ğŸ› Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| `Connection timeout` | API rate limit or network | Increase `--chunk-days`, reduce parallel locations |
| `MERGE fails: table not found` | Table not initialized | Run schema creation script first |
| `No new data detected` | Silver already up-to-date | Check Bronze upsert completed before Silver |
| `Memory error: Java heap` | Large date range processing | Reduce `--end-date` or increase executor memory |
| `HDFS safe mode` | Cluster maintenance | `hdfs dfsadmin -safemode leave` |
| `Duplicate records in output` | Dedup failed or skipped | Already handled by pandas `drop_duplicates()` |
| `NaN values in output` | API returned missing values | Expected, preserved as NULL in Iceberg |
| `Slow Spark performance` | Inefficient partitioning | Check Iceberg partition scheme, increase parallelism |

---

## ğŸ”„ Recommended Workflow

### ğŸ”§ Setup (One-time, ~6-8 hours)

```bash
# Phase 1: Backfill Bronze (4-5 hours)
for quarter in Q1 Q2 Q3 Q4; do
  bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- \
    --mode backfill \
    --start-date "2024-${quarter}-01" \
    --end-date "2024-${quarter}-30" \
    --chunk-days 30 &
done
wait

# Phase 2: Transform Silver (1 min)
bash scripts/spark_submit.sh jobs/silver/run_silver_pipeline.py -- --mode full

# Phase 3: Load Gold Dimensions (30 sec)
bash scripts/spark_submit.sh jobs/gold/load_dim_date.py -- --mode full &
bash scripts/spark_submit.sh jobs/gold/load_dim_time.py &
bash scripts/spark_submit.sh jobs/gold/load_dim_location.py &
bash scripts/spark_submit.sh jobs/gold/load_dim_pollutant.py &
wait

# Phase 4: Transform Gold Facts (2-3 min)
bash scripts/spark_submit.sh jobs/gold/transform_fact_hourly.py -- --mode full &
bash scripts/spark_submit.sh jobs/gold/transform_fact_daily.py -- --mode full &
bash scripts/spark_submit.sh jobs/gold/detect_episodes.py -- --mode full &
wait
```

### ğŸ”„ Daily (Recurring, crontab)

```bash
# 02:00 - Bronze upsert
0 2 * * * cd /home/dlhnhom2/dlh-aqi && bash scripts/spark_submit.sh jobs/bronze/run_bronze_pipeline.py -- --mode upsert >> logs/bronze.log 2>&1

# 03:00 - Silver incremental
0 3 * * * cd /home/dlhnhom2/dlh-aqi && bash scripts/spark_submit.sh jobs/silver/run_silver_pipeline.py -- --mode incremental >> logs/silver.log 2>&1

# 04:00 - Gold facts incremental (all parallel)
0 4 * * * cd /home/dlhnhom2/dlh-aqi && \
  (bash scripts/spark_submit.sh jobs/gold/transform_fact_hourly.py -- --mode incremental >> logs/gold_hourly.log 2>&1 & \
   bash scripts/spark_submit.sh jobs/gold/transform_fact_daily.py -- --mode incremental >> logs/gold_daily.log 2>&1 & \
   bash scripts/spark_submit.sh jobs/gold/detect_episodes.py -- --mode incremental >> logs/gold_episodes.log 2>&1 & \
   wait)
```

### ğŸ“Š Prefect Integration (Advanced)

```python
from Prefect.full_pipeline_flow import full_ingest_flow, bronze_ingest_task, silver_transform_task
from prefect import flow, task

@task(name="bronze_upsert", retries=2, retry_delay_seconds=300)
def bronze_task():
    result = bronze_ingest_task(mode="upsert")
    if not result["success"]:
        raise Exception(f"Bronze failed: {result['error']}")
    return result

@task(name="silver_incremental")  
def silver_task():
    result = silver_transform_task(mode="incremental")
    if not result["success"]:
        raise Exception(f"Silver failed: {result['error']}")
    return result

@task(name="gold_facts")
def gold_task():
    # Execute all 3 fact tables
    tasks = []
    # ... execute fact transforms ...
    return tasks

@flow(name="daily-aqi-pipeline")
def daily_pipeline():
    bronze = bronze_task()
    silver = silver_task()  # Only run if bronze success
    gold = gold_task()      # Only run if silver success
    return {"bronze": bronze, "silver": silver, "gold": gold}

if __name__ == "__main__":
    daily_pipeline.serve(
        name="daily-aqi-deployment",
        cron="0 2 * * *",
        tags=["production", "aqi", "incremental"]
    )
```

---

## âœ… Setup Checklist

- [ ] Install dependencies: `pip install -r requirements.txt`
- [ ] Create Iceberg schema: `bash scripts/spark_submit.sh scripts/create_lh_tables.py`
- [ ] Verify locations file: `hdfs dfs -ls /user/dlhnhom2/data/locations.jsonl`
- [ ] Test Bronze backfill (small range): `... --start-date 2024-01-01 --end-date 2024-01-07`
- [ ] Verify Bronze data: Query table in Spark SQL
- [ ] Run Silver full: `bash scripts/spark_submit.sh jobs/silver/run_silver_pipeline.py -- --mode full`
- [ ] Load Gold dimensions: Run all 4 `load_dim_*.py` scripts
- [ ] Transform Gold facts: Run all 3 `transform_fact_*.py` + `detect_episodes.py`
- [ ] Verify Gold data quality: Run monitoring queries
- [ ] Setup cron jobs for daily runs
- [ ] Setup Prefect flows (optional, for advanced scheduling)
- [ ] Monitor first week of logs for errors
- [ ] Document any customizations made to schema/parameters

---

## ğŸ“– File Structure

```
jobs/
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ run_bronze_pipeline.py          # Open-Meteo API â†’ Bronze (403 lines)
â”œâ”€â”€ silver/
â”‚   â””â”€â”€ run_silver_pipeline.py          # Bronze â†’ Silver cleaning (313 lines)
â””â”€â”€ gold/
    â”œâ”€â”€ load_dim_date.py                # Static date dimension
    â”œâ”€â”€ load_dim_time.py                # Static time dimension (24h)
    â”œâ”€â”€ load_dim_location.py            # Location metadata from JSONL
    â”œâ”€â”€ load_dim_pollutant.py           # Pollutant definitions from JSONL
    â”œâ”€â”€ transform_fact_hourly.py        # Hourly air quality enrichments
    â”œâ”€â”€ transform_fact_daily.py         # Daily aggregates & categories
    â””â”€â”€ detect_episodes.py              # High AQI episode detection

scripts/
â”œâ”€â”€ spark_submit.sh                     # YARN submission wrapper
â”œâ”€â”€ create_lh_tables.py                 # Iceberg schema initialization
â””â”€â”€ cleanup_spark_staging.sh            # Temp file cleanup

data/
â”œâ”€â”€ locations.jsonl                     # Location metadata input
â””â”€â”€ dim_pollutant.jsonl                 # Pollutant metadata input

docs/ingest/
â”œâ”€â”€ README.md                           # Overview + quick start
â”œâ”€â”€ bronze.md                           # Bronze layer details
â”œâ”€â”€ silver.md                           # Silver layer details
â””â”€â”€ gold.md                             # Gold layer details
```

---

## ğŸ“ Important Notes

- âœ… **Idempotent**: Táº¥t cáº£ transforms dÃ¹ng MERGE/upsert, safe Ä‘á»ƒ re-run
- âœ… **Scalable**: Há»— trá»£ YARN dynamic allocation + Iceberg partitioning + parallel execution
- âœ… **Monitoring**: Má»—i script print metrics (records processed, time elapsed, errors)
- âœ… **Error handling**: Comprehensive exception handling + retry logic
- âœ… **Prefect-ready**: HÃ m `execute_*()` tráº£ vá» dict cho orchestration
- âœ… **Version control**: Commit khi thay Ä‘á»•i schema hoáº·c transform logic
- âœ… **Data quality**: Builtin deduplication + NULL handling + validation

---

**Last updated**: October 18, 2025
**Pipeline version**: 1.0 (Medallion Architecture)

