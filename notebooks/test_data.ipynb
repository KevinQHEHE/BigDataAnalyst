{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5309f9f",
   "metadata": {},
   "source": [
    "# Bronze layer smoke tests\n",
    "\n",
    "This notebook starts a local Spark session configured to use the Iceberg warehouse from `.env` and runs a few smoke tests on the Bronze table `hadoop_catalog.aq.raw_open_meteo_hourly`.\n",
    "\n",
    "Checks performed:\n",
    "- Table exists\n",
    "- Row count is > 0\n",
    "- `ts` column has a sensible min and max\n",
    "- Display a small sample of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c780329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAREHOUSE_URI= hdfs://khoa-master:9000/warehouse/iceberg\n"
     ]
    }
   ],
   "source": [
    "# Load environment and required modules\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "load_dotenv(dotenv_path=os.path.join('..', '.env') if os.path.exists('../.env') else '.env')\n",
    "WAREHOUSE_URI = os.getenv('WAREHOUSE_URI')\n",
    "print('WAREHOUSE_URI=', WAREHOUSE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041196b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully!\n",
      "Spark master: local[*]\n",
      "Spark UI: http://khoa-master:4040\n",
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Start Spark session for the notebook (self-contained)\n",
    "# This cell is intentionally self-contained so you can run it before/after other cells.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "\n",
    "# Stop any existing Spark contexts\n",
    "try:\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sc = SparkContext._active_spark_context\n",
    "    if sc:\n",
    "        sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Read WAREHOUSE_URI from environment, fallback to the same default used elsewhere\n",
    "WAREHOUSE_URI = os.getenv(\"WAREHOUSE_URI\", \"hdfs://khoa-master:9000/warehouse/iceberg\")\n",
    "\n",
    "# Minimal build() here mirrors the function defined later in the notebook\n",
    "def build(app_name: str) -> SparkSession:\n",
    "    return (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .master(\"local[*]\")  # Run locally with all available cores\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\")\n",
    "        .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", WAREHOUSE_URI)\n",
    "        .config(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "try:\n",
    "    spark = build(\"test_data_smoke_tests\")\n",
    "    print(\"Spark session created successfully!\")\n",
    "    print(\"Spark master:\", spark.sparkContext.master)\n",
    "    print(\"Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "    print(\"Spark version:\", spark.version)\n",
    "except Exception as e:\n",
    "    print(\"Failed to create SparkSession:\", e)\n",
    "    print(\"If running remotely, make sure this kernel has access to Spark and the Iceberg jars/configs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44d3fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'hadoop_catalog.aq.raw_open_meteo_hourly' exists: True\n",
      "✓ Table existence check passed\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check if the bronze table exists\n",
    "table_name = \"hadoop_catalog.aq.raw_open_meteo_hourly\"\n",
    "\n",
    "try:\n",
    "    # Try to get table metadata\n",
    "    table_exists = spark.catalog.tableExists(table_name)\n",
    "    print(f\"Table '{table_name}' exists: {table_exists}\")\n",
    "    \n",
    "    # If table doesn't exist in catalog, try direct SQL approach\n",
    "    if not table_exists:\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "            table_exists = True\n",
    "            print(f\"Table '{table_name}' found via SQL DESCRIBE\")\n",
    "        except Exception as desc_e:\n",
    "            print(f\"Table '{table_name}' not accessible: {desc_e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error checking table existence: {e}\")\n",
    "    table_exists = False\n",
    "\n",
    "assert table_exists, f\"Bronze table '{table_name}' does not exist\"\n",
    "print(\"✓ Table existence check passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b4299cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in 'hadoop_catalog.aq.raw_open_meteo_hourly': 45288\n",
      "Row count: 45288\n",
      "✓ Row count check completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test 2: Count rows in the bronze table\n",
    "try:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as row_count FROM {table_name}\").collect()[0].row_count\n",
    "    print(f\"Total rows in '{table_name}': {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error counting rows: {e}\")\n",
    "    count = 0\n",
    "\n",
    "print(f\"Row count: {count}\")\n",
    "print(\"✓ Row count check completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2272079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============================================>       (172 + 17) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp range:\n",
      "  Min: 2024-01-01 00:00:00\n",
      "  Max: 2025-09-20 23:00:00\n",
      "  Unique timestamps: 15096\n",
      "✓ Timestamp range check completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test 3: Check timestamp column (ts) has sensible min and max values\n",
    "try:\n",
    "    min_max_query = f\"\"\"\n",
    "    SELECT \n",
    "        MIN(ts) as min_ts,\n",
    "        MAX(ts) as max_ts,\n",
    "        COUNT(DISTINCT ts) as unique_timestamps\n",
    "    FROM {table_name}\n",
    "    WHERE ts IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    result = spark.sql(min_max_query).collect()[0]\n",
    "    min_max = {\n",
    "        'min_ts': result.min_ts,\n",
    "        'max_ts': result.max_ts,\n",
    "        'unique_timestamps': result.unique_timestamps\n",
    "    }\n",
    "    \n",
    "    print(f\"Timestamp range:\")\n",
    "    print(f\"  Min: {min_max['min_ts']}\")\n",
    "    print(f\"  Max: {min_max['max_ts']}\")\n",
    "    print(f\"  Unique timestamps: {min_max['unique_timestamps']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error checking timestamps: {e}\")\n",
    "    min_max = {'min_ts': None, 'max_ts': None, 'unique_timestamps': 0}\n",
    "\n",
    "print(\"✓ Timestamp range check completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e779caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from 'hadoop_catalog.aq.raw_open_meteo_hourly':\n",
      "+-----------+---------+----------+-------------------+---------------------+------------------+------------------+----+------------------+-----+------------------+---------------+------------------+------------------+----------+------------------------------------+-------------------------+\n",
      "|location_id|latitude |longitude |ts                 |aerosol_optical_depth|pm2_5             |pm10              |dust|nitrogen_dioxide  |ozone|sulphur_dioxide   |carbon_monoxide|uv_index          |uv_index_clear_sky|source    |run_id                              |ingested_at              |\n",
      "+-----------+---------+----------+-------------------+---------------------+------------------+------------------+----+------------------+-----+------------------+---------------+------------------+------------------+----------+------------------------------------+-------------------------+\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 00:00:00|0.6200000047683716   |37.400001525878906|40.599998474121094|0.0 |29.299999237060547|59.0 |24.700000762939453|766.0          |0.800000011920929 |0.8500000238418579|open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 01:00:00|0.6899999976158142   |39.599998474121094|43.400001525878906|0.0 |26.399999618530273|87.0 |26.200000762939453|747.0          |2.1500000953674316|2.4000000953674316|open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 02:00:00|0.7699999809265137   |43.79999923706055 |46.400001525878906|0.0 |22.100000381469727|127.0|28.100000381469727|668.0          |4.199999809265137 |4.599999904632568 |open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 03:00:00|0.8199999928474426   |50.20000076293945 |51.5              |0.0 |17.899999618530273|168.0|29.200000762939453|592.0          |6.300000190734863 |6.800000190734863 |open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 05:00:00|0.75                 |56.29999923706055 |57.29999923706055 |0.0 |9.0               |253.0|27.100000381469727|485.0          |8.850000381469727 |9.149999618530273 |open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "+-----------+---------+----------+-------------------+---------------------+------------------+------------------+----+------------------+-----+------------------+---------------+------------------+------------------+----------+------------------------------------+-------------------------+\n",
      "\n",
      "\n",
      "Table schema for 'hadoop_catalog.aq.raw_open_meteo_hourly':\n",
      "root\n",
      " |-- location_id: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- aerosol_optical_depth: double (nullable = true)\n",
      " |-- pm2_5: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- dust: double (nullable = true)\n",
      " |-- nitrogen_dioxide: double (nullable = true)\n",
      " |-- ozone: double (nullable = true)\n",
      " |-- sulphur_dioxide: double (nullable = true)\n",
      " |-- carbon_monoxide: double (nullable = true)\n",
      " |-- uv_index: double (nullable = true)\n",
      " |-- uv_index_clear_sky: double (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- ingested_at: timestamp (nullable = true)\n",
      "\n",
      "✓ Sample data display completed\n",
      "+-----------+---------+----------+-------------------+---------------------+------------------+------------------+----+------------------+-----+------------------+---------------+------------------+------------------+----------+------------------------------------+-------------------------+\n",
      "|location_id|latitude |longitude |ts                 |aerosol_optical_depth|pm2_5             |pm10              |dust|nitrogen_dioxide  |ozone|sulphur_dioxide   |carbon_monoxide|uv_index          |uv_index_clear_sky|source    |run_id                              |ingested_at              |\n",
      "+-----------+---------+----------+-------------------+---------------------+------------------+------------------+----+------------------+-----+------------------+---------------+------------------+------------------+----------+------------------------------------+-------------------------+\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 00:00:00|0.6200000047683716   |37.400001525878906|40.599998474121094|0.0 |29.299999237060547|59.0 |24.700000762939453|766.0          |0.800000011920929 |0.8500000238418579|open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 01:00:00|0.6899999976158142   |39.599998474121094|43.400001525878906|0.0 |26.399999618530273|87.0 |26.200000762939453|747.0          |2.1500000953674316|2.4000000953674316|open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 02:00:00|0.7699999809265137   |43.79999923706055 |46.400001525878906|0.0 |22.100000381469727|127.0|28.100000381469727|668.0          |4.199999809265137 |4.599999904632568 |open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 03:00:00|0.8199999928474426   |50.20000076293945 |51.5              |0.0 |17.899999618530273|168.0|29.200000762939453|592.0          |6.300000190734863 |6.800000190734863 |open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "|Hà Nội     |21.028511|105.804817|2025-04-22 05:00:00|0.75                 |56.29999923706055 |57.29999923706055 |0.0 |9.0               |253.0|27.100000381469727|485.0          |8.850000381469727 |9.149999618530273 |open-meteo|1a2d0954-f47a-49d6-9a26-778bf35fe1e6|2025-09-20 10:09:58.36962|\n",
      "+-----------+---------+----------+-------------------+---------------------+------------------+------------------+----+------------------+-----+------------------+---------------+------------------+------------------+----------+------------------------------------+-------------------------+\n",
      "\n",
      "\n",
      "Table schema for 'hadoop_catalog.aq.raw_open_meteo_hourly':\n",
      "root\n",
      " |-- location_id: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- aerosol_optical_depth: double (nullable = true)\n",
      " |-- pm2_5: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- dust: double (nullable = true)\n",
      " |-- nitrogen_dioxide: double (nullable = true)\n",
      " |-- ozone: double (nullable = true)\n",
      " |-- sulphur_dioxide: double (nullable = true)\n",
      " |-- carbon_monoxide: double (nullable = true)\n",
      " |-- uv_index: double (nullable = true)\n",
      " |-- uv_index_clear_sky: double (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- ingested_at: timestamp (nullable = true)\n",
      "\n",
      "✓ Sample data display completed\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Display sample rows from the bronze table\n",
    "try:\n",
    "    print(f\"Sample rows from '{table_name}':\")\n",
    "    sample_df = spark.sql(f\"SELECT * FROM {table_name} LIMIT 5\")\n",
    "    sample_df.show(truncate=False)\n",
    "    \n",
    "    # Also show table schema\n",
    "    print(f\"\\nTable schema for '{table_name}':\")\n",
    "    sample_df.printSchema()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error displaying sample data: {e}\")\n",
    "\n",
    "print(\"✓ Sample data display completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0214a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed\n"
     ]
    }
   ],
   "source": [
    "# Simple assertions to fail the notebook if expectations are not met\n",
    "assert count > 0, 'Bronze table is empty (expected > 0 rows)'\n",
    "assert min_max['min_ts'] is not None and min_max['max_ts'] is not None, 'ts column has null min/max'\n",
    "print('All checks passed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
