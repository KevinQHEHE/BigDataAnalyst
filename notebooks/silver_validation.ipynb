{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Silver Layer Validation\n",
        "Queries to sanity-check Bronze → Silver transformations. Adjust the parameters below, re-run, and review counts, null rates, and detailed windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from aq_lakehouse.spark_session import build\n",
        "\n",
        "spark = build(\"silver_validation_notebook\")\n",
        "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the baseline window\n",
        "START_TS = datetime(2024, 1, 1, 0, 0, tzinfo=timezone.utc)\n",
        "END_TS = datetime(2024, 1, 7, 23, 0, tzinfo=timezone.utc)\n",
        "TARGET_LOCATION = \"Hà Nội\"  # set to None to inspect all locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_range(table: str, ts_column: str, start_ts=START_TS, end_ts=END_TS, location_id=TARGET_LOCATION):\n",
        "    df = spark.table(table).where((F.col(ts_column) >= F.lit(start_ts)) & (F.col(ts_column) <= F.lit(end_ts)))\n",
        "    if location_id:\n",
        "        df = df.where(F.col(\"location_id\") == location_id)\n",
        "    return df\n",
        "\n",
        "def counts_by_day_location(table: str, ts_column: str):\n",
        "    df = load_range(table, ts_column)\n",
        "    if df.rdd.isEmpty():\n",
        "        print(f\"{table}: no rows in selected window\")\n",
        "        return\n",
        "    (\n",
        "        df.withColumn(\"date_utc\", F.to_date(F.col(ts_column)))\n",
        "          .groupBy(\"location_id\", \"date_utc\")\n",
        "          .count()\n",
        "          .orderBy(\"location_id\", \"date_utc\")\n",
        "          .show(truncate=False)\n",
        "    )\n",
        "\n",
        "def null_rates(df):\n",
        "    total = df.count()\n",
        "    if total == 0:\n",
        "        print(\"No rows -> no null stats\")\n",
        "        return\n",
        "    metrics = [\n",
        "        F.avg(F.when(F.col(c).isNull(), 1.0).otherwise(0.0)).alias(c)\n",
        "        for c in df.columns\n",
        "    ]\n",
        "    df.select(metrics).show(vertical=True, truncate=False)\n",
        "\n",
        "def sample_window(table: str, ts_column: str, hours: int = 24):\n",
        "    df = load_range(table, ts_column)\n",
        "    first = df.orderBy(ts_column).select(ts_column).limit(1).collect()\n",
        "    if not first:\n",
        "        print(f\"{table}: no rows available\")\n",
        "        return\n",
        "    start = first[0][0]\n",
        "    end = start + timedelta(hours=hours - 1)\n",
        "    (\n",
        "        df.where((F.col(ts_column) >= F.lit(start)) & (F.col(ts_column) <= F.lit(end)))\n",
        "          .orderBy(ts_column)\n",
        "          .show(truncate=False)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counts by day & location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counts_by_day_location(\"hadoop_catalog.aq.raw_open_meteo_hourly\", \"ts\")\n",
        "counts_by_day_location(\"hadoop_catalog.aq.silver.air_quality_hourly_clean\", \"ts_utc\")\n",
        "counts_by_day_location(\"hadoop_catalog.aq.silver.aq_components_hourly\", \"ts_utc\")\n",
        "counts_by_day_location(\"hadoop_catalog.aq.silver.aq_index_hourly\", \"ts_utc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Null ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_rates(load_range(\"hadoop_catalog.aq.silver.air_quality_hourly_clean\", \"ts_utc\"))\n",
        "null_rates(load_range(\"hadoop_catalog.aq.silver.aq_components_hourly\", \"ts_utc\"))\n",
        "null_rates(load_range(\"hadoop_catalog.aq.silver.aq_index_hourly\", \"ts_utc\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_window(\"hadoop_catalog.aq.silver.air_quality_hourly_clean\", \"ts_utc\", hours=24)\n",
        "sample_window(\"hadoop_catalog.aq.silver.aq_components_hourly\", \"ts_utc\", hours=24)\n",
        "sample_window(\"hadoop_catalog.aq.silver.aq_index_hourly\", \"ts_utc\", hours=24)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10210958",
      "metadata": {},
      "source": [
        "## Quick SQL checks\n",
        "Tạo nhanh truy vấn SQL để đối chiếu số lượng và null-rate giữa Bronze và Silver cho cùng cửa sổ. Chỉnh sửa tuỳ ý trước khi chạy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ddeb134",
      "metadata": {},
      "outputs": [],
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "START_STR = START_TS.strftime('%Y-%m-%d %H:%M:%S')\n",
        "END_STR = END_TS.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "def daily_counts_sql(table: str, ts_col: str) -> str:\n",
        "    base = (\n",
        "        f\"SELECT location_id, DATE({ts_col}) AS date_utc, COUNT(*) AS rows\n",
        "\"\n",
        "        f\"FROM {table}\n",
        "\"\n",
        "        f\"WHERE {ts_col} BETWEEN TIMESTAMP '{START_STR}'\n",
        "\"\n",
        "        f\"                   AND TIMESTAMP '{END_STR}'\"\n",
        "    )\n",
        "    if TARGET_LOCATION:\n",
        "        base += f\"\n",
        "  AND location_id = '{TARGET_LOCATION}'\"\n",
        "    base += \"\n",
        "GROUP BY location_id, DATE({ts_col})\n",
        "ORDER BY location_id, date_utc\"\n",
        "    return base\n",
        "\n",
        "for table, ts_col in [\n",
        "    (\"hadoop_catalog.aq.raw_open_meteo_hourly\", \"ts\"),\n",
        "    (\"hadoop_catalog.aq.silver.air_quality_hourly_clean\", \"ts_utc\"),\n",
        "    (\"hadoop_catalog.aq.silver.aq_components_hourly\", \"ts_utc\"),\n",
        "    (\"hadoop_catalog.aq.silver.aq_index_hourly\", \"ts_utc\"),\n",
        "]:\n",
        "    sql = daily_counts_sql(table, ts_col)\n",
        "    print(f\"\n",
        "-- {table}\n",
        "{sql}\n",
        "\")\n",
        "    spark.sql(sql).show(truncate=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
