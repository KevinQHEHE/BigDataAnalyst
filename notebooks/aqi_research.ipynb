{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20645627",
   "metadata": {},
   "source": [
    "# Vietnam Air Quality Lakehouse – Research Notebook\n",
    "\n",
    "This notebook documents the end-to-end analytical workflow for the Vietnam Air Quality (AQI) lakehouse. It is written for data scientists and analytics engineers who need traceable, reproducible results grounded in the Gold layer of the lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d33615",
   "metadata": {},
   "source": [
    "## Run Book\n",
    "\n",
    "1. Environment setup & Spark session\n",
    "2. Research questions & KPI specification\n",
    "3. Data scope confirmation\n",
    "4. Snapshot freezing for reproducibility\n",
    "5. Data quality (Great Expectations)\n",
    "6. Analytical dataset materialisation\n",
    "7. Exploratory data analysis (EDA)\n",
    "8. Diagnostic analytics\n",
    "9. Statistical inference (bootstrap + Mann–Whitney)\n",
    "10. Anomaly detection\n",
    "11. Nowcasting model (GBDT, 1–6 h horizons)\n",
    "12. Sensitivity analysis\n",
    "13. Insights & operational recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40888795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /home/dlhnhom2/dlh-aqi\n",
      "Figures directory: /home/dlhnhom2/dlh-aqi/reports/figures\n",
      "Tables directory: /home/dlhnhom2/dlh-aqi/reports/tables\n"
     ]
    }
   ],
   "source": [
    "# Environment bootstrap\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "plt.rcParams[\"legend.frameon\"] = False\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "if REPO_ROOT.name == \"notebooks\":\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "SRC_DIR = REPO_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "FIG_DIR = REPO_ROOT / \"reports\" / \"figures\"\n",
    "TABLE_DIR = REPO_ROOT / \"reports\" / \"tables\"\n",
    "for directory in (FIG_DIR, TABLE_DIR):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Figures directory: {FIG_DIR}\")\n",
    "print(f\"Tables directory: {TABLE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d3650d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 17:14:36 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master khoa-master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:110)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Failed to connect to khoa-master/172.18.0.5:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:305)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:225)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:237)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:207)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: khoa-master/172.18.0.5:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:973)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:784)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:14:56 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master khoa-master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:110)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Failed to connect to khoa-master/172.18.0.5:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:305)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:225)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:237)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:207)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: khoa-master/172.18.0.5:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:973)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:784)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:14:56 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master khoa-master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:110)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Failed to connect to khoa-master/172.18.0.5:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:305)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:225)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:237)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:207)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: khoa-master/172.18.0.5:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:973)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:784)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:15:16 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master khoa-master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:110)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Failed to connect to khoa-master/172.18.0.5:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:305)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:225)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:237)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:207)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: khoa-master/172.18.0.5:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:973)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:784)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:15:16 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master khoa-master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:110)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Failed to connect to khoa-master/172.18.0.5:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:305)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:225)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:237)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:207)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: khoa-master/172.18.0.5:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:973)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:336)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:784)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:15:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "25/10/01 17:15:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "25/10/01 17:15:36 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n",
      "25/10/01 17:15:36 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "And it was stopped at:\n",
      "\n",
      "org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2284)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\n",
      "\tat org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:2124)\n",
      "\tat org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2938)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:688)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:15:36 ERROR AsyncEventQueue: Listener AppStatusListener threw an exception\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.status.api.v1.ApplicationInfo.attempts()\" because the return value of \"org.apache.spark.status.AppStatusListener.appInfo()\" is null\n",
      "\tat org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:193)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:118)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:102)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:106)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:106)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:97)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:97)\n",
      "WARNING:root:Failed to build cluster Spark session (falling back to local notebook Spark): An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "And it was stopped at:\n",
      "\n",
      "org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2284)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\n",
      "\tat org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:2124)\n",
      "\tat org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2938)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:688)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/10/01 17:15:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "25/10/01 17:15:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "25/10/01 17:15:36 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n",
      "25/10/01 17:15:36 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "And it was stopped at:\n",
      "\n",
      "org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2284)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\n",
      "\tat org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:2124)\n",
      "\tat org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2938)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:688)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:15:36 ERROR AsyncEventQueue: Listener AppStatusListener threw an exception\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.status.api.v1.ApplicationInfo.attempts()\" because the return value of \"org.apache.spark.status.AppStatusListener.appInfo()\" is null\n",
      "\tat org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:193)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:118)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:102)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:106)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:106)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:97)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:97)\n",
      "WARNING:root:Failed to build cluster Spark session (falling back to local notebook Spark): An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "And it was stopped at:\n",
      "\n",
      "org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2284)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\n",
      "\tat org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:2124)\n",
      "\tat org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2938)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:688)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0; master: local[2]\n",
      "Warehouse: hdfs://khoa-master:9000/warehouse/iceberg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 17:15:36 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "# Spark session initialisation\n",
    "from aq_lakehouse.spark_session import build\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql import types as T\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "\n",
    "APP_NAME = \"aqi_research_notebook\"\n",
    "\n",
    "# Ensure REPO_ROOT is defined if this cell is run standalone\n",
    "try:\n",
    "    REPO_ROOT  # type: ignore\n",
    "except NameError:\n",
    "    REPO_ROOT = Path.cwd().resolve()\n",
    "    if REPO_ROOT.name == \"notebooks\":\n",
    "        REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "# Defensive cleanup: if a stopped SparkContext object is lingering, clear it\n",
    "# to avoid the `Cannot call methods on a stopped SparkContext` Py4J error.\n",
    "try:\n",
    "    active_sc = SparkContext._active_spark_context\n",
    "    if active_sc is not None:\n",
    "        try:\n",
    "            active_sc.stop()\n",
    "        except Exception:\n",
    "            # best-effort stop\n",
    "            pass\n",
    "        try:\n",
    "            SparkContext._active_spark_context = None\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    # ignore if Spark is not available or attribute access fails\n",
    "    pass\n",
    "\n",
    "# For notebooks, always use HDFS warehouse to access actual lakehouse data\n",
    "HDFS_WAREHOUSE = os.getenv(\"WAREHOUSE_URI\", \"hdfs://khoa-master:9000/warehouse/iceberg\")\n",
    "\n",
    "# Try building the production-style Spark session (will pick up cluster settings).\n",
    "# If that fails (unreachable master, misconfigured cluster), fall back to a local session\n",
    "# for interactive notebook work. This keeps production code unchanged while allowing\n",
    "# researcher-friendly local execution.\n",
    "try:\n",
    "    spark = build(APP_NAME)\n",
    "    logging.info(f\"Built Spark session with master={spark.sparkContext.master}\")\n",
    "except Exception as exc:\n",
    "    logging.warning(\n",
    "        \"Failed to build cluster Spark session (falling back to local notebook Spark): %s\",\n",
    "        exc,\n",
    "    )\n",
    "    # Local Spark session for notebook: Use HDFS warehouse to access actual data\n",
    "    # Ensure any partial/failed contexts are cleared before creating a local session\n",
    "    try:\n",
    "        active_sc = SparkContext._active_spark_context\n",
    "        if active_sc is not None:\n",
    "            try:\n",
    "                active_sc.stop()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                SparkContext._active_spark_context = None\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.master(\"local[2]\")\n",
    "        .appName(APP_NAME + \"_local\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\")\n",
    "        .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", HDFS_WAREHOUSE)\n",
    "        .config(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}; master: {spark.sparkContext.master}\")\n",
    "print(f\"Warehouse: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0418dce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All required Gold tables are present.\n",
      "✅ All optional Silver tables are also present.\n"
     ]
    }
   ],
   "source": [
    "# Iceberg table catalogue (Gold/Silver/Snapshot targets)\n",
    "TABLES = {\n",
    "    \"fact_hourly\": \"hadoop_catalog.aq.gold.fact_air_quality_hourly\",\n",
    "    \"dim_location\": \"hadoop_catalog.aq.gold.dim_location\",\n",
    "    \"dim_date\": \"hadoop_catalog.aq.gold.dim_calendar_date\",\n",
    "    \"dim_time\": \"hadoop_catalog.aq.gold.dim_calendar_time\",\n",
    "    \"silver_clean\": \"hadoop_catalog.aq.silver.air_quality_hourly_clean\",\n",
    "    \"silver_components\": \"hadoop_catalog.aq.silver.aq_components_hourly\",\n",
    "    \"silver_index\": \"hadoop_catalog.aq.silver.aq_index_hourly\",\n",
    "}\n",
    "\n",
    "# Check which tables are required (Gold) vs optional (Silver for advanced analysis)\n",
    "required_tables = [\"fact_hourly\", \"dim_location\", \"dim_date\", \"dim_time\"]\n",
    "optional_tables = [\"silver_clean\", \"silver_components\", \"silver_index\"]\n",
    "\n",
    "missing_required = [TABLES[name] for name in required_tables if not spark.catalog.tableExists(TABLES[name])]\n",
    "missing_optional = [TABLES[name] for name in optional_tables if not spark.catalog.tableExists(TABLES[name])]\n",
    "\n",
    "# If running locally (not connected to the cluster), warn and continue so\n",
    "# the notebook can be used for development. If running against a real cluster,\n",
    "# be strict and raise an error so the user knows the required tables are missing.\n",
    "is_local = str(spark.sparkContext.master).lower().startswith(\"local\")\n",
    "\n",
    "if missing_required:\n",
    "    message = (\n",
    "        \"Missing required Gold tables: \" + \", \".join(missing_required)\n",
    "        + \".\\nRun the Bronze → Silver → Gold lakehouse jobs before executing this research notebook.\"\n",
    "    )\n",
    "    if is_local:\n",
    "        import warnings\n",
    "        warnings.warn(message)\n",
    "        print(\"Warning: running in local mode; continuing without required Gold tables.\")\n",
    "        print(\"Missing required tables:\", missing_required)\n",
    "        TABLES_AVAILABLE = False\n",
    "    else:\n",
    "        raise RuntimeError(message)\n",
    "else:\n",
    "    TABLES_AVAILABLE = True\n",
    "    print(\"✅ All required Gold tables are present.\")\n",
    "    \n",
    "    if missing_optional:\n",
    "        import warnings\n",
    "        warnings.warn(f\"Optional Silver tables not found (snapshots will be skipped): {', '.join(missing_optional)}\")\n",
    "        print(f\"⚠️  Optional Silver tables missing: {len(missing_optional)}\")\n",
    "    else:\n",
    "        print(\"✅ All optional Silver tables are also present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57a1c5",
   "metadata": {},
   "source": [
    "## 1. Research Questions & KPI Blueprint\n",
    "\n",
    "Core guiding question: *How do hourly and daily AQI patterns vary across Vietnamese cities by season, and which pollutant components dominate periods of elevated AQI?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad5300d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KPI</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Computation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Median AQI (daily)</td>\n",
       "      <td>Track central tendency while limiting sensitiv...</td>\n",
       "      <td>median(aqi) per (location, date)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AQI category share</td>\n",
       "      <td>Quantify proportion of hours spent in regulato...</td>\n",
       "      <td>count(category == c) / total_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PM2.5 exceedance hours</td>\n",
       "      <td>Highlight health-relevant threshold breaches (...</td>\n",
       "      <td>count(pm25 &gt; threshold)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data completeness</td>\n",
       "      <td>Ensure operational coverage for each city/day.</td>\n",
       "      <td>hours_with_aqi / 24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      KPI                                            Purpose  \\\n",
       "0      Median AQI (daily)  Track central tendency while limiting sensitiv...   \n",
       "1      AQI category share  Quantify proportion of hours spent in regulato...   \n",
       "2  PM2.5 exceedance hours  Highlight health-relevant threshold breaches (...   \n",
       "3       Data completeness     Ensure operational coverage for each city/day.   \n",
       "\n",
       "                          Computation  \n",
       "0    median(aqi) per (location, date)  \n",
       "1  count(category == c) / total_hours  \n",
       "2             count(pm25 > threshold)  \n",
       "3                 hours_with_aqi / 24  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpi_blueprint = pd.DataFrame([\n",
    "    {\n",
    "        \"KPI\": \"Median AQI (daily)\",\n",
    "        \"Purpose\": \"Track central tendency while limiting sensitivity to spikes.\",\n",
    "        \"Computation\": \"median(aqi) per (location, date)\",\n",
    "    },\n",
    "    {\n",
    "        \"KPI\": \"AQI category share\",\n",
    "        \"Purpose\": \"Quantify proportion of hours spent in regulatory categories.\",\n",
    "        \"Computation\": \"count(category == c) / total_hours\",\n",
    "    },\n",
    "    {\n",
    "        \"KPI\": \"PM2.5 exceedance hours\",\n",
    "        \"Purpose\": \"Highlight health-relevant threshold breaches (>= 35 µg/m³).\",\n",
    "        \"Computation\": \"count(pm25 > threshold)\",\n",
    "    },\n",
    "    {\n",
    "        \"KPI\": \"Data completeness\",\n",
    "        \"Purpose\": \"Ensure operational coverage for each city/day.\",\n",
    "        \"Computation\": \"hours_with_aqi / 24\",\n",
    "    },\n",
    "])\n",
    "\n",
    "kpi_blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a2cc2",
   "metadata": {},
   "source": [
    "## 2. Data Scope Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d793860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>   (431 + 2) / 457]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope overview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_ts_utc</th>\n",
       "      <th>max_ts_utc</th>\n",
       "      <th>locations</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>3</td>\n",
       "      <td>43779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  min_ts_utc max_ts_utc  locations   rows\n",
       "0 2024-01-01 2025-08-31          3  43779"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active monitoring locations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_key</th>\n",
       "      <th>location_name</th>\n",
       "      <th>admin1</th>\n",
       "      <th>country</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      location_key    location_name admin1 country timezone\n",
       "0           Hà Nội           Hà Nội   None    None     None\n",
       "1  TP. Hồ Chí Minh  TP. Hồ Chí Minh   None    None     None\n",
       "2          Đà Nẵng          Đà Nẵng   None    None     None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Scope Confirmation\n",
    "# Load tables when available; otherwise create empty placeholder DataFrames so the\n",
    "# rest of the notebook can run in local/dev mode without Iceberg.\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "if 'TABLES_AVAILABLE' in globals() and TABLES_AVAILABLE:\n",
    "    fact_df = spark.table(TABLES[\"fact_hourly\"]).cache()\n",
    "    dim_location_df = spark.table(TABLES[\"dim_location\"]).cache()\n",
    "    dim_date_df = spark.table(TABLES[\"dim_date\"]).cache()\n",
    "    dim_time_df = spark.table(TABLES[\"dim_time\"]).cache()\n",
    "else:\n",
    "    # Define minimal schemas used later in the notebook\n",
    "    fact_schema = T.StructType([\n",
    "        T.StructField('location_key', T.IntegerType(), True),\n",
    "        T.StructField('ts_utc', T.TimestampType(), True),\n",
    "        T.StructField('date_utc', T.DateType(), True),\n",
    "        T.StructField('date_key', T.IntegerType(), True),\n",
    "        T.StructField('time_key', T.IntegerType(), True),\n",
    "        T.StructField('aqi', T.DoubleType(), True),\n",
    "        T.StructField('pm25', T.DoubleType(), True),\n",
    "        T.StructField('pm10', T.DoubleType(), True),\n",
    "        T.StructField('o3', T.DoubleType(), True),\n",
    "        T.StructField('no2', T.DoubleType(), True),\n",
    "        T.StructField('so2', T.DoubleType(), True),\n",
    "        T.StructField('co', T.DoubleType(), True),\n",
    "        T.StructField('category', T.StringType(), True),\n",
    "        T.StructField('dominant_pollutant', T.StringType(), True),\n",
    "        T.StructField('aod', T.DoubleType(), True),\n",
    "        T.StructField('uv_index', T.DoubleType(), True),\n",
    "        T.StructField('uv_index_clear_sky', T.DoubleType(), True),\n",
    "        T.StructField('is_validated', T.BooleanType(), True),\n",
    "        T.StructField('quality_flag', T.StringType(), True),\n",
    "        T.StructField('data_source', T.StringType(), True),\n",
    "        T.StructField('run_id', T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "    dim_location_schema = T.StructType([\n",
    "        T.StructField('location_key', T.IntegerType(), True),\n",
    "        T.StructField('location_name', T.StringType(), True),\n",
    "        T.StructField('admin1', T.StringType(), True),\n",
    "        T.StructField('country', T.StringType(), True),\n",
    "        T.StructField('timezone', T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "    dim_date_schema = T.StructType([\n",
    "        T.StructField('date_key', T.IntegerType(), True),\n",
    "        T.StructField('date_utc', T.DateType(), True),\n",
    "        T.StructField('year', T.IntegerType(), True),\n",
    "        T.StructField('month', T.IntegerType(), True),\n",
    "        T.StructField('day', T.IntegerType(), True),\n",
    "        T.StructField('dow', T.IntegerType(), True),\n",
    "        T.StructField('week', T.IntegerType(), True),\n",
    "        T.StructField('is_weekend', T.BooleanType(), True),\n",
    "        T.StructField('month_name', T.StringType(), True),\n",
    "        T.StructField('day_name', T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "    dim_time_schema = T.StructType([\n",
    "        T.StructField('time_key', T.IntegerType(), True),\n",
    "        T.StructField('hour', T.IntegerType(), True),\n",
    "        T.StructField('day_part', T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "    fact_df = spark.createDataFrame([], schema=fact_schema)\n",
    "    dim_location_df = spark.createDataFrame([], schema=dim_location_schema)\n",
    "    dim_date_df = spark.createDataFrame([], schema=dim_date_schema)\n",
    "    dim_time_df = spark.createDataFrame([], schema=dim_time_schema)\n",
    "\n",
    "    import warnings\n",
    "    warnings.warn('TABLES are not available; using empty placeholder DataFrames. Many downstream results will be empty.')\n",
    "\n",
    "# Build a small scope summary; safe when DataFrames are empty\n",
    "scope_summary = (\n",
    "    fact_df\n",
    "    .agg(\n",
    "        F.min(\"ts_utc\").alias(\"min_ts_utc\"),\n",
    "        F.max(\"ts_utc\").alias(\"max_ts_utc\"),\n",
    "        F.countDistinct(\"location_key\").alias(\"locations\"),\n",
    "        F.count(\"*\").alias(\"rows\"),\n",
    "    )\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "locations_detail = dim_location_df.select(\"location_key\", \"location_name\", \"admin1\", \"country\", \"timezone\").orderBy(\"location_key\")\n",
    "\n",
    "print(\"Scope overview:\")\n",
    "display(scope_summary)\n",
    "\n",
    "print(\"Active monitoring locations:\")\n",
    "display(locations_detail.toPandas())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c555839",
   "metadata": {},
   "source": [
    "## 3. Snapshot Freezing (Reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab210ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.gold.fact_air_quality_hourly: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to snapshot hadoop_catalog.aq.gold.fact_air_quality_hourly: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.gold.dim_location: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.gold.dim_calendar_date: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.gold.dim_calendar_date: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to snapshot hadoop_catalog.aq.gold.dim_location: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "❌ Failed to snapshot hadoop_catalog.aq.gold.dim_calendar_date: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.gold.dim_calendar_time: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.silver.air_quality_hourly_clean: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.silver.air_quality_hourly_clean: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to snapshot hadoop_catalog.aq.gold.dim_calendar_time: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "❌ Failed to snapshot hadoop_catalog.aq.silver.air_quality_hourly_clean: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.silver.aq_components_hourly: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.silver.aq_index_hourly: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n",
      "WARNING:py.warnings:/tmp/ipykernel_44504/3742824694.py:36: UserWarning: Failed to create snapshot for hadoop_catalog.aq.silver.aq_index_hourly: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "  warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to snapshot hadoop_catalog.aq.silver.aq_components_hourly: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "❌ Failed to snapshot hadoop_catalog.aq.silver.aq_index_hourly: [FAILED_TO_LOAD_ROUTINE] Failed to load routine `hadoop_catalog`.`system`.`create_snapshot_id`. SQLSTATE: 38000\n",
      "⚠️  No snapshots created (no tables available).\n"
     ]
    }
   ],
   "source": [
    "SNAPSHOT_TARGETS = [\n",
    "    (\"gold\", TABLES[\"fact_hourly\"]),\n",
    "    (\"gold\", TABLES[\"dim_location\"]),\n",
    "    (\"gold\", TABLES[\"dim_date\"]),\n",
    "    (\"gold\", TABLES[\"dim_time\"]),\n",
    "    (\"silver\", TABLES[\"silver_clean\"]),\n",
    "    (\"silver\", TABLES[\"silver_components\"]),\n",
    "    (\"silver\", TABLES[\"silver_index\"]),\n",
    "]\n",
    "\n",
    "run_ts = datetime.now(timezone.utc)\n",
    "snapshot_records = []\n",
    "\n",
    "# Skip snapshot creation in local/dev mode when TABLES are not available\n",
    "if 'TABLES_AVAILABLE' in globals() and TABLES_AVAILABLE:\n",
    "    for layer, table_name in SNAPSHOT_TARGETS:\n",
    "        # Check if table exists before creating snapshot\n",
    "        if not spark.catalog.tableExists(table_name):\n",
    "            print(f\"⚠️  Skipping snapshot for missing table: {table_name}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            result = spark.sql(\n",
    "                f\"CALL hadoop_catalog.system.create_snapshot_id(table => '{table_name}')\"\n",
    "            ).toPandas()\n",
    "            snapshot_id = int(result.loc[0, \"snapshot_id\"])\n",
    "            snapshot_records.append({\n",
    "                \"layer\": layer,\n",
    "                \"table\": table_name,\n",
    "                \"snapshot_id\": snapshot_id,\n",
    "                \"recorded_at_utc\": run_ts.isoformat(),\n",
    "            })\n",
    "            print(f\"✅ Snapshot created for {table_name}: {snapshot_id}\")\n",
    "        except Exception as e:\n",
    "            import warnings\n",
    "            warnings.warn(f\"Failed to create snapshot for {table_name}: {e}\")\n",
    "            print(f\"❌ Failed to snapshot {table_name}: {e}\")\n",
    "\n",
    "    if snapshot_records:\n",
    "        snapshot_df = pd.DataFrame(snapshot_records)\n",
    "        display(snapshot_df)\n",
    "\n",
    "        # Persist snapshot metadata into docs/system_report.md (append section)\n",
    "        system_report_path = REPO_ROOT / \"docs\" / \"system_report.md\"\n",
    "        section_header = \"## Lakehouse Snapshots (analysis freeze)\"\n",
    "        if system_report_path.exists():\n",
    "            existing = system_report_path.read_text(encoding=\"utf-8\")\n",
    "            if section_header in existing:\n",
    "                before, _, _ = existing.partition(section_header)\n",
    "                system_report_path.write_text(before.rstrip() + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        with system_report_path.open(\"a\", encoding=\"utf-8\") as handle:\n",
    "            handle.write(f\"{section_header}\\n\")\n",
    "            handle.write(f\"- Captured: {run_ts.isoformat()}\\n\")\n",
    "            for record in snapshot_records:\n",
    "                handle.write(\n",
    "                    f\"  - {record['table']} → snapshot_id={record['snapshot_id']}\\n\"\n",
    "                )\n",
    "\n",
    "        print(f\"Snapshot metadata appended to {system_report_path}\")\n",
    "    else:\n",
    "        print(\"⚠️  No snapshots created (no tables available).\")\n",
    "else:\n",
    "    import warnings\n",
    "\n",
    "    warnings.warn(\"TABLES unavailable; skipping snapshot creation.\")\n",
    "    print(\"Skipping snapshot freezing because TABLES_AVAILABLE=False\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7925f8",
   "metadata": {},
   "source": [
    "## 4. Data Quality Gate (Great Expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "201cd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>table</th>\n",
       "      <th>check</th>\n",
       "      <th>success</th>\n",
       "      <th>unexpected_percent</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gold</td>\n",
       "      <td>hadoop_catalog.aq.gold.fact_air_quality_hourly</td>\n",
       "      <td>AQI not null</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'column': 'aqi', 'result_format': 'BASIC'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gold</td>\n",
       "      <td>hadoop_catalog.aq.gold.fact_air_quality_hourly</td>\n",
       "      <td>AQI within 0-500</td>\n",
       "      <td>False</td>\n",
       "      <td>99.965737</td>\n",
       "      <td>{'column': 'aqi', 'min_value': 0, 'max_value':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gold</td>\n",
       "      <td>hadoop_catalog.aq.gold.fact_air_quality_hourly</td>\n",
       "      <td>PM2.5 non-negative</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gold</td>\n",
       "      <td>hadoop_catalog.aq.gold.fact_air_quality_hourly</td>\n",
       "      <td>Unique (location_key, ts_utc)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'column_list': ['location_key', 'ts_utc'], 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gold</td>\n",
       "      <td>hadoop_catalog.aq.gold.fact_air_quality_hourly</td>\n",
       "      <td>AQI completeness ≥95%</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>silver</td>\n",
       "      <td>hadoop_catalog.aq.silver.air_quality_hourly_clean</td>\n",
       "      <td>Unique (location_id, ts_utc)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'column_list': ['location_id', 'ts_utc'], 're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>silver</td>\n",
       "      <td>hadoop_catalog.aq.silver.aq_components_hourly</td>\n",
       "      <td>Unique (location_id, ts_utc)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'column_list': ['location_id', 'ts_utc'], 're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>silver</td>\n",
       "      <td>hadoop_catalog.aq.silver.aq_index_hourly</td>\n",
       "      <td>Unique (location_id, ts_utc)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'column_list': ['location_id', 'ts_utc'], 're...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer                                              table  \\\n",
       "0    gold     hadoop_catalog.aq.gold.fact_air_quality_hourly   \n",
       "1    gold     hadoop_catalog.aq.gold.fact_air_quality_hourly   \n",
       "2    gold     hadoop_catalog.aq.gold.fact_air_quality_hourly   \n",
       "3    gold     hadoop_catalog.aq.gold.fact_air_quality_hourly   \n",
       "4    gold     hadoop_catalog.aq.gold.fact_air_quality_hourly   \n",
       "5  silver  hadoop_catalog.aq.silver.air_quality_hourly_clean   \n",
       "6  silver      hadoop_catalog.aq.silver.aq_components_hourly   \n",
       "7  silver           hadoop_catalog.aq.silver.aq_index_hourly   \n",
       "\n",
       "                           check  success  unexpected_percent  \\\n",
       "0                   AQI not null     True            0.000000   \n",
       "1               AQI within 0-500    False           99.965737   \n",
       "2             PM2.5 non-negative     True            0.000000   \n",
       "3  Unique (location_key, ts_utc)     True            0.000000   \n",
       "4          AQI completeness ≥95%     True            0.000000   \n",
       "5   Unique (location_id, ts_utc)     True            0.000000   \n",
       "6   Unique (location_id, ts_utc)     True            0.000000   \n",
       "7   Unique (location_id, ts_utc)     True            0.000000   \n",
       "\n",
       "                                             details  \n",
       "0        {'column': 'aqi', 'result_format': 'BASIC'}  \n",
       "1  {'column': 'aqi', 'min_value': 0, 'max_value':...  \n",
       "2                                                 {}  \n",
       "3  {'column_list': ['location_key', 'ts_utc'], 'r...  \n",
       "4                                                 {}  \n",
       "5  {'column_list': ['location_id', 'ts_utc'], 're...  \n",
       "6  {'column_list': ['location_id', 'ts_utc'], 're...  \n",
       "7  {'column_list': ['location_id', 'ts_utc'], 're...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQ summary exported to /home/dlhnhom2/dlh-aqi/reports/tables/aqi_dq_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure Great Expectations (Spark) is available\n",
    "try:\n",
    "    import great_expectations as ge\n",
    "    from great_expectations.dataset import SparkDFDataset\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"great_expectations[spark]==0.15.50\"])\n",
    "    import great_expectations as ge\n",
    "    from great_expectations.dataset import SparkDFDataset\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "def summarise_expectation(result: Dict, table_label: str, check_name: str, layer: str) -> Dict:\n",
    "    expectation_config = result.get(\"expectation_config\", {})\n",
    "    return {\n",
    "        \"layer\": layer,\n",
    "        \"table\": table_label,\n",
    "        \"check\": check_name,\n",
    "        \"success\": bool(result.get(\"success\")),\n",
    "        \"unexpected_percent\": result.get(\"result\", {}).get(\"unexpected_percent\"),\n",
    "        \"details\": expectation_config.get(\"kwargs\", {}),\n",
    "    }\n",
    "\n",
    "summary_rows: List[Dict] = []\n",
    "\n",
    "# If we don't have tables available (local dev), skip heavy expectations and provide placeholders\n",
    "if 'TABLES_AVAILABLE' in globals() and not TABLES_AVAILABLE:\n",
    "    import warnings\n",
    "\n",
    "    warnings.warn(\"TABLES unavailable; skipping Great Expectations checks in local mode.\")\n",
    "    # Provide a minimal DQ summary indicating skipped checks\n",
    "    summary_rows.append({\n",
    "        \"layer\": \"gold\",\n",
    "        \"table\": TABLES[\"fact_hourly\"],\n",
    "        \"check\": \"skipped\",\n",
    "        \"success\": None,\n",
    "        \"unexpected_percent\": None,\n",
    "        \"details\": {\"reason\": \"TABLES_AVAILABLE=False\"},\n",
    "    })\n",
    "else:\n",
    "    # Gold fact table expectations\n",
    "    fact_expect = SparkDFDataset(\n",
    "        fact_df.select(\n",
    "            \"location_key\", \"ts_utc\", \"aqi\", \"pm25\", \"pm10\", \"o3\", \"no2\", \"so2\", \"co\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Use expectation methods available in this GE version; map to conservative equivalents\n",
    "    try:\n",
    "        res = fact_expect.expect_column_values_to_not_be_null(\"aqi\")\n",
    "        summary_rows.append(summarise_expectation(res, TABLES[\"fact_hourly\"], \"AQI not null\", \"gold\"))\n",
    "    except AttributeError:\n",
    "        # Fallback: run a simple null check using Spark and craft a fake GE-like result\n",
    "        null_count = fact_df.filter(F.col(\"aqi\").isNull()).count()\n",
    "        total = fact_df.count()\n",
    "        summary_rows.append({\n",
    "            \"layer\": \"gold\",\n",
    "            \"table\": TABLES[\"fact_hourly\"],\n",
    "            \"check\": \"AQI not null\",\n",
    "            \"success\": (null_count == 0),\n",
    "            \"unexpected_percent\": (null_count / total if total > 0 else None),\n",
    "            \"details\": {},\n",
    "        })\n",
    "\n",
    "    # Bounded AQI\n",
    "    try:\n",
    "        res = fact_expect.expect_column_values_to_be_between(\"aqi\", min_value=0, max_value=500)\n",
    "        summary_rows.append(summarise_expectation(res, TABLES[\"fact_hourly\"], \"AQI within 0-500\", \"gold\"))\n",
    "    except AttributeError:\n",
    "        # Fallback\n",
    "        out_of_bounds = fact_df.filter((F.col(\"aqi\") < 0) | (F.col(\"aqi\") > 500)).count()\n",
    "        total = fact_df.count()\n",
    "        summary_rows.append({\n",
    "            \"layer\": \"gold\",\n",
    "            \"table\": TABLES[\"fact_hourly\"],\n",
    "            \"check\": \"AQI within 0-500\",\n",
    "            \"success\": (out_of_bounds == 0),\n",
    "            \"unexpected_percent\": (out_of_bounds / total if total > 0 else None),\n",
    "            \"details\": {},\n",
    "        })\n",
    "\n",
    "    # PM2.5 non-negative\n",
    "    try:\n",
    "        res = fact_expect.expect_column_values_to_be_greater_than(\"pm25\", min_value=-1e-9)\n",
    "        summary_rows.append(summarise_expectation(res, TABLES[\"fact_hourly\"], \"PM2.5 non-negative\", \"gold\"))\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            out_of_bounds = fact_df.filter(F.col(\"pm25\") < 0).count()\n",
    "            total = fact_df.count()\n",
    "            summary_rows.append({\n",
    "                \"layer\": \"gold\",\n",
    "                \"table\": TABLES[\"fact_hourly\"],\n",
    "                \"check\": \"PM2.5 non-negative\",\n",
    "                \"success\": (out_of_bounds == 0),\n",
    "                \"unexpected_percent\": (out_of_bounds / total if total > 0 else None),\n",
    "                \"details\": {},\n",
    "            })\n",
    "        except Exception:\n",
    "            summary_rows.append({\n",
    "                \"layer\": \"gold\",\n",
    "                \"table\": TABLES[\"fact_hourly\"],\n",
    "                \"check\": \"PM2.5 non-negative\",\n",
    "                \"success\": None,\n",
    "                \"unexpected_percent\": None,\n",
    "                \"details\": {\"reason\": \"fallback failed\"},\n",
    "            })\n",
    "\n",
    "    # Uniqueness on (location_key, ts_utc)\n",
    "    try:\n",
    "        res = fact_expect.expect_compound_columns_to_be_unique([\"location_key\", \"ts_utc\"])\n",
    "        summary_rows.append(summarise_expectation(res, TABLES[\"fact_hourly\"], \"Unique (location_key, ts_utc)\", \"gold\"))\n",
    "    except AttributeError:\n",
    "        dup_count = fact_df.groupBy(\"location_key\", \"ts_utc\").count().filter(F.col(\"count\") > 1).count()\n",
    "        summary_rows.append({\n",
    "            \"layer\": \"gold\",\n",
    "            \"table\": TABLES[\"fact_hourly\"],\n",
    "            \"check\": \"Unique (location_key, ts_utc)\",\n",
    "            \"success\": (dup_count == 0),\n",
    "            \"unexpected_percent\": None,\n",
    "            \"details\": {},\n",
    "        })\n",
    "\n",
    "    # Completeness ≥95% per (location, day)\n",
    "    daily_completeness = (\n",
    "        fact_df\n",
    "        .withColumn(\"date_utc\", F.to_date(\"ts_utc\"))\n",
    "        .groupBy(\"location_key\", \"date_utc\")\n",
    "        .agg(\n",
    "            (F.avg(F.when(F.col(\"aqi\").isNotNull(), 1.0).otherwise(0.0))).alias(\"aqi_completeness\")\n",
    "        )\n",
    "    )\n",
    "    completeness_expect = SparkDFDataset(daily_completeness)\n",
    "    try:\n",
    "        res = completeness_expect.expect_column_values_to_be_greater_than(\"aqi_completeness\", value=0.95 - 1e-6)\n",
    "        summary_rows.append(summarise_expectation(res, TABLES[\"fact_hourly\"], \"AQI completeness ≥95%\", \"gold\"))\n",
    "    except AttributeError:\n",
    "        # Fallback using Spark\n",
    "        low_coverage = daily_completeness.filter(F.col(\"aqi_completeness\") < (0.95 - 1e-6)).count()\n",
    "        total_days = daily_completeness.count()\n",
    "        summary_rows.append({\n",
    "            \"layer\": \"gold\",\n",
    "            \"table\": TABLES[\"fact_hourly\"],\n",
    "            \"check\": \"AQI completeness ≥95%\",\n",
    "            \"success\": (low_coverage == 0),\n",
    "            \"unexpected_percent\": (low_coverage / total_days if total_days > 0 else None),\n",
    "            \"details\": {},\n",
    "        })\n",
    "\n",
    "    # Silver uniqueness checks\n",
    "    for table_label in (\"silver_clean\", \"silver_components\", \"silver_index\"):\n",
    "        silver_df = spark.table(TABLES[table_label])\n",
    "        try:\n",
    "            expectation = SparkDFDataset(silver_df.select(\"location_id\", \"ts_utc\"))\n",
    "            res = expectation.expect_compound_columns_to_be_unique([\"location_id\", \"ts_utc\"])\n",
    "            summary_rows.append(summarise_expectation(res, TABLES[table_label], \"Unique (location_id, ts_utc)\", \"silver\",))\n",
    "        except AttributeError:\n",
    "            dup = silver_df.groupBy(\"location_id\", \"ts_utc\").count().filter(F.col(\"count\") > 1).count()\n",
    "            summary_rows.append({\n",
    "                \"layer\": \"silver\",\n",
    "                \"table\": TABLES[table_label],\n",
    "                \"check\": \"Unique (location_id, ts_utc)\",\n",
    "                \"success\": (dup == 0),\n",
    "                \"unexpected_percent\": None,\n",
    "                \"details\": {},\n",
    "            })\n",
    "\n",
    "# Compile DQ summary\n",
    "dq_summary = pd.DataFrame(summary_rows)\n",
    "display(dq_summary)\n",
    "\n",
    "# Persist to CSV for appendix\n",
    "dq_table_path = TABLE_DIR / \"aqi_dq_summary.csv\"\n",
    "dq_summary.to_csv(dq_table_path, index=False)\n",
    "print(f\"DQ summary exported to {dq_table_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a815e8e",
   "metadata": {},
   "source": [
    "## 5. Analytical Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a4db2980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:=========================================>           (360 + 2) / 457]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly analytical dataset rows: 43,779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join Fact + Dimensions → hourly analytical view (robust to duplicate column names)\n",
    "analysis_df = (\n",
    "    fact_df.alias(\"f\")\n",
    "    .join(dim_location_df.alias(\"loc\"), on=\"location_key\", how=\"left\")\n",
    "    .join(dim_date_df.alias(\"dd\"), on=\"date_key\", how=\"left\")\n",
    "    .join(dim_time_df.alias(\"dt\"), on=\"time_key\", how=\"left\")\n",
    "    .select(\n",
    "        F.col(\"f.location_key\").alias(\"location_key\"),\n",
    "        F.col(\"loc.location_name\").alias(\"location_name\"),\n",
    "        F.col(\"loc.admin1\").alias(\"admin1\"),\n",
    "        F.col(\"loc.country\").alias(\"country\"),\n",
    "        F.coalesce(F.col(\"f.date_utc\"), F.col(\"dd.date_utc\")).alias(\"date_utc\"),\n",
    "        F.col(\"dd.year\").alias(\"year\"),\n",
    "        F.col(\"dd.month\").alias(\"month\"),\n",
    "        F.col(\"dd.day\").alias(\"day\"),\n",
    "        F.col(\"dd.dow\").alias(\"dow\"),\n",
    "        F.col(\"dd.week\").alias(\"week\"),\n",
    "        F.col(\"dd.is_weekend\").alias(\"is_weekend\"),\n",
    "        F.col(\"dd.month_name\").alias(\"month_name\"),\n",
    "        F.col(\"dd.day_name\").alias(\"day_name\"),\n",
    "        F.coalesce(F.col(\"dt.hour\"), F.lit(None)).alias(\"hour\"),\n",
    "        F.col(\"dt.day_part\").alias(\"day_part\"),\n",
    "        F.col(\"f.ts_utc\").alias(\"ts_utc\"),\n",
    "        F.col(\"f.aqi\").alias(\"aqi\"),\n",
    "        F.col(\"f.category\").alias(\"category\"),\n",
    "        F.col(\"f.dominant_pollutant\").alias(\"dominant_pollutant\"),\n",
    "        F.col(\"f.pm25\").alias(\"pm25\"),\n",
    "        F.col(\"f.pm10\").alias(\"pm10\"),\n",
    "        F.col(\"f.o3\").alias(\"o3\"),\n",
    "        F.col(\"f.no2\").alias(\"no2\"),\n",
    "        F.col(\"f.so2\").alias(\"so2\"),\n",
    "        F.col(\"f.co\").alias(\"co\"),\n",
    "        F.col(\"f.aod\").alias(\"aod\"),\n",
    "        F.col(\"f.uv_index\").alias(\"uv_index\"),\n",
    "        F.col(\"f.uv_index_clear_sky\").alias(\"uv_index_clear_sky\"),\n",
    "        F.col(\"f.is_validated\").alias(\"is_validated\"),\n",
    "        F.col(\"f.quality_flag\").alias(\"quality_flag\"),\n",
    "        F.col(\"f.data_source\").alias(\"data_source\"),\n",
    "        F.col(\"f.run_id\").alias(\"run_id\"),\n",
    "    )\n",
    "    .withColumn(\"season\", F.when(F.col(\"month\").isin(5, 6, 7, 8, 9, 10), F.lit(\"Rainy\")).otherwise(F.lit(\"Dry\")))\n",
    "    .withColumn(\"hour_sin\", F.sin(2 * np.pi * F.col(\"hour\") / F.lit(24)))\n",
    "    .withColumn(\"hour_cos\", F.cos(2 * np.pi * F.col(\"hour\") / F.lit(24)))\n",
    "    .withColumn(\"dow_sin\", F.sin(2 * np.pi * F.col(\"dow\") / F.lit(7)))\n",
    "    .withColumn(\"dow_cos\", F.cos(2 * np.pi * F.col(\"dow\") / F.lit(7)))\n",
    "    .withColumn(\"month_sin\", F.sin(2 * np.pi * F.col(\"month\") / F.lit(12)))\n",
    "    .withColumn(\"month_cos\", F.cos(2 * np.pi * F.col(\"month\") / F.lit(12)))\n",
    ")\n",
    "analysis_df.createOrReplaceTempView(\"aqi_analysis_hourly\")\n",
    "analysis_df.cache()\n",
    "print(f\"Hourly analytical dataset rows: {analysis_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "28774df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:===================================================> (441 + 2) / 457]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily analytical dataset rows: 1,827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Daily aggregation for downstream analyses\n",
    "category_levels = [\"Good\", \"Moderate\", \"USG\", \"Unhealthy\", \"Very Unhealthy\", \"Hazardous\"]\n",
    "\n",
    "agg_exprs = [\n",
    "    F.expr(\"percentile_approx(aqi, 0.5, 1000)\").alias(\"median_aqi\"),\n",
    "    F.avg(\"aqi\").alias(\"mean_aqi\"),\n",
    "    (F.expr(\"percentile_approx(aqi, 0.75, 1000)\") - F.expr(\"percentile_approx(aqi, 0.25, 1000)\")).alias(\"iqr_aqi\"),\n",
    "    F.count(\"*\").alias(\"hour_count\"),\n",
    "    F.avg(F.when(F.col(\"aqi\").isNotNull(), 1.0).otherwise(0.0)).alias(\"aqi_completeness\"),\n",
    "    F.sum(F.when(F.col(\"pm25\") > 35, 1).otherwise(0)).alias(\"pm25_exceed_hours_35\"),\n",
    "    F.sum(F.when(F.col(\"pm25\") > 25, 1).otherwise(0)).alias(\"pm25_exceed_hours_25\"),\n",
    "    F.sum(F.when(F.col(\"pm25\") > 50, 1).otherwise(0)).alias(\"pm25_exceed_hours_50\"),\n",
    "]\n",
    "\n",
    "for level in category_levels:\n",
    "    slug = level.lower().replace(\" \", \"_\")\n",
    "    agg_exprs.append(F.avg(F.when(F.col(\"category\") == level, 1.0).otherwise(0.0)).alias(f\"share_{slug}\"))\n",
    "\n",
    "agg_exprs.extend([\n",
    "    F.avg(\"pm25\").alias(\"pm25_mean\"),\n",
    "    F.avg(\"pm10\").alias(\"pm10_mean\"),\n",
    "    F.avg(\"o3\").alias(\"o3_mean\"),\n",
    "    F.avg(\"no2\").alias(\"no2_mean\"),\n",
    "    F.avg(\"so2\").alias(\"so2_mean\"),\n",
    "    F.avg(\"co\").alias(\"co_mean\"),\n",
    "])\n",
    "\n",
    "# Build a clean working DataFrame selecting and aliasing explicit columns to avoid ambiguous names\n",
    "working_cols = [\n",
    "    F.col(\"location_key\"),\n",
    "    F.col(\"location_name\"),\n",
    "    F.col(\"admin1\"),\n",
    "    F.col(\"country\"),\n",
    "    F.col(\"date_utc\"),\n",
    "    F.col(\"year\"),\n",
    "    F.col(\"month\"),\n",
    "    F.col(\"day\"),\n",
    "    F.col(\"dow\"),\n",
    "    F.col(\"week\"),\n",
    "    F.col(\"is_weekend\"),\n",
    "    F.col(\"month_name\"),\n",
    "    F.col(\"day_name\"),\n",
    "    F.col(\"season\"),\n",
    "    F.col(\"aqi\"),\n",
    "    F.col(\"category\"),\n",
    "    F.col(\"pm25\"),\n",
    "    F.col(\"pm10\"),\n",
    "    F.col(\"o3\"),\n",
    "    F.col(\"no2\"),\n",
    "    F.col(\"so2\"),\n",
    "    F.col(\"co\"),\n",
    "]\n",
    "\n",
    "# Some columns may be absent in placeholder DataFrames; filter those which exist\n",
    "existing_cols = [c for c in working_cols if c._jc is not None if True]\n",
    "# The above check may not reliably determine existence; instead, build using try/except per column\n",
    "safe_cols = []\n",
    "for name in [\n",
    "    'location_key','location_name','admin1','country','date_utc','year','month','day','dow','week','is_weekend','month_name','day_name','season','aqi','category','pm25','pm10','o3','no2','so2','co'\n",
    "]:\n",
    "    try:\n",
    "        safe_cols.append(F.col(name))\n",
    "    except Exception:\n",
    "        # Skip missing columns gracefully\n",
    "        pass\n",
    "\n",
    "working_df = analysis_df.select(*safe_cols)\n",
    "\n",
    "# Perform aggregation using unambiguous column names\n",
    "group_cols = [\n",
    "    'location_key','location_name','admin1','country','date_utc','year','month','day','dow','week','is_weekend','month_name','day_name','season'\n",
    "]\n",
    "# Only keep group cols that exist in working_df\n",
    "group_cols = [c for c in group_cols if c in working_df.columns]\n",
    "\n",
    "if len(group_cols) == 0:\n",
    "    # nothing to aggregate; create empty daily_df with expected columns\n",
    "    daily_df = spark.createDataFrame([], schema=T.StructType([\n",
    "        T.StructField('location_key', T.IntegerType(), True),\n",
    "        T.StructField('location_name', T.StringType(), True),\n",
    "        T.StructField('admin1', T.StringType(), True),\n",
    "        T.StructField('country', T.StringType(), True),\n",
    "        T.StructField('date_utc', T.DateType(), True),\n",
    "    ]))\n",
    "else:\n",
    "    daily_df = (\n",
    "        working_df.groupBy(*group_cols)\n",
    "        .agg(*agg_exprs)\n",
    "        .withColumn(\"valid_hour_fraction\", F.col(\"hour_count\") / F.lit(24))\n",
    "    )\n",
    "\n",
    "# Ensure downstream column names exist even if empty\n",
    "expected_cols = ['location_key','location_name','admin1','country','date_utc','year','month','day','dow','week','is_weekend','month_name','day_name','season']\n",
    "for c in expected_cols:\n",
    "    if c not in daily_df.columns:\n",
    "        daily_df = daily_df.withColumn(c, F.lit(None).cast(T.StringType()))\n",
    "\n",
    "# Reorder/select expected columns plus metrics\n",
    "select_cols = [c for c in expected_cols if c in daily_df.columns]\n",
    "metric_cols = [\n",
    "    'median_aqi','mean_aqi','iqr_aqi','hour_count','aqi_completeness','pm25_exceed_hours_35','pm25_exceed_hours_25','pm25_exceed_hours_50',\n",
    "] + [f\"share_{lvl.lower().replace(' ', '_')}\" for lvl in category_levels] + ['pm25_mean','pm10_mean','o3_mean','no2_mean','so2_mean','co_mean','valid_hour_fraction']\n",
    "final_select = select_cols + [c for c in metric_cols if c in daily_df.columns]\n",
    "\n",
    "daily_df = daily_df.select(*final_select)\n",
    "\n",
    "daily_df.createOrReplaceTempView(\"aqi_daily_metrics\")\n",
    "print(f\"Daily analytical dataset rows: {daily_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "256399e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily analysis table refreshed: hadoop_catalog.aq.analysis.aqi_daily\n"
     ]
    }
   ],
   "source": [
    "# Persist daily dataset into Iceberg analysis namespace\n",
    "ANALYSIS_TABLE = \"hadoop_catalog.aq.analysis.aqi_daily\"\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS hadoop_catalog.aq.analysis\")\n",
    "\n",
    "daily_df.writeTo(ANALYSIS_TABLE).createOrReplace()\n",
    "print(f\"Daily analysis table refreshed: {ANALYSIS_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1902a9c1",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8feb89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>median_aqi</th>\n",
       "      <th>mean_aqi</th>\n",
       "      <th>iqr_aqi</th>\n",
       "      <th>completeness</th>\n",
       "      <th>unhealthy_share</th>\n",
       "      <th>usg_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>6746.0</td>\n",
       "      <td>8267.412014</td>\n",
       "      <td>3074.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>5916.0</td>\n",
       "      <td>7791.444718</td>\n",
       "      <td>2288.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>3585.0</td>\n",
       "      <td>4276.600164</td>\n",
       "      <td>899.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_name  median_aqi     mean_aqi  iqr_aqi  completeness  \\\n",
       "0           Hà Nội      6746.0  8267.412014   3074.0           1.0   \n",
       "1  TP. Hồ Chí Minh      5916.0  7791.444718   2288.0           1.0   \n",
       "2          Đà Nẵng      3585.0  4276.600164    899.0           1.0   \n",
       "\n",
       "   unhealthy_share  usg_share  \n",
       "0              0.0        0.0  \n",
       "1              0.0        0.0  \n",
       "2              0.0        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>season</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>Dry</td>\n",
       "      <td>6650.0</td>\n",
       "      <td>7553.436214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>7217.0</td>\n",
       "      <td>7925.422764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>Dry</td>\n",
       "      <td>5270.5</td>\n",
       "      <td>6307.057613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>9828.0</td>\n",
       "      <td>10549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>Dry</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>3844.487654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>5104.0</td>\n",
       "      <td>5216.398374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_name season  median          mean\n",
       "0           Hà Nội    Dry  6650.0   7553.436214\n",
       "1           Hà Nội  Rainy  7217.0   7925.422764\n",
       "2  TP. Hồ Chí Minh    Dry  5270.5   6307.057613\n",
       "3  TP. Hồ Chí Minh  Rainy  9828.0  10549.000000\n",
       "4          Đà Nẵng    Dry  3297.0   3844.487654\n",
       "5          Đà Nẵng  Rainy  5104.0   5216.398374"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_pd = daily_df.toPandas()\n",
    "\n",
    "# Descriptive statistics per city\n",
    "descriptive_stats = (\n",
    "    daily_pd\n",
    "    .groupby(\"location_name\")\n",
    "    .agg(\n",
    "        median_aqi=(\"median_aqi\", \"median\"),\n",
    "        mean_aqi=(\"mean_aqi\", \"mean\"),\n",
    "        iqr_aqi=(\"iqr_aqi\", \"median\"),\n",
    "        completeness=(\"aqi_completeness\", \"mean\"),\n",
    "        unhealthy_share=(\"share_unhealthy\", \"mean\"),\n",
    "        usg_share=(\"share_usg\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "display(descriptive_stats)\n",
    "\n",
    "# Plot median AQI distributions\n",
    "fig, ax = plt.subplots()\n",
    "for city, group in daily_pd.groupby('location_name'):\n",
    "    group['median_aqi'].plot(kind='kde', ax=ax, label=city)\n",
    "ax.set_title('Median Daily AQI Distribution by City')\n",
    "ax.set_xlabel('Median AQI')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'median_aqi_distribution.png', dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "# Seasonal patterns\n",
    "seasonal_stats = (\n",
    "    daily_pd.groupby(['location_name', 'season'])['median_aqi']\n",
    "    .agg(['median', 'mean']).reset_index()\n",
    ")\n",
    "display(seasonal_stats)\n",
    "\n",
    "# Diurnal pattern from hourly dataset (include dow column for heatmap)\n",
    "hourly_pd = analysis_df.select('location_name', 'hour', 'dow', 'season', 'aqi').toPandas()\n",
    "diurnal_profile = (\n",
    "    hourly_pd\n",
    "    .groupby(['location_name', 'hour'])['aqi']\n",
    "    .median()\n",
    "    .reset_index()\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "for city, group in diurnal_profile.groupby('location_name'):\n",
    "    ax.plot(group['hour'], group['aqi'], marker='o', label=city)\n",
    "ax.set_xticks(range(0, 24, 2))\n",
    "ax.set_xlabel('Hour of Day (UTC)')\n",
    "ax.set_ylabel('Median AQI')\n",
    "ax.set_title('Diurnal AQI Median by City')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'diurnal_median_aqi.png', dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "# Heatmap (hour vs day-of-week) for AQI\n",
    "for city in hourly_pd['location_name'].unique():\n",
    "    pivot = (\n",
    "        hourly_pd[hourly_pd['location_name'] == city]\n",
    "        .pivot_table(index='hour', columns='dow', values='aqi', aggfunc='median')\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    im = ax.imshow(pivot.values, aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax.set_title(f'AQI Median Heatmap – {city}')\n",
    "    ax.set_xlabel('Day of Week (0=Mon)')\n",
    "    ax.set_ylabel('Hour of Day (UTC)')\n",
    "    ax.set_xticks(range(pivot.shape[1]))\n",
    "    ax.set_xticklabels(pivot.columns)\n",
    "    ax.set_yticks(range(pivot.shape[0]))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "    fig.colorbar(im, ax=ax, label='Median AQI')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / f'aqi_heatmap_{city.lower().replace(\" \", \"_\")}.png', dpi=150)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c72969",
   "metadata": {},
   "source": [
    "## 7. Diagnostic Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e465f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>dominant_pollutant</th>\n",
       "      <th>hours</th>\n",
       "      <th>share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>co</td>\n",
       "      <td>14588</td>\n",
       "      <td>0.999657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>so2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>co</td>\n",
       "      <td>14588</td>\n",
       "      <td>0.999657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>so2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>co</td>\n",
       "      <td>14588</td>\n",
       "      <td>0.999657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>no2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>so2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_name dominant_pollutant  hours     share\n",
       "0           Hà Nội                 co  14588  0.999657\n",
       "1           Hà Nội                so2      5  0.000343\n",
       "2  TP. Hồ Chí Minh                 co  14588  0.999657\n",
       "3  TP. Hồ Chí Minh                so2      5  0.000343\n",
       "4          Đà Nẵng                 co  14588  0.999657\n",
       "5          Đà Nẵng                no2      4  0.000274\n",
       "6          Đà Nẵng                so2      1  0.000069"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_key</th>\n",
       "      <th>location_name</th>\n",
       "      <th>exceed_group</th>\n",
       "      <th>streak_start</th>\n",
       "      <th>streak_end</th>\n",
       "      <th>days_in_streak</th>\n",
       "      <th>total_exceed_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>68</td>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>2025-07-21</td>\n",
       "      <td>113</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>243</td>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>2025-07-20</td>\n",
       "      <td>63</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>60</td>\n",
       "      <td>2024-11-27</td>\n",
       "      <td>2025-01-09</td>\n",
       "      <td>44</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>47</td>\n",
       "      <td>2024-08-03</td>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>35</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>69</td>\n",
       "      <td>2025-07-23</td>\n",
       "      <td>2025-08-24</td>\n",
       "      <td>33</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>414</td>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>436</td>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>438</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>439</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>450</td>\n",
       "      <td>2025-08-19</td>\n",
       "      <td>2025-08-19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        location_key    location_name  exceed_group streak_start  streak_end  \\\n",
       "0             Hà Nội           Hà Nội            68   2025-03-31  2025-07-21   \n",
       "1    TP. Hồ Chí Minh  TP. Hồ Chí Minh           243   2025-05-19  2025-07-20   \n",
       "2             Hà Nội           Hà Nội            60   2024-11-27  2025-01-09   \n",
       "3             Hà Nội           Hà Nội            47   2024-08-03  2024-09-06   \n",
       "4             Hà Nội           Hà Nội            69   2025-07-23  2025-08-24   \n",
       "..               ...              ...           ...          ...         ...   \n",
       "161          Đà Nẵng          Đà Nẵng           414   2025-07-10  2025-07-10   \n",
       "162          Đà Nẵng          Đà Nẵng           436   2025-08-02  2025-08-02   \n",
       "163          Đà Nẵng          Đà Nẵng           438   2025-08-05  2025-08-05   \n",
       "164          Đà Nẵng          Đà Nẵng           439   2025-08-07  2025-08-07   \n",
       "165          Đà Nẵng          Đà Nẵng           450   2025-08-19  2025-08-19   \n",
       "\n",
       "     days_in_streak  total_exceed_hours  \n",
       "0               113                2021  \n",
       "1                63                 858  \n",
       "2                44                 769  \n",
       "3                35                 624  \n",
       "4                33                 589  \n",
       "..              ...                 ...  \n",
       "161               1                   6  \n",
       "162               1                   3  \n",
       "163               1                   3  \n",
       "164               1                   3  \n",
       "165               1                   1  \n",
       "\n",
       "[166 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>corr_aqi_pm25</th>\n",
       "      <th>corr_aqi_pm10</th>\n",
       "      <th>corr_aqi_o3</th>\n",
       "      <th>corr_aqi_no2</th>\n",
       "      <th>corr_aqi_so2</th>\n",
       "      <th>corr_aqi_co</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>0.287122</td>\n",
       "      <td>0.096817</td>\n",
       "      <td>0.119688</td>\n",
       "      <td>0.293251</td>\n",
       "      <td>-0.169078</td>\n",
       "      <td>0.790959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>0.512526</td>\n",
       "      <td>0.472847</td>\n",
       "      <td>-0.182451</td>\n",
       "      <td>0.490038</td>\n",
       "      <td>0.464136</td>\n",
       "      <td>0.749135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>0.586691</td>\n",
       "      <td>0.443732</td>\n",
       "      <td>-0.202795</td>\n",
       "      <td>0.581006</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.758132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_name  corr_aqi_pm25  corr_aqi_pm10  corr_aqi_o3  corr_aqi_no2  \\\n",
       "0          Đà Nẵng       0.287122       0.096817     0.119688      0.293251   \n",
       "1           Hà Nội       0.512526       0.472847    -0.182451      0.490038   \n",
       "2  TP. Hồ Chí Minh       0.586691       0.443732    -0.202795      0.581006   \n",
       "\n",
       "   corr_aqi_so2  corr_aqi_co  \n",
       "0     -0.169078     0.790959  \n",
       "1      0.464136     0.749135  \n",
       "2      0.178856     0.758132  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dominant pollutant distribution\n",
    "pollutant_dominance = (\n",
    "    analysis_df\n",
    "    .groupBy('location_name', 'dominant_pollutant')\n",
    "    .agg(F.count('*').alias('hours'))\n",
    "    .withColumn('share', F.col('hours') / F.sum('hours').over(Window.partitionBy('location_name')))\n",
    "    .orderBy('location_name', F.desc('hours'))\n",
    ")\n",
    "\n",
    "display(pollutant_dominance.toPandas())\n",
    "\n",
    "# Exceedance analysis (daily)\n",
    "daily_exceedance = (\n",
    "    daily_df\n",
    "    .withColumn('pm25_exceed_flag', F.col('pm25_exceed_hours_35') > 0)\n",
    "    .select('location_key', 'location_name', 'date_utc', 'pm25_exceed_flag', 'pm25_exceed_hours_35')\n",
    ")\n",
    "\n",
    "window = Window.partitionBy('location_key').orderBy('date_utc')\n",
    "exceedance_with_index = (\n",
    "    daily_exceedance\n",
    "    .withColumn('row_id', F.row_number().over(window))\n",
    "    .withColumn('exceed_cumsum', F.sum(F.col('pm25_exceed_flag').cast('int')).over(window))\n",
    "    .withColumn('exceed_group', F.col('row_id') - F.col('exceed_cumsum'))\n",
    ")\n",
    "\n",
    "streaks = (\n",
    "    exceedance_with_index\n",
    "    .where(F.col('pm25_exceed_flag'))\n",
    "    .groupBy('location_key', 'location_name', 'exceed_group')\n",
    "    .agg(\n",
    "        F.min('date_utc').alias('streak_start'),\n",
    "        F.max('date_utc').alias('streak_end'),\n",
    "        F.count('*').alias('days_in_streak'),\n",
    "        F.sum('pm25_exceed_hours_35').alias('total_exceed_hours'),\n",
    "    )\n",
    "    .orderBy(F.desc('days_in_streak'))\n",
    ")\n",
    "\n",
    "display(streaks.toPandas())\n",
    "\n",
    "# Correlation analysis (AQI vs pollutant components)\n",
    "correlation_stats = []\n",
    "for city in analysis_df.select('location_name').distinct().toPandas()['location_name'].tolist():\n",
    "    city_df = analysis_df.where(F.col('location_name') == city).select('aqi', 'pm25', 'pm10', 'o3', 'no2', 'so2', 'co')\n",
    "    corr_row = {'location_name': city}\n",
    "    for col in ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']:\n",
    "        corr_row[f'corr_aqi_{col}'] = city_df.corr('aqi', col)\n",
    "    correlation_stats.append(corr_row)\n",
    "\n",
    "correlation_df = pd.DataFrame(correlation_stats)\n",
    "display(correlation_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5afe6",
   "metadata": {},
   "source": [
    "## 8. Statistical Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d322d621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>median_point_estimate</th>\n",
       "      <th>ci_lower_95</th>\n",
       "      <th>ci_upper_95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>6746.0</td>\n",
       "      <td>6483.35</td>\n",
       "      <td>7060.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>5916.0</td>\n",
       "      <td>5611.00</td>\n",
       "      <td>6336.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>3585.0</td>\n",
       "      <td>3471.00</td>\n",
       "      <td>3864.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_name  median_point_estimate  ci_lower_95  ci_upper_95\n",
       "0           Hà Nội                 6746.0      6483.35      7060.45\n",
       "1  TP. Hồ Chí Minh                 5916.0      5611.00      6336.00\n",
       "2          Đà Nẵng                 3585.0      3471.00      3864.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>dry_median</th>\n",
       "      <th>rainy_median</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>6650.0</td>\n",
       "      <td>7217.0</td>\n",
       "      <td>6.775408e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>5270.5</td>\n",
       "      <td>9828.0</td>\n",
       "      <td>9.951891e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đà Nẵng</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>5104.0</td>\n",
       "      <td>2.393293e-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_name  dry_median  rainy_median       p_value\n",
       "0           Hà Nội      6650.0        7217.0  6.775408e-03\n",
       "1  TP. Hồ Chí Minh      5270.5        9828.0  9.951891e-31\n",
       "2          Đà Nẵng      3297.0        5104.0  2.393293e-20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bootstrap 95% CI for median AQI per city\n",
    "rng = np.random.default_rng(42)\n",
    "bootstrap_records = []\n",
    "boot_iterations = 1000\n",
    "\n",
    "for city, group in daily_pd.groupby('location_name'):\n",
    "    values = group['median_aqi'].dropna().values\n",
    "    if len(values) == 0:\n",
    "        continue\n",
    "    bootstrap_samples = rng.choice(values, size=(boot_iterations, len(values)), replace=True)\n",
    "    medians = np.median(bootstrap_samples, axis=1)\n",
    "    lower, upper = np.percentile(medians, [2.5, 97.5])\n",
    "    bootstrap_records.append({\n",
    "        'location_name': city,\n",
    "        'median_point_estimate': np.median(values),\n",
    "        'ci_lower_95': lower,\n",
    "        'ci_upper_95': upper,\n",
    "    })\n",
    "\n",
    "bootstrap_df = pd.DataFrame(bootstrap_records)\n",
    "display(bootstrap_df)\n",
    "\n",
    "# Mann–Whitney U test (Dry vs Rainy season)\n",
    "try:\n",
    "    from scipy.stats import mannwhitneyu\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scipy==1.11.4'])\n",
    "    from scipy.stats import mannwhitneyu\n",
    "\n",
    "mann_records = []\n",
    "for city, group in daily_pd.groupby('location_name'):\n",
    "    dry = group.loc[group['season'] == 'Dry', 'median_aqi'].dropna().values\n",
    "    rainy = group.loc[group['season'] == 'Rainy', 'median_aqi'].dropna().values\n",
    "    if len(dry) == 0 or len(rainy) == 0:\n",
    "        continue\n",
    "    stat, p_value = mannwhitneyu(dry, rainy, alternative='two-sided')\n",
    "    mann_records.append({\n",
    "        'location_name': city,\n",
    "        'dry_median': np.median(dry),\n",
    "        'rainy_median': np.median(rainy),\n",
    "        'p_value': p_value,\n",
    "    })\n",
    "\n",
    "mann_df = pd.DataFrame(mann_records)\n",
    "display(mann_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88106a95",
   "metadata": {},
   "source": [
    "## 9. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cc075b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_name</th>\n",
       "      <th>ts_utc</th>\n",
       "      <th>aqi</th>\n",
       "      <th>robust_z</th>\n",
       "      <th>abs_robust_z</th>\n",
       "      <th>dominant_pollutant</th>\n",
       "      <th>is_validated</th>\n",
       "      <th>quality_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP. Hồ Chí Minh</td>\n",
       "      <td>2025-01-15 22:00:00</td>\n",
       "      <td>50503</td>\n",
       "      <td>10.965006</td>\n",
       "      <td>10.965006</td>\n",
       "      <td>co</td>\n",
       "      <td>True</td>\n",
       "      <td>validated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_name              ts_utc    aqi   robust_z  abs_robust_z  \\\n",
       "0  TP. Hồ Chí Minh 2025-01-15 22:00:00  50503  10.965006     10.965006   \n",
       "\n",
       "  dominant_pollutant  is_validated quality_flag  \n",
       "0                 co          True    validated  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Robust z-score using IQR per (location, hour)\n",
    "quantiles = (\n",
    "    analysis_df\n",
    "    .groupBy('location_key', 'location_name', 'hour')\n",
    "    .agg(\n",
    "        F.expr('percentile_approx(aqi, 0.25, 1000)').alias('q1'),\n",
    "        F.expr('percentile_approx(aqi, 0.5, 1000)').alias('median'),\n",
    "        F.expr('percentile_approx(aqi, 0.75, 1000)').alias('q3'),\n",
    "    )\n",
    "    .withColumn('iqr', F.col('q3') - F.col('q1'))\n",
    ")\n",
    "\n",
    "anomaly_df = (\n",
    "    analysis_df.alias('f')\n",
    "    .join(quantiles.alias('q'), ['location_key', 'location_name', 'hour'], 'left')\n",
    "    .withColumn('iqr_safe', F.when(F.col('q.iqr') < 1e-6, 1e-6).otherwise(F.col('q.iqr')))\n",
    "    .withColumn('robust_z', (F.col('f.aqi') - F.col('q.median')) / (F.col('iqr_safe') / F.lit(1.349)))\n",
    "    .withColumn('abs_robust_z', F.abs(F.col('robust_z')))\n",
    ")\n",
    "\n",
    "# Safely compute threshold, handling empty DataFrames\n",
    "threshold_result = anomaly_df.approxQuantile('abs_robust_z', [0.99], 0.01)\n",
    "if threshold_result and len(threshold_result) > 0:\n",
    "    threshold = threshold_result[0]\n",
    "else:\n",
    "    # Fallback if no data\n",
    "    import warnings\n",
    "    warnings.warn(\"No data available for anomaly detection; using default threshold=3.0\")\n",
    "    threshold = 3.0\n",
    "\n",
    "spikes = (\n",
    "    anomaly_df\n",
    "    .where(F.col('abs_robust_z') >= F.lit(threshold))\n",
    "    .select(\n",
    "        'location_name', 'ts_utc', 'aqi', 'robust_z', 'abs_robust_z', 'dominant_pollutant', 'is_validated', 'quality_flag'\n",
    "    )\n",
    "    .orderBy(F.desc('abs_robust_z'))\n",
    ")\n",
    "\n",
    "spike_pd = spikes.limit(200).toPandas()\n",
    "display(spike_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7581f89",
   "metadata": {},
   "source": [
    "## 10. Nowcasting Model (1–6 h Horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2602cae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 238:==============================================>      (398 + 2) / 457]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows: 249,696; Testing rows: 12,915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Feature engineering for modelling\n",
    "lag_window = Window.partitionBy('location_key').orderBy('ts_utc')\n",
    "feature_df = (\n",
    "    analysis_df\n",
    "    .withColumn('lag_1_aqi', F.lag('aqi', 1).over(lag_window))\n",
    "    .withColumn('lag_3_aqi', F.lag('aqi', 3).over(lag_window))\n",
    "    .withColumn('lag_6_aqi', F.lag('aqi', 6).over(lag_window))\n",
    "    .withColumn('lag_24_aqi', F.lag('aqi', 24).over(lag_window))\n",
    ")\n",
    "\n",
    "for horizon in range(1, 7):\n",
    "    feature_df = feature_df.withColumn(f'lead_{horizon}_aqi', F.lead('aqi', horizon).over(lag_window))\n",
    "\n",
    "stack_cols = [F.struct(F.lit(h).alias('horizon'), F.col(f'lead_{h}_aqi').alias('target_aqi')) for h in range(1, 7)]\n",
    "model_df = (\n",
    "    feature_df\n",
    "    .withColumn('future_targets', F.array(*stack_cols))\n",
    "    .withColumn('exploded', F.explode('future_targets'))\n",
    "    .select(\n",
    "        'location_key', 'location_name', 'ts_utc', 'date_utc', 'season', 'hour', 'dow', 'month',\n",
    "        'aqi', 'pm25', 'pm10', 'o3', 'no2', 'so2', 'co', 'is_validated', 'lag_1_aqi', 'lag_3_aqi', 'lag_6_aqi', 'lag_24_aqi',\n",
    "        'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos',\n",
    "        F.col('exploded.horizon').alias('horizon'),\n",
    "        F.col('exploded.target_aqi').alias('target_aqi')\n",
    "    )\n",
    "    .where(F.col('target_aqi').isNotNull())\n",
    "    .withColumn('is_validated', F.col('is_validated').cast('double'))\n",
    ")\n",
    "\n",
    "# Safely get max_date, handling empty/null results\n",
    "max_date_result = model_df.agg(F.max('date_utc')).collect()[0][0]\n",
    "if max_date_result is None:\n",
    "    import warnings\n",
    "    warnings.warn(\"No data available for nowcasting model; skipping train/test split.\")\n",
    "    train_df = model_df.limit(0)\n",
    "    test_df = model_df.limit(0)\n",
    "    print(\"Training rows: 0; Testing rows: 0\")\n",
    "else:\n",
    "    max_date = max_date_result\n",
    "    split_date = max_date - timedelta(days=30)\n",
    "    \n",
    "    train_df = model_df.where(F.col('date_utc') <= F.lit(split_date))\n",
    "    test_df = model_df.where(F.col('date_utc') > F.lit(split_date))\n",
    "    \n",
    "    print(f\"Training rows: {train_df.count():,}; Testing rows: {test_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6de196e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 17:20:47 ERROR Executor: Exception in task 0.0 in stage 279.0 (TID 33172)\n",
      "org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Vector values MUST NOT be NaN or Infinity, but got [15.0,52.70000076293945,76.0999984741211,14.0,29.299999237060547,27.5,873.0,NaN,NaN,NaN,NaN,0.0,NaN,NaN,0.0,1.0,NaN,NaN,NaN,NaN,1.0,1.0,0.0,0.0] SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_1_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:20:47 WARN TaskSetManager: Lost task 0.0 in stage 279.0 (TID 33172) (khoa-master executor driver): org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Vector values MUST NOT be NaN or Infinity, but got [15.0,52.70000076293945,76.0999984741211,14.0,29.299999237060547,27.5,873.0,NaN,NaN,NaN,NaN,0.0,NaN,NaN,0.0,1.0,NaN,NaN,NaN,NaN,1.0,1.0,0.0,0.0] SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_1_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/10/01 17:20:47 ERROR TaskSetManager: Task 0 in stage 279.0 failed 1 times; aborting job\n",
      "25/10/01 17:20:47 ERROR Instrumentation: org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Vector values MUST NOT be NaN or Infinity, but got [15.0,52.70000076293945,76.0999984741211,14.0,29.299999237060547,27.5,873.0,NaN,NaN,NaN,NaN,0.0,NaN,NaN,0.0,1.0,NaN,NaN,NaN,NaN,1.0,1.0,0.0,0.0] SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_1_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1500)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1473)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:334)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:57)\n",
      "\tat org.apache.spark.ml.regression.GBTRegressor.$anonfun$train$1(GBTRegressor.scala:190)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:167)\n",
      "\tat org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:57)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/10/01 17:20:47 ERROR Executor: Exception in task 0.0 in stage 279.0 (TID 33172)\n",
      "org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Vector values MUST NOT be NaN or Infinity, but got [15.0,52.70000076293945,76.0999984741211,14.0,29.299999237060547,27.5,873.0,NaN,NaN,NaN,NaN,0.0,NaN,NaN,0.0,1.0,NaN,NaN,NaN,NaN,1.0,1.0,0.0,0.0] SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_1_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/01 17:20:47 WARN TaskSetManager: Lost task 0.0 in stage 279.0 (TID 33172) (khoa-master executor driver): org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Vector values MUST NOT be NaN or Infinity, but got [15.0,52.70000076293945,76.0999984741211,14.0,29.299999237060547,27.5,873.0,NaN,NaN,NaN,NaN,0.0,NaN,NaN,0.0,1.0,NaN,NaN,NaN,NaN,1.0,1.0,0.0,0.0] SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_1_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/10/01 17:20:47 ERROR TaskSetManager: Task 0 in stage 279.0 failed 1 times; aborting job\n",
      "25/10/01 17:20:47 ERROR Instrumentation: org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Vector values MUST NOT be NaN or Infinity, but got [15.0,52.70000076293945,76.0999984741211,14.0,29.299999237060547,27.5,873.0,NaN,NaN,NaN,NaN,0.0,NaN,NaN,0.0,1.0,NaN,NaN,NaN,NaN,1.0,1.0,0.0,0.0] SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_1_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1500)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1473)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:334)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:57)\n",
      "\tat org.apache.spark.ml.regression.GBTRegressor.$anonfun$train$1(GBTRegressor.scala:190)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:167)\n",
      "\tat org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:57)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "ename": "SparkRuntimeException",
     "evalue": "[USER_RAISED_EXCEPTION] Vector values MUST NOT be NaN or Infinity, but got [15.0,52.70000076293945,76.0999984741211,14.0,29.299999237060547,27.5,873.0,NaN,NaN,NaN,NaN,0.0,NaN,NaN,0.0,1.0,NaN,NaN,NaN,NaN,1.0,1.0,0.0,0.0] SQLSTATE: P0001",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 35\u001b[0m\n\u001b[1;32m     24\u001b[0m regressor \u001b[38;5;241m=\u001b[39m GBTRegressor(\n\u001b[1;32m     25\u001b[0m     featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_aqi\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[location_indexer, season_indexer, assembler, regressor])\n\u001b[0;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_df)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     38\u001b[0m rmse_evaluator \u001b[38;5;241m=\u001b[39m RegressionEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_aqi\u001b[39m\u001b[38;5;124m'\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:138\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    136\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/util.py:164\u001b[0m, in \u001b[0;36mtry_remote_fit.<locals>.wrapped\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:411\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 411\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:407\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mSparkRuntimeException\u001b[0m: [USER_RAISED_EXCEPTION] Vector values MUST NOT be NaN or Infinity, but got [15.0,52.70000076293945,76.0999984741211,14.0,29.299999237060547,27.5,873.0,NaN,NaN,NaN,NaN,0.0,NaN,NaN,0.0,1.0,NaN,NaN,NaN,NaN,1.0,1.0,0.0,0.0] SQLSTATE: P0001"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Skip model training if no data available\n",
    "if train_df.count() == 0 or test_df.count() == 0:\n",
    "    import warnings\n",
    "    warnings.warn(\"Insufficient data for model training; skipping nowcasting model.\")\n",
    "    print(\"Skipping model training due to insufficient data.\")\n",
    "else:\n",
    "    location_indexer = StringIndexer(inputCol='location_name', outputCol='location_index', handleInvalid='keep')\n",
    "    season_indexer = StringIndexer(inputCol='season', outputCol='season_index', handleInvalid='keep')\n",
    "\n",
    "    feature_columns = [\n",
    "        'aqi', 'pm25', 'pm10', 'o3', 'no2', 'so2', 'co',\n",
    "        'lag_1_aqi', 'lag_3_aqi', 'lag_6_aqi', 'lag_24_aqi',\n",
    "        'hour', 'dow', 'month', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos',\n",
    "        'horizon', 'is_validated', 'location_index', 'season_index'\n",
    "    ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol='features', handleInvalid='keep')\n",
    "\n",
    "    regressor = GBTRegressor(\n",
    "        featuresCol='features',\n",
    "        labelCol='target_aqi',\n",
    "        maxIter=100,\n",
    "        maxDepth=6,\n",
    "        stepSize=0.05,\n",
    "        subsamplingRate=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[location_indexer, season_indexer, assembler, regressor])\n",
    "    model = pipeline.fit(train_df)\n",
    "    predictions = model.transform(test_df).cache()\n",
    "\n",
    "    rmse_evaluator = RegressionEvaluator(labelCol='target_aqi', predictionCol='prediction', metricName='rmse')\n",
    "    mae_evaluator = RegressionEvaluator(labelCol='target_aqi', predictionCol='prediction', metricName='mae')\n",
    "\n",
    "    overall_rmse = rmse_evaluator.evaluate(predictions)\n",
    "    overall_mae = mae_evaluator.evaluate(predictions)\n",
    "    print(f\"Overall RMSE: {overall_rmse:.2f}; Overall MAE: {overall_mae:.2f}\")\n",
    "\n",
    "    metrics_by_city = (\n",
    "        predictions\n",
    "        .groupBy('location_name')\n",
    "        .agg(\n",
    "            F.sqrt(F.mean(F.pow(F.col('prediction') - F.col('target_aqi'), 2))).alias('rmse'),\n",
    "            F.mean(F.abs(F.col('prediction') - F.col('target_aqi'))).alias('mae')\n",
    "        )\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    display(metrics_by_city)\n",
    "\n",
    "    metrics_by_horizon = (\n",
    "        predictions\n",
    "        .groupBy('horizon')\n",
    "        .agg(\n",
    "            F.sqrt(F.mean(F.pow(F.col('prediction') - F.col('target_aqi'), 2))).alias('rmse'),\n",
    "            F.mean(F.abs(F.col('prediction') - F.col('target_aqi'))).alias('mae')\n",
    "        )\n",
    "        .orderBy('horizon')\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    display(metrics_by_horizon)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(metrics_by_horizon['horizon'], metrics_by_horizon['rmse'], marker='o', label='RMSE')\n",
    "    ax.plot(metrics_by_horizon['horizon'], metrics_by_horizon['mae'], marker='s', label='MAE')\n",
    "    ax.set_xlabel('Forecast Horizon (hours ahead)')\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.set_title('Forecast error vs horizon')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / 'forecast_error_vs_horizon.png', dpi=150)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c02409",
   "metadata": {},
   "source": [
    "## 11. Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd4b8b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['mean_aqi'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m validated_pd \u001b[38;5;241m=\u001b[39m validated_daily\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m     44\u001b[0m validated_pd\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian_aqi_validated\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian_aqi\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m validated_kpi \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_kpis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidated_pd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m validated_kpi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscenario\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidated_only\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     47\u001b[0m scenarios\u001b[38;5;241m.\u001b[39mappend(validated_kpi)\n",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m, in \u001b[0;36mcompute_kpis\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_kpis\u001b[39m(df):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mdf\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocation_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmedian_aqi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedian_aqi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmean_aqi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean_aqi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpm25_exceed_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpm25_exceed_flag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtotal_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpm25_exceed_flag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompleteness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maqi_completeness\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;241m.\u001b[39massign(pm25_exceed_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m d: d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpm25_exceed_days\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_days\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_days\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     14\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py:1269\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1266\u001b[0m func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[1;32m   1268\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args, kwargs)\n\u001b[0;32m-> 1269\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:163\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(arg):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:403\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m     selected_obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_selected_obj\n\u001b[1;32m    401\u001b[0m     selection \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_selection\n\u001b[0;32m--> 403\u001b[0m arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[1;32m    406\u001b[0m context_manager: ContextManager\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:535\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    534\u001b[0m         cols_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(safe_sort(\u001b[38;5;28mlist\u001b[39m(cols)))\n\u001b[0;32m--> 535\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcols_sorted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    537\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['mean_aqi'] do not exist\""
     ]
    }
   ],
   "source": [
    "def compute_kpis(df):\n",
    "    return (\n",
    "        df\n",
    "        .groupby('location_name')\n",
    "        .agg(\n",
    "            median_aqi=('median_aqi', 'median'),\n",
    "            mean_aqi=('mean_aqi', 'mean'),\n",
    "            pm25_exceed_days=('pm25_exceed_flag', 'sum'),\n",
    "            total_days=('pm25_exceed_flag', 'count'),\n",
    "            completeness=('aqi_completeness', 'mean'),\n",
    "        )\n",
    "        .assign(pm25_exceed_rate=lambda d: d['pm25_exceed_days'] / d['total_days'] if d['total_days'].sum() > 0 else 0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "scenarios = []\n",
    "\n",
    "# Only run sensitivity analysis if we have data\n",
    "if len(daily_pd) > 0:\n",
    "    for threshold in [25, 35, 50]:\n",
    "        temp = daily_pd.copy()\n",
    "        temp['pm25_exceed_flag'] = temp[f'pm25_exceed_hours_{threshold}'] > 0\n",
    "        temp_kpi = compute_kpis(temp)\n",
    "        temp_kpi['scenario'] = f'pm25_threshold_{threshold}'\n",
    "        scenarios.append(temp_kpi)\n",
    "\n",
    "    cutoff = daily_pd['median_aqi'].quantile(0.99)\n",
    "    filtered = daily_pd[daily_pd['median_aqi'] <= cutoff].copy()\n",
    "    filtered['pm25_exceed_flag'] = filtered['pm25_exceed_hours_35'] > 0\n",
    "    filtered_kpi = compute_kpis(filtered)\n",
    "    filtered_kpi['scenario'] = 'remove_top_1pct_spikes'\n",
    "    scenarios.append(filtered_kpi)\n",
    "\n",
    "    validated_daily = (\n",
    "        analysis_df\n",
    "        .where(F.col('is_validated'))\n",
    "        .groupBy('location_name', 'date_utc')\n",
    "        .agg(F.expr('percentile_approx(aqi, 0.5, 1000)').alias('median_aqi_validated'),\n",
    "             F.avg(F.when(F.col('aqi').isNotNull(), 1.0).otherwise(0.0)).alias('aqi_completeness'),\n",
    "             F.sum(F.when(F.col('pm25') > 35, 1).otherwise(0)).alias('pm25_exceed_hours_35'))\n",
    "        .withColumn('pm25_exceed_flag', F.col('pm25_exceed_hours_35') > 0)\n",
    "    )\n",
    "    validated_pd = validated_daily.toPandas()\n",
    "    validated_pd.rename(columns={'median_aqi_validated': 'median_aqi'}, inplace=True)\n",
    "    validated_kpi = compute_kpis(validated_pd)\n",
    "    validated_kpi['scenario'] = 'validated_only'\n",
    "    scenarios.append(validated_kpi)\n",
    "\n",
    "    sensitivity_df = pd.concat(scenarios, ignore_index=True)\n",
    "    display(sensitivity_df)\n",
    "else:\n",
    "    import warnings\n",
    "    warnings.warn(\"No data available for sensitivity analysis.\")\n",
    "    print(\"Skipping sensitivity analysis due to insufficient data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9b5e17",
   "metadata": {},
   "source": [
    "## 12. Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d7fe01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:===================================================> (442 + 2) / 457]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key generators for narrative (inspect values before drafting final report):\n",
      "Latest observation timestamp: 2025-08-31 00:00:00\n",
      "Top dominant pollutant by city:\n",
      "     location_name dominant_pollutant     share\n",
      "0           Hà Nội                 co  0.999657\n",
      "1  TP. Hồ Chí Minh                 co  0.999657\n",
      "2          Đà Nẵng                 co  0.999657\n",
      "\n",
      "Dry-season median AQI ranking:\n",
      "location_name\n",
      "Hà Nội             6650.0\n",
      "TP. Hồ Chí Minh    5270.5\n",
      "Đà Nẵng            3297.0\n",
      "Name: median_aqi, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Helper summaries to feed narrative\n",
    "latest_period = scope_summary.loc[0, 'max_ts_utc']\n",
    "\n",
    "# Safely compute season/city metrics\n",
    "if len(daily_pd) > 0:\n",
    "    season_city = (\n",
    "        daily_pd[daily_pd['season'] == 'Dry']\n",
    "        .groupby('location_name')['median_aqi']\n",
    "        .median()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    worst_dry_city = season_city.index[0] if not season_city.empty else None\n",
    "    worst_dry_value = season_city.iloc[0] if not season_city.empty else None\n",
    "else:\n",
    "    season_city = pd.Series(dtype=float)\n",
    "    worst_dry_city = None\n",
    "    worst_dry_value = None\n",
    "\n",
    "# Use already computed pollutant_dominance (avoid calling toPandas on potentially broken Spark session)\n",
    "try:\n",
    "    pollutant_top = (pollutant_dominance\n",
    "                     .withColumn('rank', F.row_number().over(Window.partitionBy('location_name').orderBy(F.desc('share'))))\n",
    "                     .where(F.col('rank') == 1)\n",
    "                     .select('location_name', 'dominant_pollutant', 'share')\n",
    "                     .toPandas())\n",
    "except Exception as e:\n",
    "    import warnings\n",
    "    warnings.warn(f\"Unable to compute pollutant dominance: {e}\")\n",
    "    pollutant_top = pd.DataFrame(columns=['location_name', 'dominant_pollutant', 'share'])\n",
    "\n",
    "print(\"Key generators for narrative (inspect values before drafting final report):\")\n",
    "print(f\"Latest observation timestamp: {latest_period}\")\n",
    "print(\"Top dominant pollutant by city:\")\n",
    "print(pollutant_top)\n",
    "print(\"\\nDry-season median AQI ranking:\")\n",
    "print(season_city)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34767835",
   "metadata": {},
   "source": [
    "### Draft Findings (to be refined after execution)\n",
    "\n",
    "- **Peak Exposure** – Review the computed seasonal median AQI ranking (see helper output) and highlight the city & hour block with the worst air quality.\n",
    "- **Pollutant Drivers** – Use `pollutant_dominance` and component means to quantify the leading pollutant shares (typically PM₂.₅) and cite the proportion of hours dominated by that pollutant.\n",
    "- **Threshold Management** – Summarise exceedance streaks (`streaks`) focusing on longest runs and cumulative hours exceeding PM₂.₅ = 35 µg/m³.\n",
    "- **Data Reliability** – Quote the AQI completeness KPI and note any gaps uncovered by sensitivity analyses (validated-only vs full dataset).\n",
    "- **Operational Actions** – Translate modelling and sensitivity outputs into recommendations (e.g., targeted alerts during dry-season morning peaks, data remediation for low-completeness days)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c49ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "791c04f7",
   "metadata": {},
   "source": [
    "## ✅ Notebook Successfully Connected to HDFS Lakehouse\n",
    "\n",
    "### Configuration Fixed Using `.env`\n",
    "\n",
    "The notebook now successfully connects to the HDFS warehouse using environment variables from `.env`:\n",
    "\n",
    "```bash\n",
    "WAREHOUSE_URI=hdfs://khoa-master:9000/warehouse/iceberg\n",
    "```\n",
    "\n",
    "### Data Successfully Loaded\n",
    "\n",
    "- **✅ All required Gold tables detected:**\n",
    "  - `hadoop_catalog.aq.gold.fact_air_quality_hourly` (43,779 rows)\n",
    "  - `hadoop_catalog.aq.gold.dim_location` (3 cities)\n",
    "  - `hadoop_catalog.aq.gold.dim_calendar_date` (274 dates)\n",
    "  - `hadoop_catalog.aq.gold.dim_calendar_time` (24 hours)\n",
    "\n",
    "- **✅ Data range:** 2024-01-01 to 2025-08-31 (8 months)\n",
    "- **✅ Locations:** Hà Nội, TP. Hồ Chí Minh, Đà Nẵng\n",
    "- **✅ Daily aggregations:** 1,827 rows\n",
    "\n",
    "### Key Findings (Initial EDA)\n",
    "\n",
    "**AQI Values by City:**\n",
    "- **Hà Nội**: Median AQI = 6,746 (highest)\n",
    "- **TP. Hồ Chí Minh**: Median AQI = 5,916\n",
    "- **Đà Nẵng**: Median AQI = 3,585 (lowest)\n",
    "\n",
    "**Seasonal Patterns:**\n",
    "- **TP. Hồ Chí Minh** shows dramatic seasonal variation:\n",
    "  - Dry season: 5,270 median AQI\n",
    "  - Rainy season: 9,828 median AQI (87% increase!)\n",
    "- **Hà Nội** shows modest seasonal variation:\n",
    "  - Dry season: 6,650\n",
    "  - Rainy season: 7,217 (8.5% increase)\n",
    "- **Đà Nẵng** shows moderate seasonal variation:\n",
    "  - Dry season: 3,297\n",
    "  - Rainy season: 5,104 (55% increase)\n",
    "\n",
    "**Data Quality:**\n",
    "- 100% completeness across all cities\n",
    "- All hourly data points validated\n",
    "\n",
    "### Generated Visualizations\n",
    "\n",
    "The notebook has created these figures in `reports/figures/`:\n",
    "- `median_aqi_distribution.png` - KDE distribution plots by city\n",
    "- `diurnal_median_aqi.png` - Hourly AQI patterns (24h cycle)\n",
    "- `aqi_heatmap_hà_nội.png` - Hour × Day-of-Week heatmap\n",
    "- `aqi_heatmap_tp._hồ_chí_minh.png`\n",
    "- `aqi_heatmap_đà_nẵng.png`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "You can now continue with the full analysis pipeline:\n",
    "1. ✅ Data Quality Gate (Great Expectations)\n",
    "2. ✅ Exploratory Data Analysis\n",
    "3. Diagnostic Analytics (pollutant dominance, exceedances)\n",
    "4. Statistical Inference (bootstrap, Mann-Whitney)\n",
    "5. Anomaly Detection\n",
    "6. Nowcasting Model (1-6h horizons)\n",
    "7. Sensitivity Analysis\n",
    "\n",
    "**Note:** The AQI values appear very high (thousands instead of 0-500 range). This suggests the Gold table might be using raw pollutant concentrations instead of normalized AQI scores. This should be investigated in the Gold layer transformation logic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
