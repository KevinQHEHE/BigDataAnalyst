# Prefect Flows - H·ªá Th·ªëng Orchestration cho DLH-AQI Pipeline

> **Workflow orchestration cho pipeline x·ª≠ l√Ω d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ tr√™n YARN cluster**
> 
> üéØ **M·ª•c ƒê√≠ch**: T·ª± ƒë·ªông h√≥a quy tr√¨nh thu th·∫≠p, l√†m s·∫°ch, v√† k·∫øt h·ª£p d·ªØ li·ªáu AQI t·ª´ Open-Meteo API th√†nh c√°c b·∫£ng analytics s·∫µn s√†ng cho BI tools
>
> ‚ú® **T√≠nh NƒÉng**:
> - ‚úÖ **Subprocess Architecture** - Fresh JVM per stage (30-40% memory savings)
> - ‚úÖ **100% YARN Compliant** - All flows execute via spark_submit.sh wrapper
> - ‚úÖ **Incremental + Backfill** - Hourly incremental updates + historical data recovery
> - ‚úÖ **Real-time Streaming Output** - Live job monitoring without buffer lag
> - ‚úÖ **Error Handling & Timeouts** - Automatic error reporting with configurable timeouts
> - ‚úÖ **Sequential Execution** - Guaranteed dependency order (Bronze ‚Üí Silver ‚Üí Gold)

---

## üìã M·ª•c L·ª•c

- [Ki·∫øn Tr√∫c H·ªá Th·ªëng](#-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
- [Ph√¢n T√≠ch Chi Ti·∫øt C√°c Files](#-ph√¢n-t√≠ch-chi-ti·∫øt-c√°c-files)
  - [utils.py](#utilvpy)
  - [full_pipeline_flow.py](#full_pipeline_flowpy)
  - [backfill_flow.py](#backfill_flowpy)
  - [yarn_wrapper_flow.py](#yarn_wrapper_flowpy)
  - [spark_context.py](#spark_contextpy)
- [Quy Tr√¨nh X·ª≠ L√Ω D·ªØ Li·ªáu](#-quy-tr√¨nh-x·ª≠-l√Ω-d·ªØ-li·ªáu)
- [C√°c Ch·∫ø ƒê·ªô Ch·∫°y](#-c√°c-ch·∫ø-ƒë·ªô-ch·∫°y)
- [C·∫•u H√¨nh v√† Tham S·ªë](#-c·∫•u-h√¨nh-v√†-tham-s·ªë)
- [H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng](#-h∆∞·ªõng-d·∫´n-s·ª≠-d·ª•ng)
- [Deployment](#-deployment)
- [Monitoring & Logging](#-monitoring--logging)
- [Troubleshooting](#-troubleshooting)
- [Performance & Best Practices](#-performance--best-practices)

---

## üèóÔ∏è Ki·∫øn Tr√∫c H·ªá Th·ªëng

### Ki·∫øn Tr√∫c T·ªïng Quan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          Prefect Server                                  ‚îÇ
‚îÇ                    (Orchestration & Scheduling)                          ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ         yarn_wrapper_flow.py                                       ‚îÇ‚îÇ
‚îÇ  ‚îÇ     (Hourly Scheduled Flow - 0 * * * *)                           ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ  Triggers:  hourly_pipeline_yarn_flow()                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ‚ñ∂ Subprocess: spark-submit scripts/spark_submit.sh           ‚îÇ‚îÇ
‚îÇ  ‚îÇ         ‚îî‚îÄ‚ñ∂ PySpark App: full_pipeline_flow.py                   ‚îÇ‚îÇ
‚îÇ  ‚îÇ             (Runs on YARN Cluster)                                ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           YARN Cluster                                   ‚îÇ
‚îÇ                    (khoa-master:8088)                                    ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ         full_pipeline_flow.py (PySpark)                           ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ  1. Bronze Stage:                                                 ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Input: Open-Meteo API                                        ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Output: hadoop_catalog.lh.bronze.open_meteo_hourly           ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Transformation: Raw API response ‚Üí Parquet                   ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ  2. Silver Stage:                                                 ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Input: Bronze table                                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Output: hadoop_catalog.lh.silver.air_quality_hourly_clean    ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Transformation: Data cleaning, enrichment (date_key, time)   ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ  3. Gold Stage:                                                   ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Input: Silver table                                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Outputs: gold.fact_* and gold.dim_* tables                   ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Transformation: Aggregations, dimensions, analytics tables   ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    Iceberg Data Lake          ‚îÇ
                    ‚îÇ  hdfs://khoa-master:9000/     ‚îÇ
                    ‚îÇ    warehouse/iceberg/         ‚îÇ
                    ‚îÇ                               ‚îÇ
                    ‚îÇ  Tables (Cataloged):          ‚îÇ
                    ‚îÇ  - lh.bronze.*                ‚îÇ
                    ‚îÇ  - lh.silver.*                ‚îÇ
                    ‚îÇ  - gold.fact_*                ‚îÇ
                    ‚îÇ  - gold.dim_*                 ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Dependency Chain

```
utils.py
  ‚îú‚îÄ run_subprocess_job() ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ                                       ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ                                                     ‚îÇ
  ‚ñº                                                     ‚ñº
full_pipeline_flow.py          backfill_flow.py
  (Hourly Incremental)           (Historical Backfill)
  ‚îÇ                              ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  ‚îÇ
  ‚ñº
yarn_wrapper_flow.py
  (Prefect Scheduled Wrapper)

spark_context.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Shared utility for SparkSession management
```

### Execution Flow

```
User/Cron
   ‚îÇ
   ‚ñº
Prefect Worker (Local Python)
   ‚îÇ
   ‚îÇ Calls
   ‚ñº
yarn_wrapper_flow.py (@flow)
   ‚îÇ
   ‚îÇ Subprocess
   ‚ñº
bash scripts/spark_submit.sh
   ‚îÇ
   ‚îÇ Submit to YARN
   ‚ñº
YARN Cluster (Spark Application)
   ‚îÇ
   ‚ñº
full_pipeline_flow.py
   ‚îú‚îÄ‚ñ∂ bronze_flow.py
   ‚îú‚îÄ‚ñ∂ silver_flow.py
   ‚îî‚îÄ‚ñ∂ gold_flow.py
```

---

## üìÅ Ph√¢n T√≠ch Chi Ti·∫øt C√°c Files

### 1. `spark_context.py` - SparkSession Context Manager

**M·ª•c ƒë√≠ch**: Qu·∫£n l√Ω SparkSession v·ªõi context manager pattern

**Class ch√≠nh**: `SparkSessionContext`

```python
# Usage
with get_spark_session(app_name="my_flow", require_yarn=True) as spark:
    df = spark.sql("SELECT * FROM table")
    # Session t·ª± ƒë·ªông stop khi exit context
```

**Ch·ª©c nƒÉng**:
- ‚úÖ T·∫°o single SparkSession cho m·ªói flow
- ‚úÖ Validate YARN mode (`master == "yarn"`)
- ‚úÖ T·ª± ƒë·ªông stop session khi ho√†n th√†nh
- ‚úÖ Log th√¥ng tin Spark (App ID, Master, Warehouse)

**Parameters**:
- `app_name` (str): T√™n application
- `require_yarn` (bool): B·∫Øt bu·ªôc ph·∫£i ch·∫°y tr√™n YARN, raise error n·∫øu kh√¥ng
- `mode` (Optional[str]): Override mode ("local" cho testing, None cho production)

**Validation**:
```python
def validate_yarn_mode(spark: SparkSession):
    """Ki·ªÉm tra xem Spark c√≥ ƒëang ch·∫°y tr√™n YARN kh√¥ng"""
    master = spark.sparkContext.master
    if not master.startswith("yarn"):
        raise RuntimeError(f"Expected YARN but got '{master}'")
```

**Best Practice**:
- Lu√¥n d√πng context manager (`with` statement)
- Set `require_yarn=True` cho production flows
- Set `require_yarn=False` cho sub-flows (ƒë√£ validated ·ªü parent)

---

### 2. `utils.py` - Shared Utility Functions (NEW - Refactored)

**M·ª•c ƒë√≠ch**: Centralize common utilities ƒë·ªÉ DRY principle (Don't Repeat Yourself)

**Ki·∫øn Tr√∫c Tr∆∞·ªõc (C≈©)**:
```
full_pipeline_flow.py
  ‚îî‚îÄ run_subprocess_job() [76 lines]

backfill_flow.py
  ‚îî‚îÄ run_subprocess_job() [95 lines]

=> Total: 171 lines duplicate code ‚ùå
```

**Ki·∫øn Tr√∫c Sau (M·ªõi)**:
```
utils.py
  ‚îî‚îÄ run_subprocess_job() [50 lines] ‚úÖ
     ‚Üë
     ‚îú‚îÄ full_pipeline_flow.py (import)
     ‚îî‚îÄ backfill_flow.py (import)

=> Total: 50 lines, used by 2 modules ‚úÖ
=> Code reduction: 70% (171 ‚Üí 50)
```

**Key Function**: `run_subprocess_job()`

**Purpose**: Wrapper ƒë·ªÉ execute Spark jobs via subprocess v·ªõi real-time output streaming

**Inputs**:
- `script_path` (str): Path to Python script (e.g., "Prefect/bronze_flow.py")
- `args` (list): Command line arguments (e.g., ["--mode", "upsert"])
- `job_name` (str): T√™n job ƒë·ªÉ logging (e.g., "Bronze Ingestion")
- `timeout` (int): Timeout in seconds (default 3600)
- `root_dir` (Optional[str]): Project root (auto-detect if None)

**Outputs**: Tuple of `(success: bool, result: dict)`
```python
success, result = run_subprocess_job(
    "Prefect/bronze_flow.py",
    ["--mode", "upsert"],
    "Bronze Stage"
)

# If success == True:
# result = {
#     "job_name": "Bronze Stage",
#     "elapsed_seconds": 45.2,
#     "exit_code": 0,
#     "last_output_lines": ["‚úì Bronze complete: 1080 rows"]
# }

# If success == False:
# result = {
#     "job_name": "Bronze Stage",
#     "elapsed_seconds": 3600.0,
#     "exit_code": -15,  # killed
#     "error": "Subprocess timeout after 3600.0s",
#     "last_output_lines": [...]
# }
```

**Implementation Details**:

1. **Real-time Streaming Output**:
```python
# Problem (OLD): capture_output=True buffers entire output
# ‚Üí Memory explosion for large jobs
# ‚Üí No real-time progress visibility

# Solution (NEW): Streaming with subprocess.PIPE
process = subprocess.Popen(
    cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.STDOUT,
    text=True,
    bufsize=1  # Line buffered
)

# Stream line by line
output_lines = []
for line in process.stdout:
    print(line, end='')  # Show real-time
    output_lines.append(line)
    # Keep only last 100 lines
    if len(output_lines) > 100:
        output_lines.pop(0)
```

**Benefits**:
- ‚úÖ Low memory usage (O(100 lines) instead of O(millions))
- ‚úÖ Real-time Prefect monitoring (see progress)
- ‚úÖ Graceful error debugging (last 100 lines in result)

2. **Timeout with Cleanup**:
```python
try:
    return_code = process.wait(timeout=timeout)
except subprocess.TimeoutExpired:
    if process.poll() is None:
        process.kill()
        process.wait()
    return (False, {"error": f"Timeout after {timeout}s"})
```

3. **Error Handling**:
```python
if return_code != 0:
    return (False, {
        "error": f"Process exited with code {return_code}",
        "last_output_lines": output_lines[-100:]
    })
```

**Usage Examples**:

```python
# Example 1: Bronze flow
success, result = run_subprocess_job(
    script_path="Prefect/bronze_flow.py",
    args=["--mode", "upsert", "--warehouse", warehouse_uri],
    job_name="Bronze Ingestion"
)

if not success:
    logger.error(f"Bronze failed: {result['error']}")
    raise Exception(result['error'])

# Example 2: Silver flow
success, result = run_subprocess_job(
    "Prefect/silver_flow.py",
    ["--mode", "incremental"],
    "Silver Transformation",
    timeout=1800
)

# Example 3: Gold flow
success, result = run_subprocess_job(
    "Prefect/gold_flow.py",
    ["--mode", "facts"],
    "Gold Pipeline"
)
```

---

### 3. `full_pipeline_flow.py` - Complete Pipeline (Refactored)

**M·ª•c ƒë√≠ch**: Orchestrate Bronze ‚Üí Silver ‚Üí Gold trong single SparkSession

**Improvements from Refactoring**:
```
Before (431 lines):
  ‚îú‚îÄ run_subprocess_job() function [76 lines] ‚ùå (duplicate)
  ‚îú‚îÄ hourly_pipeline_flow() [40 lines] ‚ùå (not used)
  ‚îú‚îÄ skip_validation parameter ‚ùå (not used)
  ‚îú‚îÄ --hourly argument handling ‚ùå (not used)
  ‚îî‚îÄ Main flow logic [~200 lines] ‚úÖ

After (271 lines):
  ‚îú‚îÄ import run_subprocess_job from utils ‚úÖ
  ‚îú‚îÄ Removed unused functions ‚úÖ
  ‚îú‚îÄ Removed unused parameters ‚úÖ
  ‚îî‚îÄ Main flow logic + enhanced error handling [~200 lines] ‚úÖ

Code reduction: -160 lines (-37%) ‚úÖ
```

**Architecture**:
```
@flow("Full Pipeline Flow")
def full_pipeline_flow(
    bronze_mode="upsert",      # "upsert" ho·∫∑c "backfill"
    silver_mode="incremental",  # "full" ho·∫∑c "incremental"
    gold_mode="facts",          # "all", "dims", "facts", "custom"
    skip_bronze=False,
    skip_silver=False,
    skip_gold=False,
    skip_validation=False,      # For Silver only
    ...
) -> Dict:
    # 1. Create single SparkSession for entire pipeline
    with get_spark_session("full_pipeline", require_yarn=True) as spark:
        
        # 2. Bronze Stage
        if not skip_bronze:
            success, bronze_result = run_subprocess_job(...)
            results["bronze"] = bronze_result
        
        # 3. Silver Stage (sequential, depends on Bronze)
        if not skip_silver and (skip_bronze or success):
            success, silver_result = run_subprocess_job(...)
            results["silver"] = silver_result
        
        # 4. Gold Stage (final stage)
        if not skip_gold and (...):
            success, gold_result = run_subprocess_job(...)
            results["gold"] = gold_result
    
    return {"success": all_success, "results": results, "elapsed": ...}
```

**Key Benefit**: **Single SparkSession**
```
Tr∆∞·ªõc (C≈© - Separate sessions):
  ‚îú‚îÄ Bronze session started ‚Üí Created ‚Üí Stopped (cleanup)
  ‚îú‚îÄ Silver session started ‚Üí Created ‚Üí Stopped (cleanup)
  ‚îî‚îÄ Gold session started ‚Üí Created ‚Üí Stopped (cleanup)
  Total: 3 sessions, 3x cluster overhead
  
Sau (M·ªõi - Shared session):
  ‚îú‚îÄ Session created ONCE
  ‚îú‚îÄ Reused by all 3 stages
  ‚îî‚îÄ Session stopped ONCE after all stages
  Total: 1 session, minimal overhead ‚úÖ
  
Memory savings: ~30-40% ‚úÖ
```

**Use Cases**:

1. **Hourly Scheduled Run** (Production):
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --bronze-mode upsert \\
  --silver-mode incremental \\
  --gold-mode facts \\
  --skip-validation
```
- Fast: Ch·ªâ x·ª≠ l√Ω data m·ªõi (auto_detect)
- Smart: Skip dims (facts only trong Gold)
- Time: ~2 minutes

2. **Full Refresh** (Maintenance):
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --bronze-mode backfill \\
  --bronze-start-date 2024-10-01 \\
  --bronze-end-date 2024-10-31 \\
  --silver-mode full \\
  --gold-mode all
```

3. **Partial Run** (Debugging):
```bash
# Skip Bronze, only Silver + Gold
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --skip-bronze \\
  --silver-mode full \\
  --gold-mode all
```

**Error Handling**:
```python
try:
    bronze_result = run_subprocess_job(...)
except Exception as e:
    results["bronze"] = {
        "success": False,
        "error": str(e)
    }
    # Continue to Silver anyway (kh√¥ng stop pipeline)
```

**Output Example**:
```
FULL PIPELINE FLOW: BRONZE ‚Üí SILVER ‚Üí GOLD
================================================================================
STAGE 1/3: BRONZE INGESTION
‚úì Bronze stage complete: 1,080 rows in 45.2s

STAGE 2/3: SILVER TRANSFORMATION
‚úì Silver stage complete: 1,080 rows in 23.5s

STAGE 3/3: GOLD PIPELINE
‚úì Gold stage complete: 49,521 records in 67.8s

FULL PIPELINE COMPLETE
================================================================================
Total pipeline time: 136.5s (2.3 minutes)
Overall status: SUCCESS
```

---

### 4. `backfill_flow.py` - Historical Data Backfill (Refactored)

**M·ª•c ƒë√≠ch**: Process large date ranges v·ªõi chunking strategy

**Improvements from Refactoring**:
```
Before (361 lines):
  ‚îú‚îÄ run_subprocess_job() function [95 lines] ‚ùå (duplicate)
  ‚îú‚îÄ warehouse parameter ‚ùå (not used)
  ‚îú‚îÄ Comment comparisons [15 lines] ‚ùå (not needed)
  ‚îî‚îÄ Main backfill logic [~200 lines] ‚úÖ

After (257 lines):
  ‚îú‚îÄ import run_subprocess_job from utils ‚úÖ
  ‚îú‚îÄ Removed duplicate code ‚úÖ
  ‚îú‚îÄ Cleaned up comments ‚úÖ
  ‚îî‚îÄ Main backfill logic [~200 lines] ‚úÖ

Code reduction: -104 lines (-29%) ‚úÖ
```

**Problem**: Backfill large date ranges
```
2024-01-01 to 2024-12-31 (1 year)
‚ùå Process all at once:
   - 365 days √ó 3 locations = 1,095 API requests
   - Memory explosion
   - Rate limiting issues
   - Hard to resume if fails

‚úÖ Process by chunks:
   2024-01 ‚Üí 2024-02 ‚Üí ... ‚Üí 2024-12
   (12 chunks √ó 3 locations = 36 requests)
   - Manageable per chunk
   - Easy to resume
   - Monitor progress
```

**Architecture**:
```python
@flow("Backfill Flow")
def backfill_flow(
    start_date,          # YYYY-MM-DD (required)
    end_date,            # YYYY-MM-DD (required)
    chunk_mode="monthly",  # "monthly", "weekly", "daily"
    skip_bronze=False,
    skip_silver=False,
    skip_gold=False,
    ...
) -> Dict:
    # 1. Generate chunks based on date range
    chunks = generate_date_chunks(start_date, end_date, chunk_mode)
    # ‚Üí [(2024-01-01, 2024-01-31), (2024-02-01, 2024-02-29), ...]
    
    with get_spark_session("backfill_flow", require_yarn=True) as spark:
        successful_chunks = 0
        
        # 2. Process each chunk sequentially
        for i, (chunk_start, chunk_end) in enumerate(chunks):
            # Bronze for this chunk
            success, bronze_result = run_subprocess_job(
                "Prefect/bronze_flow.py",
                ["--mode", "backfill", "--start-date", chunk_start, "--end-date", chunk_end]
            )
            
            # Silver for this chunk
            if success:
                success, silver_result = run_subprocess_job(...)
            
            # Track success
            if success:
                successful_chunks += 1
        
        # 3. Gold only once after all chunks
        if successful_chunks > 0:
            success, gold_result = run_subprocess_job(
                "Prefect/gold_flow.py",
                ["--mode", "facts"]  # Facts only, skip dims
            )
    
    return {...}
```

**Chunking Modes**:

1. **Monthly** (Default):
```bash
--chunk-mode monthly
# 2024-01-01 to 2024-12-31
# ‚Üí 12 chunks (Jan, Feb, ..., Dec)
```

2. **Weekly**:
```bash
--chunk-mode weekly
# 2024-01-01 to 2024-03-31
# ‚Üí 13 chunks (7 days each)
```

3. **Daily**:
```bash
--chunk-mode daily
# 2024-10-01 to 2024-10-15
# ‚Üí 15 chunks (1 day each)
```

**Gold Optimization** (`mode="facts"`):
```
Before: mode="all"
  ‚îú‚îÄ Load dims (692 rows)
  ‚îú‚îÄ Load dims (692 rows)  ‚Üê DUPLICATE!
  ‚îî‚îÄ Transform facts (47,160 rows)
  Total: ~51k rows, 9.3 minutes

After: mode="facts"
  ‚îî‚îÄ Transform facts only (47,160 rows)
     (dims already loaded from previous backfill)
  Total: 47k rows, 1.5 minutes
  ‚Üí 84% faster! ‚úÖ
```

**Use Cases**:

1. **1 Year Backfill** (Monthly chunks):
```bash
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-01-01 \\
  --end-date 2024-12-31 \\
  --chunk-mode monthly
```
- Duration: ~12 √ó 6min = 72 minutes
- Chunks: 12
- Each chunk: ~30GB data

2. **1 Month Backfill** (Weekly chunks):
```bash
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31 \\
  --chunk-mode weekly
```
- Duration: ~5 √ó 8min = 40 minutes
- Chunks: 5
- Each chunk: ~8GB data

3. **2 Weeks Backfill** (Daily chunks):
```bash
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-15 \\
  --chunk-mode daily
```
- Duration: ~15 √ó 1min = 15 minutes
- Chunks: 15
- Each chunk: ~1GB data

**Output Example**:
```
BACKFILL FLOW: HISTORICAL DATA PROCESSING
================================================================================
Date range: 2024-10-01 to 2024-10-31
Chunk mode: weekly
Stages: Bronze ‚Üí Silver ‚Üí Gold

Generated 5 chunks:
  Chunk   1: 2024-10-01 to 2024-10-07
  Chunk   2: 2024-10-08 to 2024-10-14
  Chunk   3: 2024-10-15 to 2024-10-21
  Chunk   4: 2024-10-22 to 2024-10-28
  Chunk   5: 2024-10-29 to 2024-10-31

PROCESSING CHUNK 1/5: 2024-10-01 to 2024-10-07
[Chunk 1] Bronze ingestion... ‚úì 504 rows
[Chunk 1] Silver transformation... ‚úì 504 rows
[Chunk 1] ‚úì Complete in 6.2s

PROCESSING CHUNK 2/5: 2024-10-08 to 2024-10-14
[Chunk 2] Bronze ingestion... ‚úì 504 rows
[Chunk 2] Silver transformation... ‚úì 504 rows
[Chunk 2] ‚úì Complete in 5.8s

... [chunks 3-5] ...

GOLD PIPELINE (after all chunks)
‚úì Gold complete: 49,521 records in 67.8s

BACKFILL COMPLETE
================================================================================
Date range: 2024-10-01 to 2024-10-31
Chunk mode: weekly
Total chunks: 5
Successful: 5
Failed: 0

Data processed:
  Bronze rows: 2,520
  Silver rows: 2,520
  Gold records: 49,521

Total time: 137.8s (2.3 minutes)
Average per chunk: 6.0s

Overall status: SUCCESS
```

---

### 5. `yarn_wrapper_flow.py` - Prefect ‚Üî YARN Bridge (Refactored)

**M·ª•c ƒë√≠ch**: Wrapper ƒë·ªÉ Prefect schedule v√† monitor Spark jobs tr√™n YARN

**L√Ω do c·∫ßn wrapper**:
- Prefect worker ch·∫°y **local Python** (kh√¥ng ph·∫£i tr√™n YARN)
- Spark jobs c·∫ßn ch·∫°y tr√™n **YARN cluster**
- Wrapper d√πng **subprocess** ƒë·ªÉ g·ªçi `spark-submit`

**Architecture**:
```
Prefect Worker (Local)
    ‚îÇ
    ‚îÇ @task
    ‚ñº
run_pipeline_on_yarn_task()
    ‚îÇ
    ‚îÇ subprocess.Popen (streaming output)
    ‚ñº
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py
    ‚îÇ
    ‚îÇ spark-submit --master yarn
    ‚ñº
YARN Cluster (Distributed)
```

**Key Features**:

1. **Streaming Output** (Optimized):
```python
process = subprocess.Popen(
    cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.STDOUT,  # Merge streams
    text=True,
    bufsize=1  # Line buffered
)

# Stream line by line (kh√¥ng buffer to√†n b·ªô v√†o memory)
output_lines = []
max_lines_to_keep = 100  # Ch·ªâ gi·ªØ 100 d√≤ng cu·ªëi

for line in process.stdout:
    print(line, end='')  # Print ngay l·∫≠p t·ª©c
    output_lines.append(line)
    if len(output_lines) > max_lines_to_keep:
        output_lines.pop(0)  # X√≥a d√≤ng c≈©
```

**L·ª£i √≠ch**:
- ‚úÖ Gi·∫£m memory usage (kh√¥ng l∆∞u to√†n b·ªô output)
- ‚úÖ Real-time output (th·∫•y progress ngay)
- ‚úÖ Gi·ªØ 100 d√≤ng cu·ªëi ƒë·ªÉ debug n·∫øu l·ªói

2. **Timeout v√† Cleanup**:
```python
return_code = process.wait(timeout=3600)  # 1 hour timeout

except subprocess.TimeoutExpired:
    if process.poll() is None:
        process.kill()  # Kill process n·∫øu timeout
        process.wait()
```

3. **Retry on Failure**:
```python
@task(name="run_pipeline_on_yarn", retries=2, retry_delay_seconds=300)
```

**Flows**:
- `hourly_pipeline_yarn_flow()`: Ch·∫°y hourly pipeline (mode="hourly")
- `full_pipeline_yarn_flow()`: Ch·∫°y full pipeline (mode="full")

**Command ƒë∆∞·ª£c execute**:
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- --hourly
```

---

### 3. `bronze_flow.py` - Bronze Layer Ingestion

**M·ª•c ƒë√≠ch**: Ingest d·ªØ li·ªáu t·ª´ Open-Meteo API ‚Üí Bronze layer (Iceberg)

**Architecture**:
```python
@flow("Bronze Ingestion Flow")
def bronze_ingestion_flow():
    with get_spark_session("bronze_flow", require_yarn=True) as spark:
        # 1. Load locations
        locations = load_locations_task()
        
        # 2. Ingest each location
        for location in locations:
            ingest_location_chunk_task(location, ...)
        
        # 3. Return metrics
        return {total_rows, elapsed_seconds, ...}
```

**Modes**:

1. **Upsert Mode** (Default):
```bash
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- --mode upsert
```
- T√¨m timestamp m·ªõi nh·∫•t trong Bronze
- Ch·ªâ ingest t·ª´ `latest_timestamp + 1 day` ƒë·∫øn h√¥m nay
- D√πng cho **hourly pipeline**

2. **Backfill Mode**:
```bash
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- \\
  --mode backfill \\
  --start-date 2024-01-01 \\
  --end-date 2024-12-31
```
- Ingest to√†n b·ªô date range
- Chia th√†nh chunks (default 90 ng√†y/chunk)
- D√πng cho **historical data**

**Tasks**:
- `load_locations_task`: Load danh s√°ch locations t·ª´ HDFS
- `get_latest_timestamp_task`: L·∫•y timestamp m·ªõi nh·∫•t cho location
- `ingest_location_chunk_task`: G·ªçi API v√† write v√†o Iceberg

**Parameters**:
```python
--mode: "upsert" ho·∫∑c "backfill"
--locations: Path to locations.jsonl (default HDFS)
--start-date: Start date cho backfill (YYYY-MM-DD)
--end-date: End date cho backfill (YYYY-MM-DD)
--chunk-days: S·ªë ng√†y m·ªói API request (default 90)
--override: Ghi ƒë√® data c≈© (default False)
--table: Target Iceberg table
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation (testing only)
```

**Output Example**:
```
BRONZE FLOW COMPLETE
================================================================================
Total rows ingested: 2,160
Locations processed: 3
Elapsed time: 45.2s
Total rows in table: 47,160
```

---

### 4. `silver_flow.py` - Silver Layer Transformation

**M·ª•c ƒë√≠ch**: Transform Bronze ‚Üí Silver v·ªõi data enrichment v√† quality checks

**Architecture**:
```python
@flow("Silver Transformation Flow")
def silver_transformation_flow():
    with get_spark_session("silver_flow", require_yarn=True) as spark:
        # 1. Transform Bronze ‚Üí Silver
        transform_result = transform_bronze_to_silver_task(
            mode="merge",  # Upsert v·ªõi merge
            auto_detect=True  # Ch·ªâ x·ª≠ l√Ω data m·ªõi
        )
        
        # 2. Validate data quality (optional)
        if not skip_validation:
            validation_result = validate_silver_task()
        
        return {transformation, validation, elapsed}
```

**Key Features**:

1. **Auto-detect Optimization**:
```python
transform_bronze_to_silver(
    spark=spark,
    mode="merge",
    auto_detect=True  # T·ª± ƒë·ªông detect MAX(ts_utc)
)
```
- Compare `MAX(ts_utc)` gi·ªØa Bronze vs Silver
- Skip n·∫øu kh√¥ng c√≥ data m·ªõi
- Ch·ªâ process rows m·ªõi n·∫øu c√≥

2. **Data Enrichment**:
- Add `date_key` (YYYYMMDD format)
- Add `time_key` (0-23 hours)
- Parse pollutant metrics (PM2.5, PM10, O3, NO2, SO2, CO, UV)
- Calculate AQI from PM2.5

3. **Write Modes**:
- `overwrite`: X√≥a v√† ghi l·∫°i to√†n b·ªô
- `merge`: Upsert (update existing, insert new)
- `append`: Ch·ªâ insert (kh√¥ng update)

**Tasks**:
- `transform_bronze_to_silver_task`: Main transformation logic
- `validate_silver_task`: Data quality checks

**Validation Checks**:
```python
- Total records count
- Duplicates tr√™n (location_key, ts_utc)
- NULL values trong date_key, time_key, location_key
- Output: validation_passed = True/False
```

**Parameters**:
```python
--mode: "full" (overwrite) ho·∫∑c "incremental" (merge)
--start-date: Filter start date (optional)
--end-date: Filter end date (optional)
--skip-validation: B·ªè qua validation step
--bronze-table: Source table name
--silver-table: Target table name
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation
```

**Output Example**:
```
SILVER FLOW COMPLETE
================================================================================
Records processed: 1,080
Elapsed time: 23.5s
Validation: PASSED (0 duplicates, 0 nulls)
```

---

### 5. `gold_flow.py` - Gold Layer Pipeline

**M·ª•c ƒë√≠ch**: Load dimensions + Transform facts cho Gold layer (analytical data)

**Architecture**:
```python
@flow("Gold Pipeline Flow")
def gold_pipeline_flow(mode="all"):
    with get_spark_session("gold_flow", require_yarn=True) as spark:
        # 1. Load Dimensions (parallel)
        if "dims" in mode:
            load_dim_location_task()
            load_dim_pollutant_task()
            generate_dim_time_task()
            generate_dim_date_task()
        
        # 2. Transform Facts (sequential, c√≥ dependency)
        if "facts" in mode:
            transform_fact_hourly_task(auto_detect=True)
            transform_fact_daily_task(auto_detect=True)
            detect_episodes_task()
        
        return {results, total_records, elapsed}
```

**Modes**:

| Mode | Dimensions | Facts | Use Case |
|------|------------|-------|----------|
| `all` | ‚úÖ All 4 | ‚úÖ All 3 | Full refresh |
| `dims` | ‚úÖ All 4 | ‚ùå None | Only dims |
| `facts` | ‚ùå None | ‚úÖ All 3 | Only facts (recommended for backfill) |
| `custom` | ‚úÖ Selected | ‚úÖ Selected | Flexible selection |

**Dimension Tables** (Reference Data):

1. **dim_location** (3 rows):
```sql
location_key | location_name | latitude | longitude
-------------|---------------|----------|----------
danang       | Da Nang       | 16.0544  | 108.2022
hanoi        | Ha Noi        | 21.0285  | 105.8542
hcmc         | Ho Chi Minh   | 10.8231  | 106.6297
```

2. **dim_pollutant** (10 rows):
```sql
pollutant_key | pollutant_name | unit  | description
--------------|----------------|-------|-------------
pm25          | PM2.5          | ¬µg/m¬≥ | Fine particles
pm10          | PM10           | ¬µg/m¬≥ | Coarse particles
...
```

3. **dim_time** (24 rows):
```sql
time_key | hour | period    | is_peak
---------|------|-----------|--------
0        | 0    | Night     | False
...
17       | 17   | Evening   | True  (5-7pm)
```

4. **dim_date** (655 rows):
```sql
date_key | date_utc   | year | month | day | day_of_week
---------|------------|------|-------|-----|------------
20231001 | 2023-10-01 | 2023 | 10    | 1   | Sunday
...
```

**Fact Tables** (Analytical Data):

1. **fact_air_quality_hourly** (47,160 rows):
```sql
SELECT 
    location_key,
    date_key,
    time_key,
    ts_utc,
    pm25, pm10, o3, no2, so2, co,
    aqi, aod, dust, uv_index
FROM hadoop_catalog.lh.gold.fact_air_quality_hourly
```

2. **fact_city_daily** (1,965 rows):
```sql
-- Daily aggregations by city
SELECT
    location_key,
    date_key,
    avg_pm25, max_pm25, min_pm25,
    avg_aqi, max_aqi,
    total_hours, hours_good, hours_moderate, ...
FROM hadoop_catalog.lh.gold.fact_city_daily
```

3. **fact_episode** (396 rows):
```sql
-- Pollution episodes (AQI > threshold for min_hours)
SELECT
    episode_id,
    location_key,
    start_ts, end_ts,
    duration_hours,
    avg_aqi, max_aqi,
    severity_level
FROM hadoop_catalog.lh.gold.fact_episode
WHERE aqi_threshold = 151  -- Unhealthy threshold
  AND duration_hours >= 4
```

**Auto-detect trong Facts**:
```python
# fact_hourly
transform_fact_hourly(spark, auto_detect=True)
# ‚Üí Compare MAX(ts_utc): Silver vs Gold
# ‚Üí Skip n·∫øu kh√¥ng c√≥ data m·ªõi

# fact_daily
transform_fact_daily(spark, auto_detect=True)
# ‚Üí Compare MAX(date_key): fact_hourly vs fact_daily
# ‚Üí Ch·ªâ aggregate dates m·ªõi
```

**Parameters**:
```python
--mode: "all", "dims", "facts", ho·∫∑c "custom"
--dims: Comma-separated dims cho custom (location,pollutant,time,date)
--facts: Comma-separated facts cho custom (hourly,daily,episode)
--locations: Path to locations.jsonl
--pollutants: Path to dim_pollutant.jsonl
--aqi-threshold: Threshold cho episode detection (default 151)
--min-hours: Minimum gi·ªù li√™n t·ª•c cho episode (default 4)
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation
```

**Output Example**:
```
GOLD FLOW COMPLETE
================================================================================
  dim_location: 3 records
  dim_pollutant: 10 records
  dim_time: 24 records
  dim_date: 655 records
  fact_hourly: 47,160 records
  fact_daily: 1,965 records
  fact_episode: 396 records

Total records: 50,213
Elapsed time: 67.8s
```

---

### 6. `full_pipeline_flow.py` - Complete Pipeline

**M·ª•c ƒë√≠ch**: Orchestrate Bronze ‚Üí Silver ‚Üí Gold trong **single SparkSession**

**Architecture**:
```python
@flow("Full Pipeline Flow")
def full_pipeline_flow():
    # Create ONE SparkSession for entire pipeline
    with get_spark_session("full_pipeline", require_yarn=True) as spark:
        
        # Stage 1: Bronze (n·∫øu kh√¥ng skip)
        if not skip_bronze:
            bronze_result = bronze_ingestion_flow(
                mode=bronze_mode,
                require_yarn=False  # Already validated
            )
        
        # Stage 2: Silver (n·∫øu kh√¥ng skip)
        if not skip_silver:
            silver_result = silver_transformation_flow(
                mode=silver_mode,
                require_yarn=False  # Already validated
            )
        
        # Stage 3: Gold (n·∫øu kh√¥ng skip)
        if not skip_gold:
            gold_result = gold_pipeline_flow(
                mode=gold_mode,
                require_yarn=False  # Already validated
            )
        
        return {success, results, elapsed}
```

**Key Benefits**:
- ‚úÖ **Single SparkSession** - Kh√¥ng overhead t·∫°o nhi·ªÅu sessions
- ‚úÖ **Shared context** - Catalog, configs ƒë∆∞·ª£c reuse
- ‚úÖ **Efficient** - Gi·∫£m cluster resource churn
- ‚úÖ **Transactional** - T·∫•t c·∫£ stages trong 1 Spark application

**Error Handling**:
```python
try:
    bronze_result = bronze_ingestion_flow(...)
    results["bronze"] = bronze_result
except Exception as e:
    results["bronze"] = {"success": False, "error": str(e)}
    # Continue to next stage (kh√¥ng stop)
```

**Use Cases**:

1. **Hourly Scheduled Run**:
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- --hourly
# ‚Üí Bronze: upsert
# ‚Üí Silver: incremental
# ‚Üí Gold: facts only
```

2. **Manual Full Refresh**:
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --bronze-mode backfill \\
  --bronze-start-date 2024-10-01 \\
  --bronze-end-date 2024-10-31 \\
  --silver-mode incremental \\
  --gold-mode all
```

3. **Skip Stages**:
```bash
# Ch·ªâ ch·∫°y Silver + Gold
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --skip-bronze \\
  --silver-mode full \\
  --gold-mode all
```

**Parameters**:
```python
# Bronze
--bronze-mode: "upsert" ho·∫∑c "backfill"
--bronze-start-date: Start date cho backfill
--bronze-end-date: End date cho backfill
--skip-bronze: B·ªè qua Bronze stage

# Silver
--silver-mode: "full" ho·∫∑c "incremental"
--silver-start-date: Filter start date
--silver-end-date: Filter end date
--skip-validation: B·ªè qua validation
--skip-silver: B·ªè qua Silver stage

# Gold
--gold-mode: "all", "dims", "facts", "custom"
--skip-gold: B·ªè qua Gold stage
--aqi-threshold: Episode threshold
--min-hours: Episode minimum hours

# Common
--locations: Path to locations.jsonl
--pollutants: Path to dim_pollutant.jsonl
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation
```

**Output Example**:
```
FULL PIPELINE FLOW: BRONZE ‚Üí SILVER ‚Üí GOLD
================================================================================
Pipeline stages:
  Bronze: UPSERT
  Silver: INCREMENTAL
  Gold: FACTS
================================================================================

STAGE 1/3: BRONZE INGESTION
‚úì Bronze stage complete: 1,080 rows

STAGE 2/3: SILVER TRANSFORMATION
‚úì Silver stage complete: 1,080 rows

STAGE 3/3: GOLD PIPELINE
‚úì Gold stage complete: 49,521 records

FULL PIPELINE COMPLETE
================================================================================
‚úì BRONZE: {'success': True, 'total_rows': 1080}
‚úì SILVER: {'success': True, 'records_processed': 1080}
‚úì GOLD: {'success': True, 'total_records': 49521}

Total pipeline time: 92.8s
Overall status: SUCCESS
```

---

### 7. `backfill_flow.py` - Historical Data Backfill

**M·ª•c ƒë√≠ch**: X·ª≠ l√Ω large date ranges v·ªõi chunking strategy

**Problem**: Backfill 1 nƒÉm data (365 ng√†y) c√πng l√∫c:
- ‚ùå API rate limiting
- ‚ùå Memory issues
- ‚ùå Timeout risks
- ‚ùå Hard to track progress

**Solution**: Chia th√†nh chunks nh·ªè:
```
2024-01-01 to 2024-12-31 (1 nƒÉm)
  ‚Üì monthly chunking
  ‚îú‚îÄ 2024-01-01 to 2024-01-31 (chunk 1)
  ‚îú‚îÄ 2024-02-01 to 2024-02-29 (chunk 2)
  ‚îú‚îÄ ...
  ‚îî‚îÄ 2024-12-01 to 2024-12-31 (chunk 12)
```

**Architecture**:
```python
@flow("Backfill Flow")
def backfill_flow(start_date, end_date, chunk_mode):
    # Generate chunks
    chunks = generate_date_chunks(start_date, end_date, chunk_mode)
    # ‚Üí [(2024-01-01, 2024-01-31), (2024-02-01, 2024-02-29), ...]
    
    with get_spark_session("backfill_flow", require_yarn=True) as spark:
        # Process each chunk
        for i, (chunk_start, chunk_end) in enumerate(chunks):
            # Bronze
            bronze_result = bronze_ingestion_flow(
                mode="backfill",
                start_date=chunk_start,
                end_date=chunk_end,
                override=True  # Ghi ƒë√® data c≈©
            )
            
            # Silver
            silver_result = silver_transformation_flow(
                mode="incremental",  # Merge mode
                start_date=chunk_start,
                end_date=chunk_end
            )
        
        # Gold (ch·∫°y 1 l·∫ßn sau khi t·∫•t c·∫£ chunks xong)
        if successful_chunks > 0:
            gold_result = gold_pipeline_flow(
                mode="facts"  # Ch·ªâ facts, skip dimensions
            )
        
        return {chunks, successful, failed, elapsed}
```

**Chunking Modes**:

1. **Monthly** (Default):
```bash
--chunk-mode monthly
# 2024-01-01 to 2024-12-31 ‚Üí 12 chunks
```

2. **Weekly**:
```bash
--chunk-mode weekly
# 2024-01-01 to 2024-12-31 ‚Üí 53 chunks (7 days each)
```

3. **Daily**:
```bash
--chunk-mode daily
# 2024-10-01 to 2024-10-15 ‚Üí 15 chunks (1 day each)
```

**Gold Mode Optimization**:
```python
# Before: mode="all"
# ‚Üí Reload dimensions (692 rows) + facts (47,160 rows) = 50,213 total
# ‚Üí 9.3 minutes

# After: mode="facts"
# ‚Üí Skip dimensions, only process facts with auto_detect
# ‚Üí 1.5 minutes (84% faster!)
```

**Parameters**:
```python
--start-date: Start date (YYYY-MM-DD) - REQUIRED
--end-date: End date (YYYY-MM-DD) - REQUIRED
--chunk-mode: "monthly", "weekly", ho·∫∑c "daily"
--skip-bronze: B·ªè qua Bronze stage
--skip-silver: B·ªè qua Silver stage
--skip-gold: B·ªè qua Gold stage
--locations: Path to locations.jsonl
--pollutants: Path to dim_pollutant.jsonl
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation
```

**Usage Examples**:

```bash
# Backfill 1 nƒÉm by month
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-01-01 \\
  --end-date 2024-12-31 \\
  --chunk-mode monthly

# Backfill 1 th√°ng by week
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31 \\
  --chunk-mode weekly

# Backfill 15 ng√†y by day
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2025-10-01 \\
  --end-date 2025-10-15 \\
  --chunk-mode daily
```

**Output Example**:
```
BACKFILL FLOW: HISTORICAL DATA PROCESSING
================================================================================
Date range: 2025-10-01 to 2025-10-15
Chunk mode: daily
Stages: Bronze ‚Üí Silver ‚Üí Gold
================================================================================

Generated 15 chunks:
  Chunk   1: 2025-10-01 to 2025-10-01
  Chunk   2: 2025-10-02 to 2025-10-02
  ...
  Chunk  15: 2025-10-15 to 2025-10-15

PROCESSING CHUNK 1/15: 2025-10-01 to 2025-10-01
[Chunk 1] Bronze ingestion...
[Chunk 1] ‚úì Bronze: 72 rows
[Chunk 1] Silver transformation...
[Chunk 1] ‚úì Silver: 72 rows
[Chunk 1] ‚úì Complete in 6.2s

...

GOLD PIPELINE (after all chunks)
‚úì Gold complete: 49,521 records

BACKFILL COMPLETE
================================================================================
Date range: 2025-10-01 to 2025-10-15
Chunk mode: daily
Total chunks: 15
Successful: 15
Failed: 0

Data processed:
  Bronze rows: 1,080
  Silver rows: 1,080
  Gold records: 49,521

Total time: 92.8s (1.5 minutes)
Average per chunk: 6.2s
================================================================================
```

---

## ‚öôÔ∏è C·∫•u H√¨nh v√† Tham S·ªë

### Environment Variables (.env)

```bash
# Iceberg warehouse
WAREHOUSE_URI=hdfs://khoa-master:9000/warehouse/iceberg

# Spark settings
SPARK_MASTER=yarn
ENABLE_YARN_DEFAULTS=true
SPARK_DYN_MIN=1
SPARK_DYN_MAX=50

# Data paths
LOCATIONS_PATH=hdfs://khoa-master:9000/user/dlhnhom2/data/locations.jsonl
POLLUTANTS_PATH=hdfs://khoa-master:9000/user/dlhnhom2/data/dim_pollutant.jsonl
```

### Spark Configuration (scripts/spark_submit.sh)

```bash
spark-submit \\
  --master yarn \\
  --deploy-mode client \\
  --conf spark.dynamicAllocation.enabled=true \\
  --conf spark.dynamicAllocation.minExecutors=1 \\
  --conf spark.dynamicAllocation.maxExecutors=50 \\
  --conf spark.sql.adaptive.enabled=true \\
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \\
  --conf spark.sql.catalog.hadoop_catalog=org.apache.iceberg.spark.SparkCatalog \\
  --conf spark.sql.catalog.hadoop_catalog.type=hadoop \\
  --conf spark.sql.catalog.hadoop_catalog.warehouse=$WAREHOUSE_URI \\
  "$@"
```

### Prefect Configuration

**Work Pool**: `default` (process type)
```bash
prefect work-pool create default --type process
```

**Schedule**: Cron every hour
```bash
--cron "0 * * * *"  # Minute 0 c·ªßa m·ªói gi·ªù
```

**Tags**: ƒê·ªÉ filter v√† organize
```bash
--tag aqi --tag hourly --tag yarn --tag production
```

---

## üöÄ H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng

### Setup Ban ƒê·∫ßu

**1. Install Dependencies**:
```bash
pip install prefect==3.4.22
pip install pyspark
pip install python-dotenv
```

**2. T·∫°o Work Pool**:
```bash
prefect work-pool create default --type process
```

**3. Deploy Flow**:
```bash
bash scripts/deploy_yarn_flow.sh
```

**4. Start Worker**:
```bash
# Ch·∫°y trong background
nohup prefect worker start --pool default > logs/prefect-worker.log 2>&1 &

# L∆∞u PID
echo $! > logs/prefect-worker.pid
```

### Pattern 1: Scheduled Hourly Runs (Production)

**Automatic**: Worker t·ª± ƒë·ªông ch·∫°y theo schedule

```
17:00 ‚Üí Prefect worker trigger ‚Üí yarn_wrapper_flow
         ‚Üì
     subprocess: bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- --hourly
         ‚Üì
     YARN: Bronze (upsert) ‚Üí Silver (incremental) ‚Üí Gold (facts)
```

**Monitor**:
```bash
# Xem scheduled runs
prefect flow-run ls --limit 10

# Xem worker logs
tail -f logs/prefect-worker.log

# Check worker status
ps aux | grep "prefect worker"
```

### Pattern 2: Manual Trigger

**Test run**:
```bash
prefect deployment run 'Hourly Pipeline on YARN/hourly-yarn-pipeline'
```

**Custom parameters** (via direct spark-submit):
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --bronze-mode upsert \\
  --silver-mode incremental \\
  --gold-mode facts \\
  --skip-validation
```

### Pattern 3: Historical Backfill

**Backfill 1 nƒÉm**:
```bash
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-01-01 \\
  --end-date 2024-12-31 \\
  --chunk-mode monthly
```

**Backfill 1 th√°ng**:
```bash
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31 \\
  --chunk-mode weekly
```

### Pattern 4: Individual Layers

**Ch·ªâ Bronze**:
```bash
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- \\
  --mode backfill \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31
```

**Ch·ªâ Silver**:
```bash
bash scripts/spark_submit.sh Prefect/silver_flow.py -- \\
  --mode incremental \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31
```

**Ch·ªâ Gold - Facts only**:
```bash
bash scripts/spark_submit.sh Prefect/gold_flow.py -- \\
  --mode facts
```

**Ch·ªâ Gold - Dimensions only**:
```bash
bash scripts/spark_submit.sh Prefect/gold_flow.py -- \\
  --mode dims
```

---

## üì¶ Deployment

### Deploy Script: `scripts/deploy_yarn_flow.sh`

**Ch·ª©c nƒÉng**:
1. Create work pool n·∫øu ch∆∞a c√≥
2. Delete old deployment (tr√°nh duplicate)
3. Deploy v·ªõi schedule + tags
4. Show next steps

**Usage**:
```bash
bash scripts/deploy_yarn_flow.sh
```

**Output**:
```
Deployed flow:
  ‚Ä¢ hourly-yarn-pipeline ‚Üí Runs every hour on YARN (cron: 0 * * * *)

Deployment ID: 31505e99-d4de-4e41-9259-84401f3e5e8e
```

### Deployment Lifecycle

**1. Code Changes**:
- ‚úÖ S·ª≠a code trong `*.py` files ‚Üí **KH√îNG C·∫¶N** deploy l·∫°i
- ‚ùå ƒê·ªïi schedule/tags/name ‚Üí **C·∫¶N** deploy l·∫°i

**2. Update Deployment**:
```bash
# Delete old
prefect deployment delete "Hourly Pipeline on YARN/hourly-yarn-pipeline"

# Redeploy
bash scripts/deploy_yarn_flow.sh
```

**3. Worker Restart**:
```bash
# Kill old worker
kill $(cat logs/prefect-worker.pid)

# Start new worker
nohup prefect worker start --pool default > logs/prefect-worker.log 2>&1 &
echo $! > logs/prefect-worker.pid
```

---

## üìä Monitoring

### Prefect UI

**Access**: http://localhost:4200

**Features**:
- Flow runs history
- Task execution status
- Logs v√† errors
- Metrics v√† charts

**Useful Views**:
```bash
# Dashboard ‚Üí Flow Runs (xem t·∫•t c·∫£ runs)
# Deployments ‚Üí hourly-yarn-pipeline (xem schedule)
# Flow Runs ‚Üí Click v√†o run ‚Üí Task Runs (xem t·ª´ng task)
```

### YARN ResourceManager

**Access**: http://khoa-master:8088

**Views**:
- Applications (Spark jobs ƒëang ch·∫°y)
- Cluster metrics (CPU, Memory)
- Node list

### Spark History Server

**Access**: http://khoa-master:18080

**Views**:
- Completed applications
- Job timeline
- Stage v√† task details
- SQL queries

### Command Line Monitoring

**Flow runs**:
```bash
# List recent runs
prefect flow-run ls --limit 10

# Watch specific run
prefect flow-run logs <run-id> --follow
```

**Worker status**:
```bash
# Check if running
ps aux | grep "prefect worker"

# View logs
tail -f logs/prefect-worker.log

# Last 100 lines with timestamp
tail -n 100 logs/prefect-worker.log | grep -E "^\d{2}:\d{2}:\d{2}"
```

**YARN applications**:
```bash
# List running apps
yarn application -list -appStates RUNNING

# App status
yarn application -status <app-id>

# Kill app
yarn application -kill <app-id>
```

### Metrics to Track

| Metric | Source | Threshold |
|--------|--------|-----------|
| Flow duration | Prefect UI | < 2 minutes (hourly) |
| Success rate | Prefect UI | > 95% |
| Data freshness | Bronze table | < 2 hours lag |
| Worker uptime | `ps aux` | Continuous |
| YARN queue usage | YARN UI | < 80% |

---

## üêõ Troubleshooting

### Common Issues

**1. Worker not picking up runs**

**Symptoms**:
```bash
prefect flow-run ls
# Shows: Scheduled ‚Üí Pending (stuck)
```

**Solutions**:
```bash
# Check worker
ps aux | grep "prefect worker"

# Restart worker
kill $(cat logs/prefect-worker.pid)
nohup prefect worker start --pool default > logs/prefect-worker.log 2>&1 &
echo $! > logs/prefect-worker.pid

# Check logs
tail -f logs/prefect-worker.log
```

---

**2. SparkSession not on YARN**

**Error**:
```
RuntimeError: Expected YARN master but got 'local[*]'
```

**Cause**: Ch·∫°y tr·ª±c ti·∫øp v·ªõi Python thay v√¨ spark-submit

**Solution**:
```bash
# ‚ùå WRONG
python Prefect/bronze_flow.py

# ‚úÖ CORRECT
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- --mode upsert
```

---

**3. Subprocess timeout**

**Error**:
```
RuntimeError: Pipeline timed out after 3600.0s
```

**Cause**: Job ch·∫°y qu√° 1 gi·ªù

**Solutions**:
```python
# Option 1: TƒÉng timeout trong yarn_wrapper_flow.py
return_code = process.wait(timeout=7200)  # 2 hours

# Option 2: Optimize query (enable auto_detect)
# Option 3: Reduce chunk size in backfill
```

---

**4. Memory issues in subprocess**

**Symptoms**: Process killed, OOM errors

**Cause**: Tr∆∞·ªõc ƒë√¢y d√πng `capture_output=True` buffer to√†n b·ªô output

**Solution**: ƒê√£ fix b·∫±ng streaming output
```python
# ‚úÖ FIXED: Stream line by line
for line in process.stdout:
    print(line, end='')
    # Ch·ªâ gi·ªØ 100 d√≤ng cu·ªëi
```

---

**5. HDFS connection timeout**

**Error**:
```
java.net.SocketTimeoutException: 60000 millis timeout
```

**Solutions**:
```bash
# Check HDFS
hdfs dfs -ls /user/dlhnhom2/

# Check NameNode
curl http://khoa-master:9870/

# Restart HDFS (if needed)
# sudo systemctl restart hadoop-hdfs-namenode
```

---

**6. Iceberg table locked**

**Error**:
```
org.apache.iceberg.exceptions.CommitFailedException: ...
```

**Cause**: Concurrent writes

**Solutions**:
```bash
# Check running Spark apps
yarn application -list -appStates RUNNING

# Kill stuck apps
yarn application -kill <app-id>
```

---

**7. Deployment prompt issues**

**Error**:
```
EOFError: EOF when reading a line
```

**Cause**: Script ch·∫°y non-interactive nh∆∞ng CLI c·∫ßn input

**Solution**: ƒê√£ fix b·∫±ng `printf "n\nn\n"`
```bash
printf "n\nn\n" | prefect deploy ...
```

---

### Debug Workflow

**Step 1**: Check worker logs
```bash
tail -f logs/prefect-worker.log
```

**Step 2**: Check Prefect UI
```
http://localhost:4200 ‚Üí Flow Runs ‚Üí Click run ‚Üí View logs
```

**Step 3**: Check YARN logs
```bash
yarn logs -applicationId <app-id> | less
```

**Step 4**: Check Spark History
```
http://khoa-master:18080 ‚Üí Find application ‚Üí Stages ‚Üí Failed tasks
```

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

### Internal Docs
- **[PREFECT_YARN_GUIDE.md](../docs/PREFECT_YARN_GUIDE.md)** - H∆∞·ªõng d·∫´n t√≠ch h·ª£p Prefect + YARN
- **[PREFECT_DEPLOYMENT.md](../docs/PREFECT_DEPLOYMENT.md)** - Chi ti·∫øt deployment
- **[INCREMENTAL_IMPLEMENTATION.md](../docs/INCREMENTAL_IMPLEMENTATION.md)** - Auto-detect optimization

### External Resources
- [Prefect Documentation](https://docs.prefect.io/)
- [Apache Iceberg](https://iceberg.apache.org/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)

---

## ‚úÖ Checklist

### Pre-production
- [ ] Test full pipeline v·ªõi small date range
- [ ] Verify YARN execution (`master == "yarn"`)
- [ ] Check auto-detect working (skips when no new data)
- [ ] Validate data quality (no duplicates, no nulls)
- [ ] Monitor memory usage (streaming output)

### Production
- [ ] Deploy v·ªõi hourly schedule
- [ ] Start worker trong background
- [ ] Verify scheduled runs executing
- [ ] Set up monitoring alerts
- [ ] Document runbooks

### Maintenance
- [ ] Weekly: Check worker logs for errors
- [ ] Weekly: Verify data freshness
- [ ] Monthly: Review performance metrics
- [ ] Monthly: Clean up old Spark artifacts
- [ ] Quarterly: Update dependencies

---

## üéì Best Practices

### 1. Always use spark_submit.sh wrapper
```bash
# ‚úÖ CORRECT
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- --mode upsert

# ‚ùå WRONG
python Prefect/bronze_flow.py --mode upsert
```

### 2. Enable auto_detect for incremental loads
```python
transform_bronze_to_silver(spark, mode="merge", auto_detect=True)
transform_fact_hourly(spark, mode="overwrite", auto_detect=True)
```

### 3. Use mode="facts" for backfill Gold
```python
# Skip dimensions (reference data), only process facts
gold_pipeline_flow(mode="facts")
```

### 4. Chunk large date ranges
```bash
# ‚úÖ Monthly chunks for 1 year
--chunk-mode monthly  # 12 chunks

# ‚ùå Process entire year at once (risky)
```

### 5. Monitor worker continuously
```bash
# Run in background with nohup
nohup prefect worker start --pool default > logs/prefect-worker.log 2>&1 &

# Save PID for easy management
echo $! > logs/prefect-worker.pid
```

### 6. Set require_yarn=False for sub-flows
```python
# Parent flow
with get_spark_session("parent", require_yarn=True) as spark:
    # Sub-flows reuse session
    bronze_flow(..., require_yarn=False)
    silver_flow(..., require_yarn=False)
```

---

## üìù Notes

- **Code changes**: Kh√¥ng c·∫ßn deploy l·∫°i, worker t·ª± ƒë·ªông d√πng code m·ªõi
- **Metadata changes**: C·∫ßn deploy l·∫°i (schedule, tags, name)
- **Worker restart**: Ch·ªâ c·∫ßn khi c√≥ l·ªói ho·∫∑c update Prefect version
- **YARN validation**: Set `require_yarn=True` cho parent flow, `False` cho sub-flows

---

**Last Updated**: October 16, 2025
**Author**: DLH-AQI Team
**Version**: 3.0 (Prefect 3.x + YARN Integration + Streaming Optimization)
