# Prefect Flows - H·ªá Th·ªëng Orchestration cho DLH-AQI Pipeline

> **Workflow orchestration cho pipeline x·ª≠ l√Ω d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ tr√™n YARN cluster**

Th∆∞ m·ª•c n√†y ch·ª©a c√°c Prefect flows ƒë·ªÉ ƒëi·ªÅu ph·ªëi pipeline Bronze ‚Üí Silver ‚Üí Gold v·ªõi:
- ‚úÖ **Single SparkSession** - M·ªôt session duy nh·∫•t cho m·ªói flow
- ‚úÖ **YARN Integration** - Ch·∫°y tr√™n Hadoop cluster qua subprocess wrapper
- ‚úÖ **Auto-detect Optimization** - Ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu m·ªõi
- ‚úÖ **Streaming Output** - Gi·∫£m memory usage khi ch·∫°y jobs l·ªõn
- ‚úÖ **Automatic Retry** - T·ª± ƒë·ªông retry khi l·ªói
- ‚úÖ **Hourly Scheduling** - T·ª± ƒë·ªông ch·∫°y m·ªói gi·ªù

---

## üìã M·ª•c L·ª•c

- [Ki·∫øn Tr√∫c H·ªá Th·ªëng](#-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
- [Chi Ti·∫øt C√°c Files](#-chi-ti·∫øt-c√°c-files)
- [C·∫•u H√¨nh v√† Tham S·ªë](#-c·∫•u-h√¨nh-v√†-tham-s·ªë)
- [H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng](#-h∆∞·ªõng-d·∫´n-s·ª≠-d·ª•ng)
- [Deployment](#-deployment)
- [Monitoring](#-monitoring)
- [Troubleshooting](#-troubleshooting)

---

## üèóÔ∏è Ki·∫øn Tr√∫c H·ªá Th·ªëng

### Flow Dependency Graph

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 yarn_wrapper_flow.py                       ‚îÇ
‚îÇ         (Prefect Scheduler ‚Üí YARN Executor)                ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Subprocess: bash scripts/spark_submit.sh           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ      full_pipeline_flow.py                 ‚îÇ    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ  (Single SparkSession tr√™n YARN)           ‚îÇ    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ                                            ‚îÇ    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ  ‚îÇ Bronze   ‚îÇ‚îÄ‚ñ∂‚îÇ Silver   ‚îÇ‚îÄ‚ñ∂‚îÇ  Gold    ‚îÇ‚îÇ    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Alternative: backfill_flow.py (cho historical data)
```

### Execution Flow

```
User/Cron
   ‚îÇ
   ‚ñº
Prefect Worker (Local Python)
   ‚îÇ
   ‚îÇ Calls
   ‚ñº
yarn_wrapper_flow.py (@flow)
   ‚îÇ
   ‚îÇ Subprocess
   ‚ñº
bash scripts/spark_submit.sh
   ‚îÇ
   ‚îÇ Submit to YARN
   ‚ñº
YARN Cluster (Spark Application)
   ‚îÇ
   ‚ñº
full_pipeline_flow.py
   ‚îú‚îÄ‚ñ∂ bronze_flow.py
   ‚îú‚îÄ‚ñ∂ silver_flow.py
   ‚îî‚îÄ‚ñ∂ gold_flow.py
```

---

## üìÅ Chi Ti·∫øt C√°c Files

### 1. `spark_context.py` - SparkSession Context Manager

**M·ª•c ƒë√≠ch**: Qu·∫£n l√Ω SparkSession v·ªõi context manager pattern

**Class ch√≠nh**: `SparkSessionContext`

```python
# Usage
with get_spark_session(app_name="my_flow", require_yarn=True) as spark:
    df = spark.sql("SELECT * FROM table")
    # Session t·ª± ƒë·ªông stop khi exit context
```

**Ch·ª©c nƒÉng**:
- ‚úÖ T·∫°o single SparkSession cho m·ªói flow
- ‚úÖ Validate YARN mode (`master == "yarn"`)
- ‚úÖ T·ª± ƒë·ªông stop session khi ho√†n th√†nh
- ‚úÖ Log th√¥ng tin Spark (App ID, Master, Warehouse)

**Parameters**:
- `app_name` (str): T√™n application
- `require_yarn` (bool): B·∫Øt bu·ªôc ph·∫£i ch·∫°y tr√™n YARN, raise error n·∫øu kh√¥ng
- `mode` (Optional[str]): Override mode ("local" cho testing, None cho production)

**Validation**:
```python
def validate_yarn_mode(spark: SparkSession):
    """Ki·ªÉm tra xem Spark c√≥ ƒëang ch·∫°y tr√™n YARN kh√¥ng"""
    master = spark.sparkContext.master
    if not master.startswith("yarn"):
        raise RuntimeError(f"Expected YARN but got '{master}'")
```

**Best Practice**:
- Lu√¥n d√πng context manager (`with` statement)
- Set `require_yarn=True` cho production flows
- Set `require_yarn=False` cho sub-flows (ƒë√£ validated ·ªü parent)

---

### 2. `yarn_wrapper_flow.py` - Prefect ‚Üî YARN Bridge

**M·ª•c ƒë√≠ch**: Wrapper ƒë·ªÉ Prefect schedule v√† monitor Spark jobs tr√™n YARN

**L√Ω do c·∫ßn wrapper**:
- Prefect worker ch·∫°y **local Python** (kh√¥ng ph·∫£i tr√™n YARN)
- Spark jobs c·∫ßn ch·∫°y tr√™n **YARN cluster**
- Wrapper d√πng **subprocess** ƒë·ªÉ g·ªçi `spark-submit`

**Architecture**:
```
Prefect Worker (Local)
    ‚îÇ
    ‚îÇ @task
    ‚ñº
run_pipeline_on_yarn_task()
    ‚îÇ
    ‚îÇ subprocess.Popen (streaming output)
    ‚ñº
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py
    ‚îÇ
    ‚îÇ spark-submit --master yarn
    ‚ñº
YARN Cluster (Distributed)
```

**Key Features**:

1. **Streaming Output** (Optimized):
```python
process = subprocess.Popen(
    cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.STDOUT,  # Merge streams
    text=True,
    bufsize=1  # Line buffered
)

# Stream line by line (kh√¥ng buffer to√†n b·ªô v√†o memory)
output_lines = []
max_lines_to_keep = 100  # Ch·ªâ gi·ªØ 100 d√≤ng cu·ªëi

for line in process.stdout:
    print(line, end='')  # Print ngay l·∫≠p t·ª©c
    output_lines.append(line)
    if len(output_lines) > max_lines_to_keep:
        output_lines.pop(0)  # X√≥a d√≤ng c≈©
```

**L·ª£i √≠ch**:
- ‚úÖ Gi·∫£m memory usage (kh√¥ng l∆∞u to√†n b·ªô output)
- ‚úÖ Real-time output (th·∫•y progress ngay)
- ‚úÖ Gi·ªØ 100 d√≤ng cu·ªëi ƒë·ªÉ debug n·∫øu l·ªói

2. **Timeout v√† Cleanup**:
```python
return_code = process.wait(timeout=3600)  # 1 hour timeout

except subprocess.TimeoutExpired:
    if process.poll() is None:
        process.kill()  # Kill process n·∫øu timeout
        process.wait()
```

3. **Retry on Failure**:
```python
@task(name="run_pipeline_on_yarn", retries=2, retry_delay_seconds=300)
```

**Flows**:
- `hourly_pipeline_yarn_flow()`: Ch·∫°y hourly pipeline (mode="hourly")
- `full_pipeline_yarn_flow()`: Ch·∫°y full pipeline (mode="full")

**Command ƒë∆∞·ª£c execute**:
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- --hourly
```

---

### 3. `bronze_flow.py` - Bronze Layer Ingestion

**M·ª•c ƒë√≠ch**: Ingest d·ªØ li·ªáu t·ª´ Open-Meteo API ‚Üí Bronze layer (Iceberg)

**Architecture**:
```python
@flow("Bronze Ingestion Flow")
def bronze_ingestion_flow():
    with get_spark_session("bronze_flow", require_yarn=True) as spark:
        # 1. Load locations
        locations = load_locations_task()
        
        # 2. Ingest each location
        for location in locations:
            ingest_location_chunk_task(location, ...)
        
        # 3. Return metrics
        return {total_rows, elapsed_seconds, ...}
```

**Modes**:

1. **Upsert Mode** (Default):
```bash
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- --mode upsert
```
- T√¨m timestamp m·ªõi nh·∫•t trong Bronze
- Ch·ªâ ingest t·ª´ `latest_timestamp + 1 day` ƒë·∫øn h√¥m nay
- D√πng cho **hourly pipeline**

2. **Backfill Mode**:
```bash
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- \\
  --mode backfill \\
  --start-date 2024-01-01 \\
  --end-date 2024-12-31
```
- Ingest to√†n b·ªô date range
- Chia th√†nh chunks (default 90 ng√†y/chunk)
- D√πng cho **historical data**

**Tasks**:
- `load_locations_task`: Load danh s√°ch locations t·ª´ HDFS
- `get_latest_timestamp_task`: L·∫•y timestamp m·ªõi nh·∫•t cho location
- `ingest_location_chunk_task`: G·ªçi API v√† write v√†o Iceberg

**Parameters**:
```python
--mode: "upsert" ho·∫∑c "backfill"
--locations: Path to locations.jsonl (default HDFS)
--start-date: Start date cho backfill (YYYY-MM-DD)
--end-date: End date cho backfill (YYYY-MM-DD)
--chunk-days: S·ªë ng√†y m·ªói API request (default 90)
--override: Ghi ƒë√® data c≈© (default False)
--table: Target Iceberg table
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation (testing only)
```

**Output Example**:
```
BRONZE FLOW COMPLETE
================================================================================
Total rows ingested: 2,160
Locations processed: 3
Elapsed time: 45.2s
Total rows in table: 47,160
```

---

### 4. `silver_flow.py` - Silver Layer Transformation

**M·ª•c ƒë√≠ch**: Transform Bronze ‚Üí Silver v·ªõi data enrichment v√† quality checks

**Architecture**:
```python
@flow("Silver Transformation Flow")
def silver_transformation_flow():
    with get_spark_session("silver_flow", require_yarn=True) as spark:
        # 1. Transform Bronze ‚Üí Silver
        transform_result = transform_bronze_to_silver_task(
            mode="merge",  # Upsert v·ªõi merge
            auto_detect=True  # Ch·ªâ x·ª≠ l√Ω data m·ªõi
        )
        
        # 2. Validate data quality (optional)
        if not skip_validation:
            validation_result = validate_silver_task()
        
        return {transformation, validation, elapsed}
```

**Key Features**:

1. **Auto-detect Optimization**:
```python
transform_bronze_to_silver(
    spark=spark,
    mode="merge",
    auto_detect=True  # T·ª± ƒë·ªông detect MAX(ts_utc)
)
```
- Compare `MAX(ts_utc)` gi·ªØa Bronze vs Silver
- Skip n·∫øu kh√¥ng c√≥ data m·ªõi
- Ch·ªâ process rows m·ªõi n·∫øu c√≥

2. **Data Enrichment**:
- Add `date_key` (YYYYMMDD format)
- Add `time_key` (0-23 hours)
- Parse pollutant metrics (PM2.5, PM10, O3, NO2, SO2, CO, UV)
- Calculate AQI from PM2.5

3. **Write Modes**:
- `overwrite`: X√≥a v√† ghi l·∫°i to√†n b·ªô
- `merge`: Upsert (update existing, insert new)
- `append`: Ch·ªâ insert (kh√¥ng update)

**Tasks**:
- `transform_bronze_to_silver_task`: Main transformation logic
- `validate_silver_task`: Data quality checks

**Validation Checks**:
```python
- Total records count
- Duplicates tr√™n (location_key, ts_utc)
- NULL values trong date_key, time_key, location_key
- Output: validation_passed = True/False
```

**Parameters**:
```python
--mode: "full" (overwrite) ho·∫∑c "incremental" (merge)
--start-date: Filter start date (optional)
--end-date: Filter end date (optional)
--skip-validation: B·ªè qua validation step
--bronze-table: Source table name
--silver-table: Target table name
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation
```

**Output Example**:
```
SILVER FLOW COMPLETE
================================================================================
Records processed: 1,080
Elapsed time: 23.5s
Validation: PASSED (0 duplicates, 0 nulls)
```

---

### 5. `gold_flow.py` - Gold Layer Pipeline

**M·ª•c ƒë√≠ch**: Load dimensions + Transform facts cho Gold layer (analytical data)

**Architecture**:
```python
@flow("Gold Pipeline Flow")
def gold_pipeline_flow(mode="all"):
    with get_spark_session("gold_flow", require_yarn=True) as spark:
        # 1. Load Dimensions (parallel)
        if "dims" in mode:
            load_dim_location_task()
            load_dim_pollutant_task()
            generate_dim_time_task()
            generate_dim_date_task()
        
        # 2. Transform Facts (sequential, c√≥ dependency)
        if "facts" in mode:
            transform_fact_hourly_task(auto_detect=True)
            transform_fact_daily_task(auto_detect=True)
            detect_episodes_task()
        
        return {results, total_records, elapsed}
```

**Modes**:

| Mode | Dimensions | Facts | Use Case |
|------|------------|-------|----------|
| `all` | ‚úÖ All 4 | ‚úÖ All 3 | Full refresh |
| `dims` | ‚úÖ All 4 | ‚ùå None | Only dims |
| `facts` | ‚ùå None | ‚úÖ All 3 | Only facts (recommended for backfill) |
| `custom` | ‚úÖ Selected | ‚úÖ Selected | Flexible selection |

**Dimension Tables** (Reference Data):

1. **dim_location** (3 rows):
```sql
location_key | location_name | latitude | longitude
-------------|---------------|----------|----------
danang       | Da Nang       | 16.0544  | 108.2022
hanoi        | Ha Noi        | 21.0285  | 105.8542
hcmc         | Ho Chi Minh   | 10.8231  | 106.6297
```

2. **dim_pollutant** (10 rows):
```sql
pollutant_key | pollutant_name | unit  | description
--------------|----------------|-------|-------------
pm25          | PM2.5          | ¬µg/m¬≥ | Fine particles
pm10          | PM10           | ¬µg/m¬≥ | Coarse particles
...
```

3. **dim_time** (24 rows):
```sql
time_key | hour | period    | is_peak
---------|------|-----------|--------
0        | 0    | Night     | False
...
17       | 17   | Evening   | True  (5-7pm)
```

4. **dim_date** (655 rows):
```sql
date_key | date_utc   | year | month | day | day_of_week
---------|------------|------|-------|-----|------------
20231001 | 2023-10-01 | 2023 | 10    | 1   | Sunday
...
```

**Fact Tables** (Analytical Data):

1. **fact_air_quality_hourly** (47,160 rows):
```sql
SELECT 
    location_key,
    date_key,
    time_key,
    ts_utc,
    pm25, pm10, o3, no2, so2, co,
    aqi, aod, dust, uv_index
FROM hadoop_catalog.lh.gold.fact_air_quality_hourly
```

2. **fact_city_daily** (1,965 rows):
```sql
-- Daily aggregations by city
SELECT
    location_key,
    date_key,
    avg_pm25, max_pm25, min_pm25,
    avg_aqi, max_aqi,
    total_hours, hours_good, hours_moderate, ...
FROM hadoop_catalog.lh.gold.fact_city_daily
```

3. **fact_episode** (396 rows):
```sql
-- Pollution episodes (AQI > threshold for min_hours)
SELECT
    episode_id,
    location_key,
    start_ts, end_ts,
    duration_hours,
    avg_aqi, max_aqi,
    severity_level
FROM hadoop_catalog.lh.gold.fact_episode
WHERE aqi_threshold = 151  -- Unhealthy threshold
  AND duration_hours >= 4
```

**Auto-detect trong Facts**:
```python
# fact_hourly
transform_fact_hourly(spark, auto_detect=True)
# ‚Üí Compare MAX(ts_utc): Silver vs Gold
# ‚Üí Skip n·∫øu kh√¥ng c√≥ data m·ªõi

# fact_daily
transform_fact_daily(spark, auto_detect=True)
# ‚Üí Compare MAX(date_key): fact_hourly vs fact_daily
# ‚Üí Ch·ªâ aggregate dates m·ªõi
```

**Parameters**:
```python
--mode: "all", "dims", "facts", ho·∫∑c "custom"
--dims: Comma-separated dims cho custom (location,pollutant,time,date)
--facts: Comma-separated facts cho custom (hourly,daily,episode)
--locations: Path to locations.jsonl
--pollutants: Path to dim_pollutant.jsonl
--aqi-threshold: Threshold cho episode detection (default 151)
--min-hours: Minimum gi·ªù li√™n t·ª•c cho episode (default 4)
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation
```

**Output Example**:
```
GOLD FLOW COMPLETE
================================================================================
  dim_location: 3 records
  dim_pollutant: 10 records
  dim_time: 24 records
  dim_date: 655 records
  fact_hourly: 47,160 records
  fact_daily: 1,965 records
  fact_episode: 396 records

Total records: 50,213
Elapsed time: 67.8s
```

---

### 6. `full_pipeline_flow.py` - Complete Pipeline

**M·ª•c ƒë√≠ch**: Orchestrate Bronze ‚Üí Silver ‚Üí Gold trong **single SparkSession**

**Architecture**:
```python
@flow("Full Pipeline Flow")
def full_pipeline_flow():
    # Create ONE SparkSession for entire pipeline
    with get_spark_session("full_pipeline", require_yarn=True) as spark:
        
        # Stage 1: Bronze (n·∫øu kh√¥ng skip)
        if not skip_bronze:
            bronze_result = bronze_ingestion_flow(
                mode=bronze_mode,
                require_yarn=False  # Already validated
            )
        
        # Stage 2: Silver (n·∫øu kh√¥ng skip)
        if not skip_silver:
            silver_result = silver_transformation_flow(
                mode=silver_mode,
                require_yarn=False  # Already validated
            )
        
        # Stage 3: Gold (n·∫øu kh√¥ng skip)
        if not skip_gold:
            gold_result = gold_pipeline_flow(
                mode=gold_mode,
                require_yarn=False  # Already validated
            )
        
        return {success, results, elapsed}
```

**Key Benefits**:
- ‚úÖ **Single SparkSession** - Kh√¥ng overhead t·∫°o nhi·ªÅu sessions
- ‚úÖ **Shared context** - Catalog, configs ƒë∆∞·ª£c reuse
- ‚úÖ **Efficient** - Gi·∫£m cluster resource churn
- ‚úÖ **Transactional** - T·∫•t c·∫£ stages trong 1 Spark application

**Error Handling**:
```python
try:
    bronze_result = bronze_ingestion_flow(...)
    results["bronze"] = bronze_result
except Exception as e:
    results["bronze"] = {"success": False, "error": str(e)}
    # Continue to next stage (kh√¥ng stop)
```

**Use Cases**:

1. **Hourly Scheduled Run**:
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- --hourly
# ‚Üí Bronze: upsert
# ‚Üí Silver: incremental
# ‚Üí Gold: facts only
```

2. **Manual Full Refresh**:
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --bronze-mode backfill \\
  --bronze-start-date 2024-10-01 \\
  --bronze-end-date 2024-10-31 \\
  --silver-mode incremental \\
  --gold-mode all
```

3. **Skip Stages**:
```bash
# Ch·ªâ ch·∫°y Silver + Gold
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --skip-bronze \\
  --silver-mode full \\
  --gold-mode all
```

**Parameters**:
```python
# Bronze
--bronze-mode: "upsert" ho·∫∑c "backfill"
--bronze-start-date: Start date cho backfill
--bronze-end-date: End date cho backfill
--skip-bronze: B·ªè qua Bronze stage

# Silver
--silver-mode: "full" ho·∫∑c "incremental"
--silver-start-date: Filter start date
--silver-end-date: Filter end date
--skip-validation: B·ªè qua validation
--skip-silver: B·ªè qua Silver stage

# Gold
--gold-mode: "all", "dims", "facts", "custom"
--skip-gold: B·ªè qua Gold stage
--aqi-threshold: Episode threshold
--min-hours: Episode minimum hours

# Common
--locations: Path to locations.jsonl
--pollutants: Path to dim_pollutant.jsonl
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation
```

**Output Example**:
```
FULL PIPELINE FLOW: BRONZE ‚Üí SILVER ‚Üí GOLD
================================================================================
Pipeline stages:
  Bronze: UPSERT
  Silver: INCREMENTAL
  Gold: FACTS
================================================================================

STAGE 1/3: BRONZE INGESTION
‚úì Bronze stage complete: 1,080 rows

STAGE 2/3: SILVER TRANSFORMATION
‚úì Silver stage complete: 1,080 rows

STAGE 3/3: GOLD PIPELINE
‚úì Gold stage complete: 49,521 records

FULL PIPELINE COMPLETE
================================================================================
‚úì BRONZE: {'success': True, 'total_rows': 1080}
‚úì SILVER: {'success': True, 'records_processed': 1080}
‚úì GOLD: {'success': True, 'total_records': 49521}

Total pipeline time: 92.8s
Overall status: SUCCESS
```

---

### 7. `backfill_flow.py` - Historical Data Backfill

**M·ª•c ƒë√≠ch**: X·ª≠ l√Ω large date ranges v·ªõi chunking strategy

**Problem**: Backfill 1 nƒÉm data (365 ng√†y) c√πng l√∫c:
- ‚ùå API rate limiting
- ‚ùå Memory issues
- ‚ùå Timeout risks
- ‚ùå Hard to track progress

**Solution**: Chia th√†nh chunks nh·ªè:
```
2024-01-01 to 2024-12-31 (1 nƒÉm)
  ‚Üì monthly chunking
  ‚îú‚îÄ 2024-01-01 to 2024-01-31 (chunk 1)
  ‚îú‚îÄ 2024-02-01 to 2024-02-29 (chunk 2)
  ‚îú‚îÄ ...
  ‚îî‚îÄ 2024-12-01 to 2024-12-31 (chunk 12)
```

**Architecture**:
```python
@flow("Backfill Flow")
def backfill_flow(start_date, end_date, chunk_mode):
    # Generate chunks
    chunks = generate_date_chunks(start_date, end_date, chunk_mode)
    # ‚Üí [(2024-01-01, 2024-01-31), (2024-02-01, 2024-02-29), ...]
    
    with get_spark_session("backfill_flow", require_yarn=True) as spark:
        # Process each chunk
        for i, (chunk_start, chunk_end) in enumerate(chunks):
            # Bronze
            bronze_result = bronze_ingestion_flow(
                mode="backfill",
                start_date=chunk_start,
                end_date=chunk_end,
                override=True  # Ghi ƒë√® data c≈©
            )
            
            # Silver
            silver_result = silver_transformation_flow(
                mode="incremental",  # Merge mode
                start_date=chunk_start,
                end_date=chunk_end
            )
        
        # Gold (ch·∫°y 1 l·∫ßn sau khi t·∫•t c·∫£ chunks xong)
        if successful_chunks > 0:
            gold_result = gold_pipeline_flow(
                mode="facts"  # Ch·ªâ facts, skip dimensions
            )
        
        return {chunks, successful, failed, elapsed}
```

**Chunking Modes**:

1. **Monthly** (Default):
```bash
--chunk-mode monthly
# 2024-01-01 to 2024-12-31 ‚Üí 12 chunks
```

2. **Weekly**:
```bash
--chunk-mode weekly
# 2024-01-01 to 2024-12-31 ‚Üí 53 chunks (7 days each)
```

3. **Daily**:
```bash
--chunk-mode daily
# 2024-10-01 to 2024-10-15 ‚Üí 15 chunks (1 day each)
```

**Gold Mode Optimization**:
```python
# Before: mode="all"
# ‚Üí Reload dimensions (692 rows) + facts (47,160 rows) = 50,213 total
# ‚Üí 9.3 minutes

# After: mode="facts"
# ‚Üí Skip dimensions, only process facts with auto_detect
# ‚Üí 1.5 minutes (84% faster!)
```

**Parameters**:
```python
--start-date: Start date (YYYY-MM-DD) - REQUIRED
--end-date: End date (YYYY-MM-DD) - REQUIRED
--chunk-mode: "monthly", "weekly", ho·∫∑c "daily"
--skip-bronze: B·ªè qua Bronze stage
--skip-silver: B·ªè qua Silver stage
--skip-gold: B·ªè qua Gold stage
--locations: Path to locations.jsonl
--pollutants: Path to dim_pollutant.jsonl
--warehouse: Iceberg warehouse URI
--no-yarn-check: Skip YARN validation
```

**Usage Examples**:

```bash
# Backfill 1 nƒÉm by month
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-01-01 \\
  --end-date 2024-12-31 \\
  --chunk-mode monthly

# Backfill 1 th√°ng by week
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31 \\
  --chunk-mode weekly

# Backfill 15 ng√†y by day
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2025-10-01 \\
  --end-date 2025-10-15 \\
  --chunk-mode daily
```

**Output Example**:
```
BACKFILL FLOW: HISTORICAL DATA PROCESSING
================================================================================
Date range: 2025-10-01 to 2025-10-15
Chunk mode: daily
Stages: Bronze ‚Üí Silver ‚Üí Gold
================================================================================

Generated 15 chunks:
  Chunk   1: 2025-10-01 to 2025-10-01
  Chunk   2: 2025-10-02 to 2025-10-02
  ...
  Chunk  15: 2025-10-15 to 2025-10-15

PROCESSING CHUNK 1/15: 2025-10-01 to 2025-10-01
[Chunk 1] Bronze ingestion...
[Chunk 1] ‚úì Bronze: 72 rows
[Chunk 1] Silver transformation...
[Chunk 1] ‚úì Silver: 72 rows
[Chunk 1] ‚úì Complete in 6.2s

...

GOLD PIPELINE (after all chunks)
‚úì Gold complete: 49,521 records

BACKFILL COMPLETE
================================================================================
Date range: 2025-10-01 to 2025-10-15
Chunk mode: daily
Total chunks: 15
Successful: 15
Failed: 0

Data processed:
  Bronze rows: 1,080
  Silver rows: 1,080
  Gold records: 49,521

Total time: 92.8s (1.5 minutes)
Average per chunk: 6.2s
================================================================================
```

---

## ‚öôÔ∏è C·∫•u H√¨nh v√† Tham S·ªë

### Environment Variables (.env)

```bash
# Iceberg warehouse
WAREHOUSE_URI=hdfs://khoa-master:9000/warehouse/iceberg

# Spark settings
SPARK_MASTER=yarn
ENABLE_YARN_DEFAULTS=true
SPARK_DYN_MIN=1
SPARK_DYN_MAX=50

# Data paths
LOCATIONS_PATH=hdfs://khoa-master:9000/user/dlhnhom2/data/locations.jsonl
POLLUTANTS_PATH=hdfs://khoa-master:9000/user/dlhnhom2/data/dim_pollutant.jsonl
```

### Spark Configuration (scripts/spark_submit.sh)

```bash
spark-submit \\
  --master yarn \\
  --deploy-mode client \\
  --conf spark.dynamicAllocation.enabled=true \\
  --conf spark.dynamicAllocation.minExecutors=1 \\
  --conf spark.dynamicAllocation.maxExecutors=50 \\
  --conf spark.sql.adaptive.enabled=true \\
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \\
  --conf spark.sql.catalog.hadoop_catalog=org.apache.iceberg.spark.SparkCatalog \\
  --conf spark.sql.catalog.hadoop_catalog.type=hadoop \\
  --conf spark.sql.catalog.hadoop_catalog.warehouse=$WAREHOUSE_URI \\
  "$@"
```

### Prefect Configuration

**Work Pool**: `default` (process type)
```bash
prefect work-pool create default --type process
```

**Schedule**: Cron every hour
```bash
--cron "0 * * * *"  # Minute 0 c·ªßa m·ªói gi·ªù
```

**Tags**: ƒê·ªÉ filter v√† organize
```bash
--tag aqi --tag hourly --tag yarn --tag production
```

---

## üöÄ H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng

### Setup Ban ƒê·∫ßu

**1. Install Dependencies**:
```bash
pip install prefect==3.4.22
pip install pyspark
pip install python-dotenv
```

**2. T·∫°o Work Pool**:
```bash
prefect work-pool create default --type process
```

**3. Deploy Flow**:
```bash
bash scripts/deploy_yarn_flow.sh
```

**4. Start Worker**:
```bash
# Ch·∫°y trong background
nohup prefect worker start --pool default > logs/prefect-worker.log 2>&1 &

# L∆∞u PID
echo $! > logs/prefect-worker.pid
```

### Pattern 1: Scheduled Hourly Runs (Production)

**Automatic**: Worker t·ª± ƒë·ªông ch·∫°y theo schedule

```
17:00 ‚Üí Prefect worker trigger ‚Üí yarn_wrapper_flow
         ‚Üì
     subprocess: bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- --hourly
         ‚Üì
     YARN: Bronze (upsert) ‚Üí Silver (incremental) ‚Üí Gold (facts)
```

**Monitor**:
```bash
# Xem scheduled runs
prefect flow-run ls --limit 10

# Xem worker logs
tail -f logs/prefect-worker.log

# Check worker status
ps aux | grep "prefect worker"
```

### Pattern 2: Manual Trigger

**Test run**:
```bash
prefect deployment run 'Hourly Pipeline on YARN/hourly-yarn-pipeline'
```

**Custom parameters** (via direct spark-submit):
```bash
bash scripts/spark_submit.sh Prefect/full_pipeline_flow.py -- \\
  --bronze-mode upsert \\
  --silver-mode incremental \\
  --gold-mode facts \\
  --skip-validation
```

### Pattern 3: Historical Backfill

**Backfill 1 nƒÉm**:
```bash
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-01-01 \\
  --end-date 2024-12-31 \\
  --chunk-mode monthly
```

**Backfill 1 th√°ng**:
```bash
bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31 \\
  --chunk-mode weekly
```

### Pattern 4: Individual Layers

**Ch·ªâ Bronze**:
```bash
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- \\
  --mode backfill \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31
```

**Ch·ªâ Silver**:
```bash
bash scripts/spark_submit.sh Prefect/silver_flow.py -- \\
  --mode incremental \\
  --start-date 2024-10-01 \\
  --end-date 2024-10-31
```

**Ch·ªâ Gold - Facts only**:
```bash
bash scripts/spark_submit.sh Prefect/gold_flow.py -- \\
  --mode facts
```

**Ch·ªâ Gold - Dimensions only**:
```bash
bash scripts/spark_submit.sh Prefect/gold_flow.py -- \\
  --mode dims
```

---

## üì¶ Deployment

### Deploy Script: `scripts/deploy_yarn_flow.sh`

**Ch·ª©c nƒÉng**:
1. Create work pool n·∫øu ch∆∞a c√≥
2. Delete old deployment (tr√°nh duplicate)
3. Deploy v·ªõi schedule + tags
4. Show next steps

**Usage**:
```bash
bash scripts/deploy_yarn_flow.sh
```

**Output**:
```
Deployed flow:
  ‚Ä¢ hourly-yarn-pipeline ‚Üí Runs every hour on YARN (cron: 0 * * * *)

Deployment ID: 31505e99-d4de-4e41-9259-84401f3e5e8e
```

### Deployment Lifecycle

**1. Code Changes**:
- ‚úÖ S·ª≠a code trong `*.py` files ‚Üí **KH√îNG C·∫¶N** deploy l·∫°i
- ‚ùå ƒê·ªïi schedule/tags/name ‚Üí **C·∫¶N** deploy l·∫°i

**2. Update Deployment**:
```bash
# Delete old
prefect deployment delete "Hourly Pipeline on YARN/hourly-yarn-pipeline"

# Redeploy
bash scripts/deploy_yarn_flow.sh
```

**3. Worker Restart**:
```bash
# Kill old worker
kill $(cat logs/prefect-worker.pid)

# Start new worker
nohup prefect worker start --pool default > logs/prefect-worker.log 2>&1 &
echo $! > logs/prefect-worker.pid
```

---

## üìä Monitoring

### Prefect UI

**Access**: http://localhost:4200

**Features**:
- Flow runs history
- Task execution status
- Logs v√† errors
- Metrics v√† charts

**Useful Views**:
```bash
# Dashboard ‚Üí Flow Runs (xem t·∫•t c·∫£ runs)
# Deployments ‚Üí hourly-yarn-pipeline (xem schedule)
# Flow Runs ‚Üí Click v√†o run ‚Üí Task Runs (xem t·ª´ng task)
```

### YARN ResourceManager

**Access**: http://khoa-master:8088

**Views**:
- Applications (Spark jobs ƒëang ch·∫°y)
- Cluster metrics (CPU, Memory)
- Node list

### Spark History Server

**Access**: http://khoa-master:18080

**Views**:
- Completed applications
- Job timeline
- Stage v√† task details
- SQL queries

### Command Line Monitoring

**Flow runs**:
```bash
# List recent runs
prefect flow-run ls --limit 10

# Watch specific run
prefect flow-run logs <run-id> --follow
```

**Worker status**:
```bash
# Check if running
ps aux | grep "prefect worker"

# View logs
tail -f logs/prefect-worker.log

# Last 100 lines with timestamp
tail -n 100 logs/prefect-worker.log | grep -E "^\d{2}:\d{2}:\d{2}"
```

**YARN applications**:
```bash
# List running apps
yarn application -list -appStates RUNNING

# App status
yarn application -status <app-id>

# Kill app
yarn application -kill <app-id>
```

### Metrics to Track

| Metric | Source | Threshold |
|--------|--------|-----------|
| Flow duration | Prefect UI | < 2 minutes (hourly) |
| Success rate | Prefect UI | > 95% |
| Data freshness | Bronze table | < 2 hours lag |
| Worker uptime | `ps aux` | Continuous |
| YARN queue usage | YARN UI | < 80% |

---

## üêõ Troubleshooting

### Common Issues

**1. Worker not picking up runs**

**Symptoms**:
```bash
prefect flow-run ls
# Shows: Scheduled ‚Üí Pending (stuck)
```

**Solutions**:
```bash
# Check worker
ps aux | grep "prefect worker"

# Restart worker
kill $(cat logs/prefect-worker.pid)
nohup prefect worker start --pool default > logs/prefect-worker.log 2>&1 &
echo $! > logs/prefect-worker.pid

# Check logs
tail -f logs/prefect-worker.log
```

---

**2. SparkSession not on YARN**

**Error**:
```
RuntimeError: Expected YARN master but got 'local[*]'
```

**Cause**: Ch·∫°y tr·ª±c ti·∫øp v·ªõi Python thay v√¨ spark-submit

**Solution**:
```bash
# ‚ùå WRONG
python Prefect/bronze_flow.py

# ‚úÖ CORRECT
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- --mode upsert
```

---

**3. Subprocess timeout**

**Error**:
```
RuntimeError: Pipeline timed out after 3600.0s
```

**Cause**: Job ch·∫°y qu√° 1 gi·ªù

**Solutions**:
```python
# Option 1: TƒÉng timeout trong yarn_wrapper_flow.py
return_code = process.wait(timeout=7200)  # 2 hours

# Option 2: Optimize query (enable auto_detect)
# Option 3: Reduce chunk size in backfill
```

---

**4. Memory issues in subprocess**

**Symptoms**: Process killed, OOM errors

**Cause**: Tr∆∞·ªõc ƒë√¢y d√πng `capture_output=True` buffer to√†n b·ªô output

**Solution**: ƒê√£ fix b·∫±ng streaming output
```python
# ‚úÖ FIXED: Stream line by line
for line in process.stdout:
    print(line, end='')
    # Ch·ªâ gi·ªØ 100 d√≤ng cu·ªëi
```

---

**5. HDFS connection timeout**

**Error**:
```
java.net.SocketTimeoutException: 60000 millis timeout
```

**Solutions**:
```bash
# Check HDFS
hdfs dfs -ls /user/dlhnhom2/

# Check NameNode
curl http://khoa-master:9870/

# Restart HDFS (if needed)
# sudo systemctl restart hadoop-hdfs-namenode
```

---

**6. Iceberg table locked**

**Error**:
```
org.apache.iceberg.exceptions.CommitFailedException: ...
```

**Cause**: Concurrent writes

**Solutions**:
```bash
# Check running Spark apps
yarn application -list -appStates RUNNING

# Kill stuck apps
yarn application -kill <app-id>
```

---

**7. Deployment prompt issues**

**Error**:
```
EOFError: EOF when reading a line
```

**Cause**: Script ch·∫°y non-interactive nh∆∞ng CLI c·∫ßn input

**Solution**: ƒê√£ fix b·∫±ng `printf "n\nn\n"`
```bash
printf "n\nn\n" | prefect deploy ...
```

---

### Debug Workflow

**Step 1**: Check worker logs
```bash
tail -f logs/prefect-worker.log
```

**Step 2**: Check Prefect UI
```
http://localhost:4200 ‚Üí Flow Runs ‚Üí Click run ‚Üí View logs
```

**Step 3**: Check YARN logs
```bash
yarn logs -applicationId <app-id> | less
```

**Step 4**: Check Spark History
```
http://khoa-master:18080 ‚Üí Find application ‚Üí Stages ‚Üí Failed tasks
```

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

### Internal Docs
- **[PREFECT_YARN_GUIDE.md](../docs/PREFECT_YARN_GUIDE.md)** - H∆∞·ªõng d·∫´n t√≠ch h·ª£p Prefect + YARN
- **[PREFECT_DEPLOYMENT.md](../docs/PREFECT_DEPLOYMENT.md)** - Chi ti·∫øt deployment
- **[INCREMENTAL_IMPLEMENTATION.md](../docs/INCREMENTAL_IMPLEMENTATION.md)** - Auto-detect optimization

### External Resources
- [Prefect Documentation](https://docs.prefect.io/)
- [Apache Iceberg](https://iceberg.apache.org/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)

---

## ‚úÖ Checklist

### Pre-production
- [ ] Test full pipeline v·ªõi small date range
- [ ] Verify YARN execution (`master == "yarn"`)
- [ ] Check auto-detect working (skips when no new data)
- [ ] Validate data quality (no duplicates, no nulls)
- [ ] Monitor memory usage (streaming output)

### Production
- [ ] Deploy v·ªõi hourly schedule
- [ ] Start worker trong background
- [ ] Verify scheduled runs executing
- [ ] Set up monitoring alerts
- [ ] Document runbooks

### Maintenance
- [ ] Weekly: Check worker logs for errors
- [ ] Weekly: Verify data freshness
- [ ] Monthly: Review performance metrics
- [ ] Monthly: Clean up old Spark artifacts
- [ ] Quarterly: Update dependencies

---

## üéì Best Practices

### 1. Always use spark_submit.sh wrapper
```bash
# ‚úÖ CORRECT
bash scripts/spark_submit.sh Prefect/bronze_flow.py -- --mode upsert

# ‚ùå WRONG
python Prefect/bronze_flow.py --mode upsert
```

### 2. Enable auto_detect for incremental loads
```python
transform_bronze_to_silver(spark, mode="merge", auto_detect=True)
transform_fact_hourly(spark, mode="overwrite", auto_detect=True)
```

### 3. Use mode="facts" for backfill Gold
```python
# Skip dimensions (reference data), only process facts
gold_pipeline_flow(mode="facts")
```

### 4. Chunk large date ranges
```bash
# ‚úÖ Monthly chunks for 1 year
--chunk-mode monthly  # 12 chunks

# ‚ùå Process entire year at once (risky)
```

### 5. Monitor worker continuously
```bash
# Run in background with nohup
nohup prefect worker start --pool default > logs/prefect-worker.log 2>&1 &

# Save PID for easy management
echo $! > logs/prefect-worker.pid
```

### 6. Set require_yarn=False for sub-flows
```python
# Parent flow
with get_spark_session("parent", require_yarn=True) as spark:
    # Sub-flows reuse session
    bronze_flow(..., require_yarn=False)
    silver_flow(..., require_yarn=False)
```

---

## üìù Notes

- **Code changes**: Kh√¥ng c·∫ßn deploy l·∫°i, worker t·ª± ƒë·ªông d√πng code m·ªõi
- **Metadata changes**: C·∫ßn deploy l·∫°i (schedule, tags, name)
- **Worker restart**: Ch·ªâ c·∫ßn khi c√≥ l·ªói ho·∫∑c update Prefect version
- **YARN validation**: Set `require_yarn=True` cho parent flow, `False` cho sub-flows

---

**Last Updated**: October 16, 2025
**Author**: DLH-AQI Team
**Version**: 3.0 (Prefect 3.x + YARN Integration + Streaming Optimization)
