"""Prefect Backfill Flow: Historical data processing with chunking.

This flow processes large date ranges by:
- Chunking date range into monthly or weekly segments
- Running full pipeline for each chunk via spark-submit (fresh JVM per job)
- Tracking progress and collecting metrics
- Generating comprehensive summary report

OPTIMIZED: Uses subprocess to spawn fresh spark-submit jobs instead of 
reusing SparkSession across chunks. This prevents memory bloat and GC issues.

Usage (via YARN):
  # Backfill entire year by month
  bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
    --start-date 2024-01-01 \\
    --end-date 2024-12-31 \\
    --chunk-mode monthly

  # Backfill quarter by week
  bash scripts/spark_submit.sh Prefect/backfill_flow.py -- \\
    --start-date 2024-10-01 \\
    --end-date 2024-12-31 \\
    --chunk-mode weekly

DO NOT run directly with python - use spark_submit.sh wrapper for YARN deployment.
"""
import argparse
import json
import os
import subprocess
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple

from prefect import flow

# Setup paths
ROOT_DIR = Path(__file__).resolve().parent.parent
SRC_DIR = ROOT_DIR / "src"
PREFECT_DIR = ROOT_DIR / "Prefect"

for path in [SRC_DIR, str(PREFECT_DIR)]:
    if path not in sys.path:
        sys.path.insert(0, path)

from dotenv import load_dotenv
load_dotenv(ROOT_DIR / ".env")


def generate_date_chunks(
    start_date: str,
    end_date: str,
    chunk_mode: str = "monthly"
) -> List[Tuple[str, str]]:
    """Generate date chunks for backfill processing.
    
    Args:
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)
        chunk_mode: 'monthly', 'weekly', or 'daily'
        
    Returns:
        List of (chunk_start, chunk_end) tuples
    """
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    
    chunks = []
    current = start
    
    while current <= end:
        if chunk_mode == "monthly":
            # Monthly chunks: start of month to end of month
            chunk_start = current.replace(day=1)
            # Last day of month
            if current.month == 12:
                chunk_end = current.replace(day=31)
            else:
                next_month = current.replace(month=current.month + 1, day=1)
                chunk_end = next_month - timedelta(days=1)
            
            # Don't exceed overall end date
            if chunk_end > end:
                chunk_end = end
            
            chunks.append((
                chunk_start.strftime("%Y-%m-%d"),
                chunk_end.strftime("%Y-%m-%d")
            ))
            
            # Move to next month
            if current.month == 12:
                current = current.replace(year=current.year + 1, month=1, day=1)
            else:
                current = current.replace(month=current.month + 1, day=1)
        
        elif chunk_mode == "weekly":
            # Weekly chunks: 7 days at a time
            chunk_start = current
            chunk_end = min(current + timedelta(days=6), end)
            
            chunks.append((
                chunk_start.strftime("%Y-%m-%d"),
                chunk_end.strftime("%Y-%m-%d")
            ))
            
            current = chunk_end + timedelta(days=1)
        
        elif chunk_mode == "daily":
            # Daily chunks: one day at a time
            chunks.append((
                current.strftime("%Y-%m-%d"),
                current.strftime("%Y-%m-%d")
            ))
            
            current += timedelta(days=1)
        
        else:
            raise ValueError(f"Invalid chunk_mode: {chunk_mode}")
        
        # Safety check to prevent infinite loop
        if len(chunks) > 10000:
            raise ValueError("Too many chunks generated (>10000). Check date range.")
    
    return chunks


def run_subprocess_job(
    script_path: str,
    args: List[str],
    job_name: str = "job"
) -> Tuple[bool, Dict]:
    """Run a spark-submit job as a subprocess with fresh JVM.
    
    This avoids SparkSession reuse, memory bloat, and Prefect overhead.
    
    Args:
        script_path: Path to the Python script (e.g., "jobs/bronze/run_bronze_pipeline.py")
        args: List of command-line arguments
        job_name: Name for logging
        
    Returns:
        Tuple of (success, result_dict)
    """
    start_time = time.time()
    
    # Build command
    script_full = ROOT_DIR / script_path
    if not script_full.exists():
        return False, {"error": f"Script not found: {script_full}"}
    
    cmd = [
        "bash",
        str(ROOT_DIR / "scripts/spark_submit.sh"),
        str(script_path),
        "--"
    ] + args
    
    print(f"\n[{job_name}] Running: {' '.join(cmd)}")
    print(f"[{job_name}] Subprocess started (fresh JVM)...")
    
    try:
        result = subprocess.run(
            cmd,
            cwd=str(ROOT_DIR),
            capture_output=True,
            text=True,
            timeout=3600  # 1 hour timeout
        )
        
        elapsed = time.time() - start_time
        
        if result.returncode == 0:
            print(f"[{job_name}] ✓ Completed in {elapsed:.1f}s")
            return True, {
                "success": True,
                "elapsed_seconds": elapsed,
                "stdout": result.stdout[-500:] if result.stdout else "",  # Last 500 chars
            }
        else:
            print(f"[{job_name}] ✗ Failed (exit code: {result.returncode})")
            print(f"[{job_name}] stderr: {result.stderr[-500:]}")
            return False, {
                "success": False,
                "exit_code": result.returncode,
                "error": result.stderr[-500:] if result.stderr else "Unknown error",
                "elapsed_seconds": elapsed
            }
    
    except subprocess.TimeoutExpired:
        print(f"[{job_name}] ✗ Timeout after 1 hour")
        return False, {
            "success": False,
            "error": "Job timeout (>1 hour)",
            "elapsed_seconds": 3600
        }
    except Exception as e:
        print(f"[{job_name}] ✗ Exception: {e}")
        return False, {
            "success": False,
            "error": str(e),
            "elapsed_seconds": time.time() - start_time
        }


@flow(
    name="Backfill Flow",
    description="Historical data backfill with subprocess-spawned Spark jobs (fresh JVM per stage)",
    log_prints=True
)
def backfill_flow(
    start_date: str,
    end_date: str,
    chunk_mode: str = "monthly",
    include_bronze: bool = True,
    include_silver: bool = True,
    include_gold: bool = True,
    locations_path: str = "hdfs://khoa-master:9000/user/dlhnhom2/data/locations.jsonl",
    pollutants_path: str = "hdfs://khoa-master:9000/user/dlhnhom2/data/dim_pollutant.jsonl",
    warehouse: str = "hdfs://khoa-master:9000/warehouse/iceberg",
    require_yarn: bool = True
) -> Dict:
    """Execute backfill for historical date range with chunking.
    
    OPTIMIZED: Uses subprocess to spawn fresh spark-submit jobs for each stage.
    This prevents memory bloat, GC issues, and Prefect overhead.
    
    Args:
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)
        chunk_mode: 'monthly', 'weekly', or 'daily' (passed to bronze job)
        include_bronze: Include bronze ingestion
        include_silver: Include silver transformation
        include_gold: Include gold pipeline
        locations_path: Path to locations configuration
        pollutants_path: Path to pollutants configuration
        warehouse: Iceberg warehouse URI
        require_yarn: Validate YARN deployment
        
    Returns:
        Dictionary with backfill statistics
    """
    print("="*80)
    print("BACKFILL FLOW: OPTIMIZED WITH SUBPROCESS")
    print("="*80)
    print(f"Date range: {start_date} to {end_date}")
    print(f"Chunk mode: {chunk_mode}")
    print(f"Warehouse: {warehouse}")
    print(f"Stages: ", end="")
    stages = []
    if include_bronze:
        stages.append("Bronze")
    if include_silver:
        stages.append("Silver")
    if include_gold:
        stages.append("Gold")
    print(" → ".join(stages))
    print("="*80)
    
    backfill_start = time.time()
    results = {
        "success": True,
        "start_date": start_date,
        "end_date": end_date,
        "chunk_mode": chunk_mode,
        "stages": {},
        "elapsed_seconds": 0,
        "errors": []
    }
    
    # === BRONZE STAGE ===
    if include_bronze:
        print(f"\n{'='*80}")
        print("STAGE 1: BRONZE INGESTION (backfill)")
        print(f"{'='*80}")
        
        bronze_args = [
            "--mode", "backfill",
            "--start-date", start_date,
            "--end-date", end_date,
            "--locations", locations_path,
            "--override"  # Override existing data
        ]
        
        success, bronze_result = run_subprocess_job(
            "jobs/bronze/run_bronze_pipeline.py",
            bronze_args,
            "Bronze"
        )
        
        results["stages"]["bronze"] = bronze_result
        if not success:
            results["success"] = False
            results["errors"].append(f"Bronze stage failed: {bronze_result.get('error')}")
    
    # === SILVER STAGE ===
    if include_silver:
        print(f"\n{'='*80}")
        print("STAGE 2: SILVER TRANSFORMATION (full)")
        print(f"{'='*80}")
        
        silver_args = [
            "--mode", "full"
        ]
        
        success, silver_result = run_subprocess_job(
            "jobs/silver/run_silver_pipeline.py",
            silver_args,
            "Silver"
        )
        
        results["stages"]["silver"] = silver_result
        if not success:
            results["success"] = False
            results["errors"].append(f"Silver stage failed: {silver_result.get('error')}")
    
    # === GOLD STAGE ===
    if include_gold:
        print(f"\n{'='*80}")
        print("STAGE 3: GOLD PIPELINE (all)")
        print(f"{'='*80}")
        
        gold_args = [
            "--mode", "all",
            "--locations", locations_path,
            "--pollutants", pollutants_path
        ]
        
        success, gold_result = run_subprocess_job(
            "jobs/gold/run_gold_pipeline.py",
            gold_args,
            "Gold"
        )
        
        results["stages"]["gold"] = gold_result
        if not success:
            results["success"] = False
            results["errors"].append(f"Gold stage failed: {gold_result.get('error')}")
    
    # === SUMMARY ===
    elapsed = time.time() - backfill_start
    results["elapsed_seconds"] = elapsed
    
    print(f"\n{'='*80}")
    print("BACKFILL COMPLETE")
    print(f"{'='*80}")
    print(f"Overall status: {'✓ SUCCESS' if results['success'] else '✗ FAILED'}")
    print(f"Date range: {start_date} to {end_date}")
    print(f"Total time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)")
    
    if results["stages"]:
        print("\nStage breakdown:")
        for stage_name, stage_result in results["stages"].items():
            stage_elapsed = stage_result.get("elapsed_seconds", 0)
            status = "✓" if stage_result.get("success") else "✗"
            print(f"  {status} {stage_name.upper()}: {stage_elapsed:.1f}s")
    
    if results["errors"]:
        print("\nErrors:")
        for error in results["errors"]:
            print(f"  - {error}")
    
    print("="*80)
    
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Backfill Flow with Chunking - Submit via spark_submit.sh"
    )
    
    parser.add_argument(
        "--start-date",
        required=True,
        help="Start date (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--end-date",
        required=True,
        help="End date (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--chunk-mode",
        choices=["monthly", "weekly", "daily"],
        default="monthly",
        help="Chunking strategy"
    )
    parser.add_argument(
        "--skip-bronze",
        action="store_true",
        help="Skip bronze ingestion"
    )
    parser.add_argument(
        "--skip-silver",
        action="store_true",
        help="Skip silver transformation"
    )
    parser.add_argument(
        "--skip-gold",
        action="store_true",
        help="Skip gold pipeline"
    )
    parser.add_argument(
        "--locations",
        default="hdfs://khoa-master:9000/user/dlhnhom2/data/locations.jsonl"
    )
    parser.add_argument(
        "--pollutants",
        default="hdfs://khoa-master:9000/user/dlhnhom2/data/dim_pollutant.jsonl"
    )
    parser.add_argument(
        "--warehouse",
        default=os.getenv("WAREHOUSE_URI", "hdfs://khoa-master:9000/warehouse/iceberg")
    )
    parser.add_argument(
        "--no-yarn-check",
        action="store_true",
        help="Skip YARN validation (for local testing only)"
    )
    
    args = parser.parse_args()
    
    # Run backfill
    result = backfill_flow(
        start_date=args.start_date,
        end_date=args.end_date,
        chunk_mode=args.chunk_mode,
        include_bronze=not args.skip_bronze,
        include_silver=not args.skip_silver,
        include_gold=not args.skip_gold,
        locations_path=args.locations,
        pollutants_path=args.pollutants,
        warehouse=args.warehouse,
        require_yarn=not args.no_yarn_check
    )
    
    sys.exit(0 if result["success"] else 1)
